<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>phi.math API documentation</title>
<meta name="description" content="Vectorized operations, tensors with named dimensions …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.math</code></h1>
</header>
<section id="section-intro">
<p>Vectorized operations, tensors with named dimensions.</p>
<p>This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.</p>
<p>Main classes: <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code>, <code><a title="phi.math.Extrapolation" href="#phi.math.Extrapolation">Extrapolation</a></code>.</p>
<p>The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Vectorized operations, tensors with named dimensions.

This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.

Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.

The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.

See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html
&#34;&#34;&#34;

from .backend import precision, set_global_precision, get_precision, Solve, LinearSolve, DType, NUMPY_BACKEND

from .extrapolation import Extrapolation

from ._config import GLOBAL_AXIS_ORDER

from ._shape import Shape, spatial_shape, EMPTY_SHAPE, batch_shape, channel_shape, shape
from ._tensors import wrap, tensor, tensors, Tensor, TensorDim
from ._functions import (
    choose_backend_t as choose_backend, all_available,
    print_ as print,
    map_ as map,
    trace_function, gradient_function,
    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace,  # creation operators (use default backend)
    zeros_like, ones_like,
    batch_stack, spatial_stack, channel_stack, unstack, concat,
    pad, spatial_pad,
    join_dimensions, split_dimension, flatten, expand, expand_batch, expand_spatial, expand_channel, transpose, # reshape operations
    divide_no_nan,
    where, nonzero,
    sum_ as sum, mean, std, prod, max_ as max, min_ as min, any_ as any, all_ as all,  # reduce
    dot, matmul, einsum,
    abs, sign,
    round_ as round, ceil, floor,
    maximum, minimum, clip,
    sqrt, exp, sin, cos,
    to_float, to_int, to_complex, imag, real,
    boolean_mask,
    isfinite,
    closest_grid_values, grid_sample, scatter, gather,
    fft, ifft, conv,
    dtype, cast,
    tile,
    close, assert_close,
    solve, minimize,
    record_gradients, gradients, stop_gradient
)
from ._nd import (
    shift,
    spatial_sum, vec_abs, vec_squared, cross_product,
    normalize_to,
    l1_loss, l2_loss, l_n_loss, frequency_loss,
    gradient, laplace,
    fourier_laplace, fourier_poisson, abs_square,
    downsample2x, upsample2x, sample_subgrid,
    extrapolate_valid_values,
)

PI = 3.14159265358979323846
&#34;&#34;&#34;Value of π to double precision &#34;&#34;&#34;
pi = PI

SCIPY_BACKEND = NUMPY_BACKEND  # to show up in pdoc
&#34;&#34;&#34; Alias for `NUMPY_BACKEND` &#34;&#34;&#34;
NUMPY_BACKEND = NUMPY_BACKEND  # to show up in pdoc
&#34;&#34;&#34;Default backend for NumPy arrays and SciPy objects.&#34;&#34;&#34;

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></dt>
<dd>
<div class="desc"><p>Low-level library wrappers for delegating vector operations.</p></div>
</dd>
<dt><code class="name"><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></dt>
<dd>
<div class="desc"><p>Defines standard extrapolations …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phi.math.NUMPY_BACKEND"><code class="name">var <span class="ident">NUMPY_BACKEND</span></code></dt>
<dd>
<div class="desc"><p>Default backend for NumPy arrays and SciPy objects.</p></div>
</dd>
<dt id="phi.math.PI"><code class="name">var <span class="ident">PI</span></code></dt>
<dd>
<div class="desc"><p>Value of π to double precision</p></div>
</dd>
<dt id="phi.math.SCIPY_BACKEND"><code class="name">var <span class="ident">SCIPY_BACKEND</span></code></dt>
<dd>
<div class="desc"><p>Alias for <code><a title="phi.math.NUMPY_BACKEND" href="#phi.math.NUMPY_BACKEND">NUMPY_BACKEND</a></code></p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.math.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.abs)</code></pre>
</details>
</dd>
<dt id="phi.math.abs_square"><code class="name flex">
<span>def <span class="ident">abs_square</span></span>(<span>complex)</span>
</code></dt>
<dd>
<div class="desc"><p>get the square magnitude</p>
<h2 id="args">Args</h2>
<p>complex(Tensor): complex input data</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dt>
<dd>real valued magnitude squared</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_square(complex):
    &#34;&#34;&#34;
    get the square magnitude

    Args:
      complex(Tensor): complex input data

    Returns:
      Tensor: real valued magnitude squared

    &#34;&#34;&#34;
    return math.imag(complex) ** 2 + math.real(complex) ** 2</code></pre>
</details>
</dd>
<dt id="phi.math.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.all(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.all_available"><code class="name flex">
<span>def <span class="ident">all_available</span></span>(<span>*values: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the values of all given tensors are known and can be read at this point.</p>
<p>Tensors are typically available when the backend operates in eager mode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>tensors to check</dd>
<dt><strong><code>*values</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>bool</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_available(*values: Tensor):
    &#34;&#34;&#34;
    Tests if the values of all given tensors are known and can be read at this point.
    
    Tensors are typically available when the backend operates in eager mode.

    Args:
      values: tensors to check
      *values: Tensor: 

    Returns:
      bool

    &#34;&#34;&#34;
    for value in values:
        natives = value._natives()
        natives_available = [choose_backend(native).is_available(native) for native in natives]
        if not all(natives_available):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.any(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.assert_close"><code class="name flex">
<span>def <span class="ident">assert_close</span></span>(<span>*tensors, rel_tolerance: float = 1e-05, abs_tolerance: float = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks that all tensors have equal values within the specified tolerance.
Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong></dt>
<dd>tensor or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
<dt><strong><code>*tensors</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_close(*tensors,
                 rel_tolerance: float = 1e-5,
                 abs_tolerance: float = 0):
    &#34;&#34;&#34;
    Checks that all tensors have equal values within the specified tolerance.
    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
      tensors: tensor or tensor-like (constant) each
      rel_tolerance: relative tolerance (Default value = 1e-5)
      abs_tolerance: absolute tolerance (Default value = 0)
      *tensors: 

    Returns:

    &#34;&#34;&#34;
    any_tensor = next(filter(lambda t: isinstance(t, Tensor), tensors))
    if any_tensor is None:
        tensors = [wrap(t) for t in tensors]
    else:  # use Tensor to infer dimensions
        tensors = [any_tensor._tensor(t).__simplify__() for t in tensors]
    for other in tensors[1:]:
        _assert_close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_shape"><code class="name flex">
<span>def <span class="ident">batch_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'batch'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None):
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;batch&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_stack"><code class="name flex">
<span>def <span class="ident">batch_stack</span></span>(<span>values, dim: str = 'batch')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_stack(values, dim: str = &#39;batch&#39;):
    return _stack(values, dim, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>x: phi.math._tensors.Tensor, mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_mask(x: Tensor, mask):
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>x: phi.math._tensors.Tensor, dtype: phi.math.backend._dtype.DType) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cast(x: Tensor, dtype: DType) -&gt; Tensor:
    return x._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))</code></pre>
</details>
</dd>
<dt id="phi.math.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ceil(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.ceil)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_shape"><code class="name flex">
<span>def <span class="ident">channel_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'channel'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or list or tuple: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_shape(sizes: Shape or dict or list or tuple, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;channel&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or list or tuple: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_stack"><code class="name flex">
<span>def <span class="ident">channel_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_stack(values, dim: str):
    return _stack(values, dim, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Choose backend for given <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_backend_t(*values, prefer_default=False):
    &#34;&#34;&#34; Choose backend for given `Tensor` or native tensor values. &#34;&#34;&#34;
    natives = sum([v._natives() if isinstance(v, Tensor) else (v,) for v in values], ())
    return choose_backend(*natives, prefer_default=prefer_default)</code></pre>
</details>
</dd>
<dt id="phi.math.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>x: phi.math._tensors.Tensor, lower_limit: float, upper_limit: float)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip(x: Tensor, lower_limit: float or Tensor, upper_limit: float or Tensor):
    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):

        def clip_(x):
            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))

        return broadcast_op(clip_, [x])
    else:
        return maximum(lower_limit, minimum(x, upper_limit))</code></pre>
</details>
</dd>
<dt id="phi.math.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>*tensors, rel_tolerance=1e-05, abs_tolerance=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all tensors have equal values within the specified tolerance.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong></dt>
<dd>tensor or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
<dt><strong><code>*tensors</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0):
    &#34;&#34;&#34;
    Checks whether all tensors have equal values within the specified tolerance.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
      tensors: tensor or tensor-like (constant) each
      rel_tolerance: relative tolerance (Default value = 1e-5)
      abs_tolerance: absolute tolerance (Default value = 0)
      *tensors: 

    Returns:

    &#34;&#34;&#34;
    tensors = [wrap(t) for t in tensors]
    for other in tensors[1:]:
        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.closest_grid_values"><code class="name flex">
<span>def <span class="ident">closest_grid_values</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>, stack_dim_prefix='closest_')</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the neighboring grid points in all spatial directions and returns their values.
The result will have 2^d values for each vector in coordiantes in d dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid data. The grid is spanned by the spatial dimensions of the tensor</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>tensor with 1 channel dimension holding vectors pointing to locations in grid index space</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>stack_dim_prefix</code></strong></dt>
<dd>For each spatial dimension <code>dim</code>, stacks lower and upper closest values along dimension <code>stack_dim_prefix+dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,&hellip;), grid_channel)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def closest_grid_values(grid: Tensor,
                        coordinates: Tensor,
                        extrap: &#39;extrapolation.Extrapolation&#39;,
                        stack_dim_prefix=&#39;closest_&#39;):
    &#34;&#34;&#34;
    Finds the neighboring grid points in all spatial directions and returns their values.
    The result will have 2^d values for each vector in coordiantes in d dimensions.

    Args:
      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
      extrap: grid extrapolation
      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.

    Returns:
      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)

    &#34;&#34;&#34;
    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix), [grid, coordinates])</code></pre>
</details>
</dd>
<dt id="phi.math.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>values: tuple, dim: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates a sequence of tensors along one dimension.
The shapes of all values must be equal, except for the size of the concat dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to concatenate</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>concat dimension, must be present in all values</dd>
<dt><strong><code>values</code></strong></dt>
<dd>tuple or list: </dd>
<dt><strong><code>dim</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>concatenated tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat(values: tuple or list, dim: str) -&gt; Tensor:
    &#34;&#34;&#34;
    Concatenates a sequence of tensors along one dimension.
    The shapes of all values must be equal, except for the size of the concat dimension.

    Args:
      values: Tensors to concatenate
      dim: concat dimension, must be present in all values
      values: tuple or list: 
      dim: str: 

    Returns:
      concatenated tensor

    &#34;&#34;&#34;
    broadcast_shape = values[0].shape
    natives = [v.native(order=broadcast_shape.names) for v in values]
    backend = choose_backend(*natives)
    concatenated = backend.concat(natives, broadcast_shape.index(dim))
    return NativeTensor(concatenated, broadcast_shape.with_sizes(backend.staticshape(concatenated)))</code></pre>
</details>
</dd>
<dt id="phi.math.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>value: phi.math._tensors.Tensor, kernel: phi.math._tensors.Tensor, padding='same')</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv(value: Tensor, kernel: Tensor, padding=&#39;same&#39;):
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cos(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.cos)</code></pre>
</details>
</dd>
<dt id="phi.math.cross_product"><code class="name flex">
<span>def <span class="ident">cross_product</span></span>(<span>vec1: phi.math._tensors.Tensor, vec2: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_product(vec1: Tensor, vec2: Tensor):
    vec1, vec2 = math.tensors(vec1, vec2)
    spatial_rank = vec1.vector.size if &#39;vector&#39; in vec1.shape else vec2.vector.size
    if spatial_rank == 2:  # Curl in 2D
        dist_0, dist_1 = vec2.vector.unstack()
        if GLOBAL_AXIS_ORDER.is_x_first:
            velocity = vec1 * math.channel_stack([-dist_1, dist_0], &#39;vector&#39;)
        else:
            velocity = vec1 * math.channel_stack([dist_1, -dist_0], &#39;vector&#39;)
        return velocity
    elif spatial_rank == 3:  # Curl in 3D
        raise NotImplementedError(f&#39;spatial_rank={spatial_rank} not yet implemented&#39;)
    else:
        raise AssertionError(f&#39;dims = {spatial_rank}. Vector product not available in &gt; 3 dimensions&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def divide_no_nan(x, y):
    return custom_op2(x, y, divide_no_nan, lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_), lambda y_, x_: divide_no_nan(x_, y_), lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>a, b, axes) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(a, b, axes) -&gt; Tensor:
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.downsample2x"><code class="name flex">
<span>def <span class="ident">downsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to half the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation. Used to insert an additional value for odd spatial dims</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which down-sampling is applied. If None, down-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>half-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def downsample2x(grid: Tensor,
                 padding: Extrapolation = extrapolation.BOUNDARY,
                 dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to half the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: full size grid
      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      half-size grid

    &#34;&#34;&#34;
    dims = grid.shape.spatial.only(dims).names
    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
    for dim in dims:
        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(x):
    if isinstance(x, Tensor):
        return x.dtype
    else:
        return choose_backend(x).dtype(x)</code></pre>
</details>
</dd>
<dt id="phi.math.einsum"><code class="name flex">
<span>def <span class="ident">einsum</span></span>(<span>equation, *tensors) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def einsum(equation, *tensors) -&gt; Tensor:
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.exp)</code></pre>
</details>
</dd>
<dt id="phi.math.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>value: phi.math._tensors.Tensor, dim_name: str, dim_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(value: Tensor, dim_name: str, dim_size: int = 1):
    dim_type = _infer_dim_type_from_name(dim_name)
    return _expand_dim(value, dim_name, dim_size, dim_type)</code></pre>
</details>
</dd>
<dt id="phi.math.expand_batch"><code class="name flex">
<span>def <span class="ident">expand_batch</span></span>(<span>value: phi.math._tensors.Tensor, dim_name: str, dim_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_batch(value: Tensor, dim_name: str, dim_size: int = 1):
    return _expand_dim(value, dim_name, dim_size, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.expand_channel"><code class="name flex">
<span>def <span class="ident">expand_channel</span></span>(<span>value: phi.math._tensors.Tensor, dim_name: str, dim_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_channel(value: Tensor, dim_name: str, dim_size: int = 1):
    return _expand_dim(value, dim_name, dim_size, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.expand_spatial"><code class="name flex">
<span>def <span class="ident">expand_spatial</span></span>(<span>value: phi.math._tensors.Tensor, dim_name: str, dim_size: int = 1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_spatial(value: Tensor, dim_name: str, dim_size: int = 1):
    return _expand_dim(value, dim_name, dim_size, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.extrapolate_valid_values"><code class="name flex">
<span>def <span class="ident">extrapolate_valid_values</span></span>(<span>values: phi.math._tensors.Tensor, valid: phi.math._tensors.Tensor, distance_cells: int = 1) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolates the values of <code>values</code> which are marked by the nonzero values of <code>valid</code> for <code>distance_cells</code> steps in all spatial directions.
Overlapping extrapolated values get averaged.</p>
<p>Examples (1-step extrapolation), x marks the values for extrapolation:
200
000
210
004
00x
044
100
000
100
010 + 0x0 =&gt; 111
000 + 000 =&gt; 204
000 + 000 =&gt; 204
040
000
010
200
x00
220
204
x0x
234</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor which holds the values for extrapolation</dd>
<dt><strong><code>valid</code></strong></dt>
<dd>Tensor with same size as <code>x</code> marking the values for extrapolation with nonzero values</dd>
<dt><strong><code>distance_cells</code></strong></dt>
<dd>Number of extrapolation steps</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>values</code></dt>
<dd>Extrapolation result</dd>
<dt><code>valid</code></dt>
<dd>mask marking all valid values after extrapolation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extrapolate_valid_values(values: Tensor, valid: Tensor, distance_cells: int = 1) -&gt; Tuple[Tensor, Tensor]:
    &#34;&#34;&#34;
    Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance_cells` steps in all spatial directions.
    Overlapping extrapolated values get averaged.

    Examples (1-step extrapolation), x marks the values for extrapolation:
        200   000    210        004   00x    044        100   000    100
        010 + 0x0 =&gt; 111        000 + 000 =&gt; 204        000 + 000 =&gt; 204
        040   000    010        200   x00    220        204   x0x    234

    Args:
        values: Tensor which holds the values for extrapolation
        valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values
        distance_cells: Number of extrapolation steps

    Returns:
        values: Extrapolation result
        valid: mask marking all valid values after extrapolation
    &#34;&#34;&#34;
    distance_cells = min(distance_cells, max(values.shape))
    for _ in range(distance_cells):
        valid = math.divide_no_nan(valid, valid)  # ensure binary mask
        values_l, values_r = shift(values * valid, (-1, 1))
        mask_l, mask_r = shift(valid, (-1, 1))
        overlap = math.sum_(mask_l + mask_r, dim=&#39;shift&#39;)
        extp = math.divide_no_nan(math.sum_(values_l + values_r, dim=&#39;shift&#39;), overlap)  # take mean where extrapolated values overlap
        new_valid = valid + overlap
        values = math.where(valid, values, math.where(new_valid, extp, values))  # don&#39;t overwrite initial values within the mask / keep values not affected by extrapolation
        valid = new_valid
    return values, math.divide_no_nan(valid, valid)</code></pre>
</details>
</dd>
<dt id="phi.math.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>x: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a fast Fourier transform (FFT) on all spatial dimensions of x.</p>
<p>The inverse operation is :func:<code><a title="phi.math.ifft" href="#phi.math.ifft">ifft()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of type float or complex</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>FFT(x) of type complex</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft(x: Tensor):
    &#34;&#34;&#34;
    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
    
    The inverse operation is :func:`ifft`.

    Args:
      x: tensor of type float or complex
      x: Tensor: 

    Returns:
      FFT(x) of type complex

    &#34;&#34;&#34;
    native, assemble = _invertible_standard_form(x)
    result = choose_backend(native).fft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.fftfreq"><code class="name flex">
<span>def <span class="ident">fftfreq</span></span>(<span>resolution: phi.math._shape.Shape, dtype: phi.math.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the discrete Fourier transform sample frequencies.
These are the frequencies corresponding to the components of the result of <code>math.fft</code> on a tensor of shape <code>resolution</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>Grid resolution measured in cells</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type of the returned tensor (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor holding the frequencies of the corresponding values computed by math.fft</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fftfreq(resolution: Shape, dtype: DType = None):
    &#34;&#34;&#34;
    Returns the discrete Fourier transform sample frequencies.
    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.

    Args:
      resolution: Grid resolution measured in cells
      dtype: Data type of the returned tensor (Default value = None)

    Returns:
      tensor holding the frequencies of the corresponding values computed by math.fft

    &#34;&#34;&#34;
    resolution = spatial_shape(resolution)
    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.named_sizes})
    return to_float(k) if dtype is None else cast(k, dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>value: phi.math._tensors.Tensor, flat_dim: str = 'flat')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten(value: Tensor, flat_dim: str = &#39;flat&#39;):
    return join_dimensions(value, value.shape, flat_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def floor(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.floor)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_laplace"><code class="name flex">
<span>def <span class="ident">fourier_laplace</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the spatial laplace operator to the given tensor with periodic boundary conditions.</p>
<p><em>Note:</em> The results of <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code> and <code><a title="phi.math.laplace" href="#phi.math.laplace">laplace()</a></code> are close but not identical.</p>
<p>This implementation computes the laplace operator in Fourier space.
The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>tensor, assumed to have periodic boundary conditions</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>distance between grid points, tensor-like, scalar or vector</dd>
<dt><strong><code>times</code></strong></dt>
<dd>number of times the laplace operator is applied. The computational cost is independent of this parameter.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape as <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_laplace(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
    
    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
    
    This implementation computes the laplace operator in Fourier space.
    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.

    Args:
      grid: tensor, assumed to have periodic boundary conditions
      dx: distance between grid points, tensor-like, scalar or vector
      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:
      tensor of same shape as `tensor`

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi)**2 * k_squared
    result = math.real(math.ifft(frequencies * fft_laplace ** times))
    return math.cast(result / wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_poisson"><code class="name flex">
<span>def <span class="ident">fourier_poisson</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse operation to <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_poisson(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Inverse operation to `fourier_laplace`.

    Args:
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi)**2 * k_squared
    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
    result = math.real(math.ifft(math.divide_no_nan(frequencies, math.to_complex(fft_laplace ** times))))
    return math.cast(result * wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.frequency_loss"><code class="name flex">
<span>def <span class="ident">frequency_loss</span></span>(<span>tensor, frequency_falloff=100, reduce_batches=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Instead of minimizing each entry of the tensor, minimize the frequencies of the tensor, emphasizing lower frequencies over higher ones.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>reduce_batches</code></strong></dt>
<dd>whether to reduce the batch dimension of the loss by adding the losses along the first dimension (Default value = True)</dd>
<dt><strong><code>tensor</code></strong></dt>
<dd>typically actual - target</dd>
<dt><strong><code>frequency_falloff</code></strong></dt>
<dd>large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally. (Default value = 100)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>scalar loss value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def frequency_loss(tensor, frequency_falloff=100, reduce_batches=True):
    &#34;&#34;&#34;
    Instead of minimizing each entry of the tensor, minimize the frequencies of the tensor, emphasizing lower frequencies over higher ones.

    Args:
      reduce_batches: whether to reduce the batch dimension of the loss by adding the losses along the first dimension (Default value = True)
      tensor: typically actual - target
      frequency_falloff: large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally. (Default value = 100)

    Returns:
      scalar loss value

    &#34;&#34;&#34;
    if struct.isstruct(tensor):
        all_tensors = struct.flatten(tensor)
        return sum(frequency_loss(tensor, frequency_falloff, reduce_batches) for tensor in all_tensors)
    diff_fft = abs_square(math.fft(tensor))
    k_squared = math.sum_(math.fftfreq(tensor.shape[1:-1]) ** 2, &#39;vector&#39;)
    weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)
    return l1_loss(diff_fft * weights, reduce_batches=reduce_batches)</code></pre>
</details>
</dd>
<dt id="phi.math.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>values: phi.math._tensors.Tensor, indices: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather(values: Tensor, indices: Tensor):
    b_values = join_dimensions(values, values.shape.batch, &#39;batch&#39;)
    b_values = join_dimensions(b_values, b_values.shape.channel, &#39;channel&#39;, pos=-1)
    b_indices = join_dimensions(indices, indices.shape.batch, &#39;batch&#39;)
    native_values = b_values.native()
    native_indices = b_indices.native()
    native_result = choose_backend(native_values, native_indices).batched_gather_nd(native_values, native_indices)
    b_result = tensor(native_result, (&#39;batch&#39;, *indices.shape.spatial.names, &#39;vector&#39;))
    result = split_dimension(b_result.vector, values.shape.channel)
    result = split_dimension(result.batch, values.shape.batch &amp; indices.shape.batch)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_precision() -&gt; int:
    &#34;&#34;&#34;
    Gets the current target floating point precision in bits.
    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.

    Any Backend method may convert floating point values to this precision, even if the input had a different precision.

    Returns:
        16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    return _PRECISION[-1]</code></pre>
</details>
</dd>
<dt id="phi.math.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>grid: phi.math._tensors.Tensor, dx: float = 1, difference: str = 'central', padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None, stack_dim: str = 'gradient')</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the gradient of a scalar channel from finite differences.
The gradient vectors are in reverse order, lowest dimension first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid values</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>optional) sequence of dimension names</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>physical distance between grid points (default 1)</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>type of difference, one of ('forward', 'backward', 'central') (default 'forward')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>tensor padding mode</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>name of the new vector dimension listing the gradient w.r.t. the various axes</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>float or int:
(Default value = 1)</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>str:
(Default value = 'central')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation or None:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>str:
(Default value = 'gradient')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch_size, spatial_dimensions&hellip;, spatial rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient(grid: Tensor,
             dx: float or int = 1,
             difference: str = &#39;central&#39;,
             padding: Extrapolation or None = extrapolation.BOUNDARY,
             dims: tuple or None = None,
             stack_dim: str = &#39;gradient&#39;):
    &#34;&#34;&#34;
    Calculates the gradient of a scalar channel from finite differences.
    The gradient vectors are in reverse order, lowest dimension first.

    Args:
      grid: grid values
      dims: optional) sequence of dimension names
      dx: physical distance between grid points (default 1)
      difference: type of difference, one of (&#39;forward&#39;, &#39;backward&#39;, &#39;central&#39;) (default &#39;forward&#39;)
      padding: tensor padding mode
      stack_dim: name of the new vector dimension listing the gradient w.r.t. the various axes
      grid: Tensor: 
      dx: float or int:  (Default value = 1)
      difference: str:  (Default value = &#39;central&#39;)
      padding: Extrapolation or None:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)
      stack_dim: str:  (Default value = &#39;gradient&#39;)

    Returns:
      tensor of shape (batch_size, spatial_dimensions..., spatial rank)

    &#34;&#34;&#34;
    grid = wrap(grid)
    if difference.lower() == &#39;central&#39;:
        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / (dx * 2)
    elif difference.lower() == &#39;forward&#39;:
        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    elif difference.lower() == &#39;backward&#39;:
        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    else:
        raise ValueError(&#39;Invalid difference type: {}. Can be CENTRAL or FORWARD&#39;.format(difference))</code></pre>
</details>
</dd>
<dt id="phi.math.gradient_function"><code class="name flex">
<span>def <span class="ident">gradient_function</span></span>(<span>f: Callable, wrt: tuple = (0,), get_output=False) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the gradient of <code>f</code>.</p>
<p>Example:</p>
<pre><code class="language-python">def loss_function(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx, = gradient_function(loss_function)(x, y)

loss, prediction, dx, dy = gradient_function(loss_function, wrt=(0, 1),
                                             get_output=True)(x, y)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if <code>return_values=True</code>.
All arguments for which the gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the gradient function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Arguments of <code>f</code> with respect to which the gradient should be computed.
Example: <code>wrt_indices=[0]</code> computes the gradient with respect to the first argument of <code>f</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns the value of <code>f</code>, auxiliary data and gradient of <code>f</code> if <code>get_output=True</code>, else just the gradient of <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient_function(f: Callable, wrt: tuple or list = (0,), get_output=False) -&gt; Callable:
    &#34;&#34;&#34;
    Creates a function which computes the gradient of `f`.

    Example:

    ```python
    def loss_function(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    dx, = gradient_function(loss_function)(x, y)

    loss, prediction, dx, dy = gradient_function(loss_function, wrt=(0, 1),
                                                 get_output=True)(x, y)
    ```

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the gradient function if `return_values=True`.
            All arguments for which the gradient is computed must be of dtype float or complex.
        get_output: Whether the gradient function should also return the return values of `f`.
        wrt: Arguments of `f` with respect to which the gradient should be computed.
            Example: `wrt_indices=[0]` computes the gradient with respect to the first argument of `f`.

    Returns:
        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and gradient of `f` if `get_output=True`, else just the gradient of `f`.
    &#34;&#34;&#34;
    INPUT_TENSORS = []
    OUTPUT_TENSORS = []
    ARG_INDICES = []

    def native_function(*natives):
        natives = list(natives)
        values = [t._op1(lambda _: natives.pop(0)) for t in INPUT_TENSORS]
        assert len(natives) == 0, &#34;Not all arguments were converted&#34;
        result = f(*values)
        results = [result] if not isinstance(result, (tuple, list)) else result
        assert all(isinstance(t, Tensor) for t in results), f&#34;Function output must be Tensor or sequence of tensors but got {result}.&#34;
        OUTPUT_TENSORS.clear()
        OUTPUT_TENSORS.extend(results)
        return sum([v._natives() for v in results], ())

    backend = default_backend()

    class GradientFunction:

        def __init__(self):
            self.gradf = None

        def __call__(self, *args, **kwargs):
            assert not len(kwargs)
            shifted_wrt = [i for i in range(len(ARG_INDICES)) if ARG_INDICES[i] in wrt]
            if self.gradf is None:
                self.gradf = backend.gradient_function(native_function, shifted_wrt, get_output=get_output)
            return self.gradf(*args)

    grad_native = GradientFunction()

    def wrapper(*values: Tensor):
        assert all(isinstance(v, Tensor) for v in values)
        INPUT_TENSORS.clear()
        INPUT_TENSORS.extend(values)
        ARG_INDICES.clear()
        natives = []
        for arg_index, v in enumerate(values):
            v._expand()
            n = v._natives()
            natives.extend(n)
            for _ in range(len(n)):
                ARG_INDICES.append(arg_index)
        results_native = list(grad_native(*natives))
        proto_tensors = []
        if get_output:
            proto_tensors.extend(OUTPUT_TENSORS)
        proto_tensors.extend([t for i, t in enumerate(INPUT_TENSORS) if i in wrt])
        results = [t._with_natives_replaced(results_native) for t in proto_tensors]
        assert len(results_native) == 0
        return results

    return wrapper</code></pre>
</details>
</dd>
<dt id="phi.math.gradients"><code class="name flex">
<span>def <span class="ident">gradients</span></span>(<span>y: phi.math._tensors.Tensor, *x: phi.math._tensors.Tensor, grad_y: phi.math._tensors.Tensor = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the gradients dy/dx for each <code>x</code>.
The parameters <code>x</code> must be marked prior to all operations for which gradients should be recorded using <code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>Scalar <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> computed from <code>x</code>, typically loss.</dd>
<dt><strong><code>*x</code></strong></dt>
<dd>(Optional) Parameter tensors which were previously marked in <code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients()</a></code>.
If empty, computes the gradients w.r.t. all marked tensors.</dd>
<dt><strong><code>grad_y</code></strong></dt>
<dd>(optional) Gradient at <code>y</code>, defaults to 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> if one <code>x</code> was passed, else sequence of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradients(y: Tensor,
              *x: Tensor,
              grad_y: Tensor or None = None):
    &#34;&#34;&#34;
    Computes the gradients dy/dx for each `x`.
    The parameters `x` must be marked prior to all operations for which gradients should be recorded using `record_gradients()`.

    Args:
        y: Scalar `Tensor` computed from `x`, typically loss.
        *x: (Optional) Parameter tensors which were previously marked in `record_gradients()`.
            If empty, computes the gradients w.r.t. all marked tensors.
        grad_y: (optional) Gradient at `y`, defaults to 1.

    Returns:
        Single `Tensor` if one `x` was passed, else sequence of tensors.
    &#34;&#34;&#34;
    assert isinstance(y, NativeTensor), f&#34;{type(y)}&#34;
    if len(x) == 0:
        x = _PARAM_STACK[-1]
    backend = choose_backend_t(y, *x)
    x_natives = sum([x_._natives() for x_ in x], ())
    native_gradients = list(backend.gradients(y.native(), x_natives, grad_y.native() if grad_y is not None else None))
    for i, grad in enumerate(native_gradients):
        assert grad is not None, f&#34;Missing gradient for source with shape {x_natives[i].shape}&#34;
    grads = [x_._op1(lambda native: native_gradients.pop(0)) for x_ in x]
    return grads[0] if len(x) == 1 else grads</code></pre>
</details>
</dd>
<dt id="phi.math.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid_sample(grid: Tensor, coordinates: Tensor, extrap: &#39;extrapolation.Extrapolation&#39;):
    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap), [grid, coordinates])
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>k: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft(k: Tensor):
    native, assemble = _invertible_standard_form(k)
    result = choose_backend(native).ifft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>complex: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def imag(complex: Tensor) -&gt; Tensor:
    return _backend_op1(complex, Backend.imag)</code></pre>
</details>
</dd>
<dt id="phi.math.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isfinite(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.isfinite)</code></pre>
</details>
</dd>
<dt id="phi.math.join_dimensions"><code class="name flex">
<span>def <span class="ident">join_dimensions</span></span>(<span>value: phi.math._tensors.Tensor, dims: phi.math._shape.Shape, joined_dim_name: str, pos: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses multiple dimensions into a single dimension by concatenating the elements.
Elements along the new dimensions are laid out according to the order of <code>dims</code>.
If the order of <code>dims</code> differs from the current dimension order, the tensor is transposed accordingly.</p>
<p>The type of the new dimension will be equal to the types of <code>dims</code>.
If <code>dims</code> have varying types, the new dimension will be a batch dimension.</p>
<p>See Also:
<code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor containing the dimensions <code>dims</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to be compressed in the specified order.</dd>
<dt><strong><code>joined_dim_name</code></strong></dt>
<dd>Name of the new dimension.</dd>
<dt><strong><code>pos</code></strong></dt>
<dd>Index of new dimension. <code>None</code> for automatic, <code>-1</code> for last, <code>0</code> for first.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with compressed shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def join_dimensions(value: Tensor,
                    dims: Shape or tuple or list,
                    joined_dim_name: str,
                    pos: int or None = None):
    &#34;&#34;&#34;
    Compresses multiple dimensions into a single dimension by concatenating the elements.
    Elements along the new dimensions are laid out according to the order of `dims`.
    If the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.

    The type of the new dimension will be equal to the types of `dims`.
    If `dims` have varying types, the new dimension will be a batch dimension.

    See Also:
        `split_dimension()`

    Args:
        value: Tensor containing the dimensions `dims`.
        dims: Dimensions to be compressed in the specified order.
        joined_dim_name: Name of the new dimension.
        pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.

    Returns:
        `Tensor` with compressed shape.
    &#34;&#34;&#34;
    if len(dims) == 0:
        return CollapsedTensor(value, value.shape.expand(1, joined_dim_name, _infer_dim_type_from_name(joined_dim_name), pos))
    order = value.shape.order_group(dims)
    native = value.native(order)
    types = value.shape.get_type(dims)
    dim_type = types[0] if len(set(types)) == 1 else BATCH_DIM
    if pos is None:
        pos = min(value.shape.indices(dims))
    new_shape = value.shape.without(dims).expand(value.shape.only(dims).volume, joined_dim_name, dim_type, pos)
    native = choose_backend(native).reshape(native, new_shape.sizes)
    return NativeTensor(native, new_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.l1_loss"><code class="name flex">
<span>def <span class="ident">l1_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, batch_norm=True, reduce_batches=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get L1 loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
<dt><strong><code>reduce_batches</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l1_loss(tensor: Tensor, batch_norm=True, reduce_batches=True):
    &#34;&#34;&#34;
    get L1 loss

    Args:
      tensor: Tensor: 
      batch_norm:  (Default value = True)
      reduce_batches:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    if struct.isstruct(tensor):
        all_tensors = struct.flatten(tensor)
        return sum(l1_loss(tensor, batch_norm, reduce_batches) for tensor in all_tensors)
    if reduce_batches:
        total_loss = math.sum_(math.abs(tensor))
    else:
        total_loss = math.sum_(math.abs(tensor), dim=list(range(1, len(tensor.shape))))
    if batch_norm and reduce_batches:
        batch_size = tensor.shape.sizes[0]
        return math.divide_no_nan(total_loss, math.to_float(batch_size))
    else:
        return total_loss</code></pre>
</details>
</dd>
<dt id="phi.math.l2_loss"><code class="name flex">
<span>def <span class="ident">l2_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, batch_norm=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get L2 loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l2_loss(tensor: Tensor, batch_norm=True):
    &#34;&#34;&#34;
    get L2 loss

    Args:
      tensor: Tensor: 
      batch_norm:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    return l_n_loss(tensor, 2, batch_norm=batch_norm)</code></pre>
</details>
</dd>
<dt id="phi.math.l_n_loss"><code class="name flex">
<span>def <span class="ident">l_n_loss</span></span>(<span>tensor: phi.math._tensors.Tensor, n: int, batch_norm=True)</span>
</code></dt>
<dd>
<div class="desc"><p>get Ln loss</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensor</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>n</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>(Default value = True)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l_n_loss(tensor: Tensor, n: int, batch_norm=True):
    &#34;&#34;&#34;
    get Ln loss

    Args:
      tensor: Tensor: 
      n: int: 
      batch_norm:  (Default value = True)

    Returns:

    &#34;&#34;&#34;
    assert isinstance(tensor, Tensor), f&#34;Must be a Tensor but got {type(tensor).__name__}&#34;
    total_loss = math.sum_(tensor ** n) / n
    if batch_norm:
        batch_size = tensor.shape.batch.volume
        return math.divide_no_nan(total_loss, batch_size)
    else:
        return total_loss</code></pre>
</details>
</dd>
<dt id="phi.math.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>x: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor = 1, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Laplace operator as defined for scalar fields.
If a vector field is passed, the laplace is computed component-wise.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional field of shape (batch, spacial dimensions&hellip;, components)</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>scalar or 1d tensor</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>The second derivative along these dimensions is summed over</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or float:
(Default value = 1)</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace(x: Tensor,
            dx: Tensor or float = 1,
            padding: Extrapolation = extrapolation.BOUNDARY,
            dims: tuple or None = None):
    &#34;&#34;&#34;
    Spatial Laplace operator as defined for scalar fields.
    If a vector field is passed, the laplace is computed component-wise.

    Args:
      x: n-dimensional field of shape (batch, spacial dimensions..., components)
      dx: scalar or 1d tensor
      padding: extrapolation
      dims: The second derivative along these dimensions is summed over
      x: Tensor: 
      dx: Tensor or float:  (Default value = 1)
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      tensor of same shape

    &#34;&#34;&#34;
    if not isinstance(dx, (int, float)):
        dx = wrap(dx, names=&#39;_laplace&#39;)
    if isinstance(x, Extrapolation):
        return x.gradient()
    left, center, right = shift(wrap(x), (-1, 0, 1), dims, padding, stack_dim=&#39;_laplace&#39;)
    result = (left + right - 2 * center) / dx
    result = math.sum_(result, &#39;_laplace&#39;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>start, stop, number: int, dim='linspace')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace(start, stop, number: int, dim=&#39;linspace&#39;):
    native = choose_backend(start, stop, number, prefer_default=True).linspace(start, stop, number)
    return NativeTensor(native, shape_(**{dim: number}))</code></pre>
</details>
</dd>
<dt id="phi.math.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>function, *values: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>function</code> on all elements of <code>value</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to be called on single elements contained in <code>value</code>. Must return a value that can be stored in tensors.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to iterate over. Number of tensors must match <code>function</code> signature.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_(function, *values: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Calls `function` on all elements of `value`.

    Args:
        function: Function to be called on single elements contained in `value`. Must return a value that can be stored in tensors.
        values: Tensors to iterate over. Number of tensors must match `function` signature.

    Returns:
        `Tensor` of same shape as `value`.
    &#34;&#34;&#34;
    shape = combine_safe(*[v.shape for v in values])
    values_reshaped = [CollapsedTensor(v, shape) for v in values]
    flat = [flatten(v) for v in values_reshaped]
    result = []
    for items in zip(*flat):
        result.append(function(*items))
    return wrap(result).vector.split(shape)</code></pre>
</details>
</dd>
<dt id="phi.math.matmul"><code class="name flex">
<span>def <span class="ident">matmul</span></span>(<span>A, b) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def matmul(A, b) -&gt; Tensor:
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.max(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.mean(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>**dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>generate a TensorStack meshgrid from keyword dimensions</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(**dimensions):
    &#34;&#34;&#34;
    generate a TensorStack meshgrid from keyword dimensions

    Args:
      **dimensions: 

    Returns:

    &#34;&#34;&#34;
    assert &#39;vector&#39; not in dimensions
    dim_values = {}
    for dim, spec in dimensions.items():
        if isinstance(spec, int):
            dim_values[dim] = tuple(range(spec))
        elif isinstance(spec, Tensor):
            dim_values[dim] = spec.native()
        else:
            dim_values[dim] = spec
    backend = choose_backend(*dim_values.values(), prefer_default=True)
    indices_list = backend.meshgrid(*dim_values.values())
    single_shape = Shape([len(val) for val in dim_values.values()], dim_values.keys(), [SPATIAL_DIM] * len(dim_values))
    channels = [NativeTensor(t, single_shape) for t in indices_list]
    return TensorStack(channels, &#39;vector&#39;, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def min_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.min(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.minimize"><code class="name flex">
<span>def <span class="ident">minimize</span></span>(<span>function, x0: phi.math._tensors.Tensor, solve_params: phi.math.backend._optim.Solve) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Finds a minimum of the scalar function <code>function(x)</code> where <code>x</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> like <code>x0</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to be minimized</dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial guess for <code>x</code></dd>
<dt><strong><code>solve_params</code></strong></dt>
<dd>Specifies solver type and parameters. Additional solve information will be stored in <code>solve_params.result</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>converged</code></dt>
<dd>scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations</dd>
<dt><code>x</code></dt>
<dd>solution, the minimum point <code>x</code>.</dd>
<dt><code>iterations</code></dt>
<dd>number of iterations performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimize(function, x0: Tensor, solve_params: Solve) -&gt; Tuple[Tensor, Tensor, Tensor]:
    &#34;&#34;&#34;
    Finds a minimum of the scalar function `function(x)` where `x` is a `Tensor` like `x0`.

    Args:
        function: Function to be minimized
        x0: Initial guess for `x`
        solve_params: Specifies solver type and parameters. Additional solve information will be stored in `solve_params.result`.

    Returns:
        converged: scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations
        x: solution, the minimum point `x`.
        iterations: number of iterations performed
    &#34;&#34;&#34;
    backend = choose_backend_t(x0)
    x0._expand()
    natives = x0._natives()
    natives_flat = [backend.flatten(n) for n in natives]
    x0_flat = backend.concat(natives_flat, 0)

    def unflatten_assemble(x_native):
        i = 0
        x_natives = []
        for native, native_flat in zip(natives, natives_flat):
            vol = backend.shape(native_flat)[0]
            x_natives.append(backend.reshape(x_native[i:i + vol], backend.shape(native)))
            i += vol
        x = x0._op1(lambda _: x_natives.pop(0))  # assemble x0 structure
        return x

    def native_function(native_x):
        x = unflatten_assemble(native_x)
        y = function(x)
        return y.native()

    x_native = backend.minimize(native_function, x0_flat, solve_params)
    x = unflatten_assemble(x_native)
    return wrap(solve_params.result.success), x, wrap(solve_params.result.iterations)</code></pre>
</details>
</dd>
<dt id="phi.math.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>value: phi.math._tensors.Tensor, list_dim='nonzero', index_dim='vector')</span>
</code></dt>
<dd>
<div class="desc"><p>Get spatial indices of non-zero / True values.</p>
<p>Batch dimensions are preserved by this operation.
If channel dimensions are present, this method returns the indices where any entry is nonzero.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>spatial tensor to find non-zero / True values in.</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing non-zero values (Default value = 'nonzero')</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>name of index dimension (Default value = 'vector')</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch dims&hellip;, list_dim=#non-zero, index_dim=value.shape.spatial_rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonzero(value: Tensor, list_dim=&#39;nonzero&#39;, index_dim=&#39;vector&#39;):
    &#34;&#34;&#34;
    Get spatial indices of non-zero / True values.
    
    Batch dimensions are preserved by this operation.
    If channel dimensions are present, this method returns the indices where any entry is nonzero.

    Args:
      value: spatial tensor to find non-zero / True values in.
      list_dim: name of dimension listing non-zero values (Default value = &#39;nonzero&#39;)
      index_dim: name of index dimension (Default value = &#39;vector&#39;)
      value: Tensor: 

    Returns:
      tensor of shape (batch dims..., list_dim=#non-zero, index_dim=value.shape.spatial_rank)

    &#34;&#34;&#34;
    if value.shape.channel_rank &gt; 0:
        value = sum_(abs(value), value.shape.channel.names)

    def unbatched_nonzero(value):
        native = value.native()
        backend = choose_backend(native)
        indices = backend.nonzero(native)
        indices_shape = Shape(backend.staticshape(indices), (list_dim, index_dim), (BATCH_DIM, CHANNEL_DIM))
        return NativeTensor(indices, indices_shape)

    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)</code></pre>
</details>
</dd>
<dt id="phi.math.normalize_to"><code class="name flex">
<span>def <span class="ident">normalize_to</span></span>(<span>target: phi.math._tensors.Tensor, source: phi.math._tensors.Tensor, epsilon=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies the target so that its total content matches the source.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd>a tensor</dd>
<dt><strong><code>source</code></strong></dt>
<dd>a tensor or number</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>small number to prevent division by zero or None. (Default value = 1e-5)</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>source</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>normalized tensor of the same shape as target</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_to(target: Tensor, source: Tensor, epsilon=1e-5):
    &#34;&#34;&#34;
    Multiplies the target so that its total content matches the source.

    Args:
      target: a tensor
      source: a tensor or number
      epsilon: small number to prevent division by zero or None. (Default value = 1e-5)
      target: Tensor: 
      source: Tensor: 

    Returns:
      normalized tensor of the same shape as target

    &#34;&#34;&#34;
    target_total = math.sum_(target, dim=target.shape.non_batch.names)
    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
    source_total = math.sum_(source, dim=source.shape.non_batch.names)
    return target * (source_total / denominator)</code></pre>
</details>
</dd>
<dt id="phi.math.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 1 / True everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 1 / True everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().ones((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>tensor: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_like(tensor: Tensor):
    return zeros(tensor.shape, dtype=tensor.dtype) + 1</code></pre>
</details>
</dd>
<dt id="phi.math.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>value: phi.math._tensors.Tensor, widths: dict, mode: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.</p>
<p>This is equivalent to calling <code>mode.pad(value, widths)</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Extrapolation object</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
<dt><strong><code>mode</code></strong></dt>
<dd>'extrapolation.Extrapolation': </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>padded Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(value: Tensor, widths: dict, mode: &#39;extrapolation.Extrapolation&#39;) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation.
    
    This is equivalent to calling `mode.pad(value, widths)`.

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)
      mode: Extrapolation object
      value: Tensor: 
      widths: dict: 
      mode: &#39;extrapolation.Extrapolation&#39;: 

    Returns:
      padded Tensor

    &#34;&#34;&#34;
    return mode.pad(value, widths)</code></pre>
</details>
</dd>
<dt id="phi.math.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision for the local context.

    Usage: `with precision(p):`

    This overrides the global setting, see `set_global_precision()`.

    Args:
        floating_point_bits: 16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    _PRECISION.append(floating_point_bits)
    try:
        yield None
    finally:
        _PRECISION.pop(-1)</code></pre>
</details>
</dd>
<dt id="phi.math.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>value: phi.math._tensors.Tensor = None, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tensor with no more than two spatial dimensions, splitting it along all batch and channel dimensions.</p>
<p>Unlike regular printing, the primary dimension, typically x, is oriented to the right.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>name of the tensor</dd>
<dt><strong><code>value</code></strong></dt>
<dd>tensor-like</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor:
(Default value = None)</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str:
(Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_(value: Tensor = None, name: str = None):
    &#34;&#34;&#34;
    Print a tensor with no more than two spatial dimensions, splitting it along all batch and channel dimensions.
    
    Unlike regular printing, the primary dimension, typically x, is oriented to the right.

    Args:
      name: name of the tensor
      value: tensor-like
      value: Tensor:  (Default value = None)
      name: str:  (Default value = None)

    Returns:

    &#34;&#34;&#34;
    if value is None:
        print()
        return
    if name is not None:
        print(&#34; &#34; * 16 + name)
    value = wrap(value)
    dim_order = tuple(sorted(value.shape.spatial.names, reverse=True))
    if value.shape.spatial_rank == 0:
        print(value.numpy())
    elif value.shape.spatial_rank == 1:
        for index_dict in value.shape.non_spatial.meshgrid():
            if value.shape.non_spatial.volume &gt; 1:
                print(f&#34;--- {&#39;, &#39;.join(&#39;%s=%d&#39; % (name, idx) for name, idx in index_dict.items())} ---&#34;)
            text = np.array2string(value[index_dict].numpy(dim_order), precision=2, separator=&#39;, &#39;, max_line_width=np.inf)
            print(&#39; &#39; + re.sub(&#39;[\[\]]&#39;, &#39;&#39;, text))
    elif value.shape.spatial_rank == 2:
        for index_dict in value.shape.non_spatial.meshgrid():
            if value.shape.non_spatial.volume &gt; 1:
                print(f&#34;--- {&#39;, &#39;.join(&#39;%s=%d&#39; % (name, idx) for name, idx in index_dict.items())} ---&#34;)
            text = np.array2string(value[index_dict].numpy(dim_order), precision=2, separator=&#39;, &#39;, max_line_width=np.inf)
            print(&#39; &#39; + re.sub(&#39;[\[\]]&#39;, &#39;&#39;, re.sub(&#39;\],&#39;, &#39;&#39;, text)))
    else:
        raise NotImplementedError(&#39;Can only print tensors with up to 2 spatial dimensions.&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.prod(native, dim),
                   collapsed_function=lambda inner, red_shape: inner ** red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_normal(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_normal, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_uniform(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_uniform(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_uniform, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>complex: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def real(complex: Tensor) -&gt; Tensor:
    return _backend_op1(complex, Backend.real)</code></pre>
</details>
</dd>
<dt id="phi.math.record_gradients"><code class="name flex">
<span>def <span class="ident">record_gradients</span></span>(<span>*x: phi.math._tensors.Tensor, persistent=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Context expression to record gradients for operations within that directly or indirectly depend on <code>x</code>.</p>
<p>The function <code><a title="phi.math.gradients" href="#phi.math.gradients">gradients()</a></code> may be called within the context to evaluate the gradients of a Tensor derived from <code>x</code> w.r.t. <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*x</code></strong></dt>
<dd>Parameters for which gradients of the form dL/dx may be computed</dd>
<dt><strong><code>persistent</code></strong></dt>
<dd>if <code>False</code>, <code><a title="phi.math.gradients" href="#phi.math.gradients">gradients()</a></code> may only be called once within the context</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def record_gradients(*x: Tensor, persistent=False):
    &#34;&#34;&#34;
    Context expression to record gradients for operations within that directly or indirectly depend on `x`.

    The function `gradients()` may be called within the context to evaluate the gradients of a Tensor derived from `x` w.r.t. `x`.

    Args:
        *x: Parameters for which gradients of the form dL/dx may be computed
        persistent: if `False`, `gradients()` may only be called once within the context
    &#34;&#34;&#34;
    for x_ in x:
        x_._expand()
    natives = sum([x_._natives() for x_ in x], ())
    backend = choose_backend(*natives)
    ctx = backend.record_gradients(natives, persistent=persistent)
    _PARAM_STACK.append(x)
    ctx.__enter__()
    try:
        yield None
    finally:
        ctx.__exit__(None, None, None)
        _PARAM_STACK.pop(0)</code></pre>
</details>
</dd>
<dt id="phi.math.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.round)</code></pre>
</details>
</dd>
<dt id="phi.math.sample_subgrid"><code class="name flex">
<span>def <span class="ident">sample_subgrid</span></span>(<span>grid: phi.math._tensors.Tensor, start: phi.math._tensors.Tensor, size: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a sub-grid from <code>grid</code> with equal distance between sampling points.
The values at the new sample points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid to be resampled</dd>
<dt><strong><code>start</code></strong></dt>
<dd>origin point of sub-grid within <code>grid</code>, measured in number of cells.</dd>
</dl>
<p>Must have a single dimension called <code>vector</code>.
Example: <code>start=(1, 0.5)</code> would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
The order of dims must be equal to <code>size</code> and <code>grid.shape.spatial</code>.
size: resolution of the sub-grid. Must not be larger than the resolution of <code>grid</code>.
The order of dims must be equal to <code>start</code> and <code>grid.shape.spatial</code>.
grid: Tensor:
start: Tensor:
size: Shape: </p>
<h2 id="returns">Returns</h2>
<p>sampled sub-grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Samples a sub-grid from `grid` with equal distance between sampling points.
    The values at the new sample points are determined via linear interpolation.

    Args:
      grid: full size grid to be resampled
      start: origin point of sub-grid within `grid`, measured in number of cells.
    Must have a single dimension called `vector`.
    Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
    The order of dims must be equal to `size` and `grid.shape.spatial`.
      size: resolution of the sub-grid. Must not be larger than the resolution of `grid`.
    The order of dims must be equal to `start` and `grid.shape.spatial`.
      grid: Tensor: 
      start: Tensor: 
      size: Shape: 

    Returns:
      sampled sub-grid

    &#34;&#34;&#34;
    assert start.shape.names == (&#39;vector&#39;,)
    assert grid.shape.spatial.names == size.names
    discard = {}
    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size):
        discard[dim] = slice(int(d_start), int(d_start) + d_size + (1 if d_start != 0 else 0))
    grid = grid[discard]
    upper_weight = start % 1
    lower_weight = 1 - upper_weight
    for i, dim in enumerate(grid.shape.spatial.names):
        if upper_weight[i] not in (0, 1):
            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
            grid = upper * upper_weight[i] + lower * lower_weight[i]
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>indices: phi.math._tensors.Tensor, values: phi.math._tensors.Tensor, size: phi.math._shape.Shape, scatter_dims: str, duplicates_handling: str = 'undefined', outside_handling: str = 'discard')</span>
</code></dt>
<dd>
<div class="desc"><p>Create a dense tensor from sparse values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>indices</code></strong></dt>
<dd>n-dimensional indices corresponding to values</dd>
<dt><strong><code>values</code></strong></dt>
<dd>values to scatter at indices</dd>
<dt><strong><code>size</code></strong></dt>
<dd>spatial size of dense tensor</dd>
<dt><strong><code>scatter_dims</code></strong></dt>
<dd>dimensions of values/indices to reduce during scattering</dd>
<dt><strong><code>duplicates_handling</code></strong></dt>
<dd>one of ('undefined', 'add', 'mean', 'any') (Default value = 'undefined')</dd>
<dt><strong><code>outside_handling</code></strong></dt>
<dd>one of ('discard', 'clamp', 'undefined') (Default value = 'discard')</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of shape <code>size</code> and dtype matching <code>values</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scatter(indices: Tensor,
            values: Tensor,
            size: Shape,
            scatter_dims: str or tuple or list or &#39;Shape&#39;,
            duplicates_handling: str = &#39;undefined&#39;,
            outside_handling: str = &#39;discard&#39;):
    &#34;&#34;&#34;
    Create a dense tensor from sparse values.

    Args:
        indices: n-dimensional indices corresponding to values
        values: values to scatter at indices
        size: spatial size of dense tensor
        scatter_dims: dimensions of values/indices to reduce during scattering
        duplicates_handling: one of (&#39;undefined&#39;, &#39;add&#39;, &#39;mean&#39;, &#39;any&#39;) (Default value = &#39;undefined&#39;)
        outside_handling: one of (&#39;discard&#39;, &#39;clamp&#39;, &#39;undefined&#39;) (Default value = &#39;discard&#39;)

    Returns:
        Tensor of shape `size` and dtype matching `values`.
    &#34;&#34;&#34;
    indices_ = indices.native()
    values_ = values.native(values.shape.combined(indices.shape.non_channel).names)
    backend = choose_backend(indices_, values_)
    result_ = backend.scatter(indices_, values_, tuple(size), duplicates_handling=duplicates_handling, outside_handling=outside_handling)
    result_shape = size &amp; indices.shape.batch &amp; values.shape.non_spatial
    result_shape = result_shape.without(scatter_dims)
    return NativeTensor(result_, result_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.

    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
    Operations may also convert floating point values to this precision, even if the input had a different precision.

    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
    The output of math operations has the same precision as its inputs.

    Args:
      floating_point_bits: one of (16, 32, 64, None)
    &#34;&#34;&#34;
    _PRECISION[0] = floating_point_bits</code></pre>
</details>
</dd>
<dt id="phi.math.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>**dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape from the dimension names and their respective sizes.</p>
<p>Dimension types are inferred from the names according to the following rules:</p>
<ul>
<li>single letter -&gt; spatial dimension</li>
<li>starts with 'vector' -&gt; channel dimension</li>
<li>else -&gt; batch dimension</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>names -&gt; size</dd>
<dt><strong><code>**dims</code></strong></dt>
<dd>int: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shape(**dims: int) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape from the dimension names and their respective sizes.
    
    Dimension types are inferred from the names according to the following rules:
    
    * single letter -&gt; spatial dimension
    * starts with &#39;vector&#39; -&gt; channel dimension
    * else -&gt; batch dimension

    Args:
      dims: names -&gt; size
      **dims: int: 

    Returns:
      Shape

    &#34;&#34;&#34;
    types = []
    for name, size in dims.items():
        types.append(_infer_dim_type_from_name(name))
    return Shape(dims.values(), dims.keys(), types)</code></pre>
</details>
</dd>
<dt id="phi.math.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>x: phi.math._tensors.Tensor, offsets: tuple, dims: tuple = None, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, stack_dim: str = 'shift') ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>shift Tensor by a fixed offset and abiding by extrapolation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input data</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Shift size</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to shift, defaults to None</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>padding to be performed at the boundary, defaults to extrapolation.BOUNDARY</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>dimensions to be stacked, defaults to 'shift'</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>tuple: </dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation or None:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>str or None:
(Default value = 'shift')</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>offset_tensor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shift(x: Tensor,
          offsets: tuple,
          dims: tuple or None = None,
          padding: Extrapolation or None = extrapolation.BOUNDARY,
          stack_dim: str or None = &#39;shift&#39;) -&gt; list:
    &#34;&#34;&#34;
    shift Tensor by a fixed offset and abiding by extrapolation

    Args:
      x: Input data
      offsets: Shift size
      dims: Dimensions along which to shift, defaults to None
      padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
      stack_dim: dimensions to be stacked, defaults to &#39;shift&#39;
      x: Tensor: 
      offsets: tuple: 
      dims: tuple or None:  (Default value = None)
      padding: Extrapolation or None:  (Default value = extrapolation.BOUNDARY)
      stack_dim: str or None:  (Default value = &#39;shift&#39;)

    Returns:
      list: offset_tensor

    &#34;&#34;&#34;
    if stack_dim is None:
        assert len(dims) == 1
    x = wrap(x)
    dims = dims if dims is not None else x.shape.spatial.names
    pad_lower = max(0, -min(offsets))
    pad_upper = max(0, max(offsets))
    if padding is not None:
        x = math.pad(x, {axis: (pad_lower, pad_upper) for axis in dims}, mode=padding)
    offset_tensors = []
    for offset in offsets:
        components = []
        for dimension in dims:
            slices = {dim: slice(pad_lower + offset, -pad_upper + offset) if dim == dimension else slice(pad_lower, -pad_upper) for dim in dims}
            slices = {dim: slice(sl.start, sl.stop if sl.stop &lt; 0 else None) for dim, sl in slices.items()}  # replace stop=0 by stop=None
            components.append(x[slices])
        offset_tensors.append(channel_stack(components, stack_dim) if stack_dim is not None else components[0])
    return offset_tensors</code></pre>
</details>
</dd>
<dt id="phi.math.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sign(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sign)</code></pre>
</details>
</dd>
<dt id="phi.math.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sin(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sin)</code></pre>
</details>
</dd>
<dt id="phi.math.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>operator, y: phi.math._tensors.Tensor, x0: phi.math._tensors.Tensor, solve_params: phi.math.backend._optim.Solve, constants: tuple = (), callback=None) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the system of linear or nonlinear equations <em>operator · x = y</em>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>operator</code></strong></dt>
<dd>Function <code>operator(x)</code> or matrix</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>operator · x</code></dd>
<dt><strong><code>x0</code></strong></dt>
<dd>Initial guess for <code>x</code></dd>
<dt><strong><code>solve_params</code></strong></dt>
<dd>Specifies solver type and parameters. Additional solve information will be stored in <code>solve_params.result</code>.</dd>
<dt><strong><code>callback</code></strong></dt>
<dd>Function to be called after each iteration as <code>callback(x_n)</code>. <em>This argument may be ignored by some backends.</em></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>converged</code></dt>
<dd>scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations</dd>
<dt><code>x</code></dt>
<dd>solution of the linear system of equations <code>operator · x = y</code>.</dd>
<dt><code>iterations</code></dt>
<dd>number of iterations performed</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(operator,
          y: Tensor,
          x0: Tensor,
          solve_params: Solve,
          constants: tuple or list = (),
          callback=None) -&gt; Tuple[Tensor, Tensor, Tensor]:
    &#34;&#34;&#34;
    Solves the system of linear or nonlinear equations *operator · x = y*.

    Args:
        operator: Function `operator(x)` or matrix
        y: Desired output of `operator · x`
        x0: Initial guess for `x`
        solve_params: Specifies solver type and parameters. Additional solve information will be stored in `solve_params.result`.
        callback: Function to be called after each iteration as `callback(x_n)`. *This argument may be ignored by some backends.*

    Returns:
        converged: scalar bool tensor representing whether the solve found a solution within the specified accuracy within the allowed iterations
        x: solution of the linear system of equations `operator · x = y`.
        iterations: number of iterations performed
    &#34;&#34;&#34;
    if not isinstance(solve_params, LinearSolve):
        from ._nd import l2_loss

        def min_func(x):
            diff = operator(x) - y
            l2 = l2_loss(diff)
            return l2

        rel_tol_to_abs = solve_params.relative_tolerance * l2_loss(y, batch_norm=True)
        solve_params.absolute_tolerance = rel_tol_to_abs
        solve_params.relative_tolerance = None
        return minimize(min_func, x0, solve_params=solve_params)
    if solve_params.solver not in (None, &#39;CG&#39;):
        raise NotImplementedError(&#34;Only &#39;CG&#39; solver currently supported&#34;)

    for c in constants:
        c._expand()

    from ._track import lin_placeholder, ShiftLinOp
    x0, y = tensors(x0, y)
    backend = choose_backend(*x0._natives(), *y._natives())
    batch = (y.shape &amp; x0.shape).batch
    x0_native = backend.reshape(x0.native(), (x0.shape.batch.volume, -1))
    y_native = backend.reshape(y.native(), (y.shape.batch.volume, -1))

    if callable(operator):
        operator_or_matrix = None
        if solve_params.bake == &#39;sparse&#39;:
            track_time = time.perf_counter()
            x_track = lin_placeholder(x0)
            Ax_track = operator(x_track)
            assert isinstance(Ax_track, ShiftLinOp), &#39;Baking sparse matrix failed. Make sure only supported linear operations are used.&#39;
            track_time = time.perf_counter() - track_time
            build_time = time.perf_counter()
            try:
                operator_or_matrix = Ax_track.build_sparse_coordinate_matrix()
            except NotImplementedError as err:
                warnings.warn(f&#34;Failed to build sparse matrix, using function directly. {err}&#34;)
            # TODO reshape x0, y so that independent dimensions are batch
            build_time = time.perf_counter() - build_time
        if operator_or_matrix is None:
            def operator_or_matrix(native_x):
                native_x_shaped = backend.reshape(native_x, x0.shape.non_batch.sizes)
                x = NativeTensor(native_x_shaped, x0.shape.non_batch)
                Ax = operator(x)
                Ax_native = backend.reshape(Ax.native(), backend.shape(native_x))
                return Ax_native
    else:
        operator_or_matrix = backend.reshape(operator.native(), (y.shape.non_batch.volume, x0.shape.non_batch.volume))

    loop_time = time.perf_counter()
    x = backend.conjugate_gradient(operator_or_matrix, y_native, x0_native, solve_params, &#39;implicit&#39;, callback)
    converged = solve_params.result.success
    iterations = solve_params.result.iterations
    loop_time = time.perf_counter() - loop_time
    if get_current_profile():
        info = &#34;  \tProfile with trace=False to get more accurate results.&#34; if get_current_profile()._trace else &#34;&#34;
        get_current_profile().add_external_message(f&#34;CG   track: {round(track_time * 1000)} ms  \tbuild: {round(build_time * 1000)} ms  \tloop: {round(loop_time * 1000)} ms / {iterations} iterations {info}&#34;)
    x = backend.reshape(x, batch.sizes + x0.shape.non_batch.sizes)
    return NativeTensor(converged, EMPTY_SHAPE), NativeTensor(x, batch.combined(x0.shape.non_batch)), NativeTensor(iterations, EMPTY_SHAPE)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_pad"><code class="name flex">
<span>def <span class="ident">spatial_pad</span></span>(<span>value, pad_width: tuple, mode: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a>) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_pad(value, pad_width: tuple or list, mode: &#39;extrapolation.Extrapolation&#39;) -&gt; Tensor:
    value = wrap(value)
    return pad(value, {n: w for n, w in zip(value.shape.spatial.names, pad_width)}, mode=mode)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_shape"><code class="name flex">
<span>def <span class="ident">spatial_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'spatial'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;spatial&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_stack"><code class="name flex">
<span>def <span class="ident">spatial_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_stack(values, dim: str):
    return _stack(values, dim, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_sum"><code class="name flex">
<span>def <span class="ident">spatial_sum</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_sum(value: Tensor):
    return math.sum_(value, dim=value.shape.spatial.names)</code></pre>
</details>
</dd>
<dt id="phi.math.split_dimension"><code class="name flex">
<span>def <span class="ident">split_dimension</span></span>(<span>dim: phi.math._tensors.TensorDim, split_dims: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses a tensor dimension by unstacking the elements along it.
The compressed dimension <code>dim</code> is assumed to contain elements laid out according to the order or <code>split_dims</code>.</p>
<p>See Also:
<code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>Compressed dimension to be decompressed.</dd>
<dt><strong><code>split_dims</code></strong></dt>
<dd>Ordered new dimensions to replace <code>dim</code> as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with decompressed shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_dimension(dim: TensorDim, split_dims: Shape):
    &#34;&#34;&#34;
    Decompresses a tensor dimension by unstacking the elements along it.
    The compressed dimension `dim` is assumed to contain elements laid out according to the order or `split_dims`.

    See Also:
        `join_dimensions()`

    Args:
        dim: Compressed dimension to be decompressed.
        split_dims: Ordered new dimensions to replace `dim` as `Shape`.

    Returns:
        `Tensor` with decompressed shape
    &#34;&#34;&#34;
    if split_dims.rank == 0:
        return dim[0]
    if split_dims.rank == 1:
        value = dim.tensor
        new_shape = value.shape.with_names([split_dims.name if name == dim.name else name for name in value.shape.names])
        return value._with_shape_replaced(new_shape)
    else:
        value = dim.tensor
        native = value.native()
        new_shape = value.shape.without(dim.name)
        i = dim.index
        for size, name, dim_type in split_dims.dimensions:
            new_shape = new_shape.expand(size, name, dim_type, pos=i)
            i += 1
        native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
        return NativeTensor(native_reshaped, new_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.sqrt)</code></pre>
</details>
</dd>
<dt id="phi.math.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.std(native, dim),
                   collapsed_function=lambda inner, red_shape: inner,
                   unaffected_function=lambda value: value * 0)</code></pre>
</details>
</dd>
<dt id="phi.math.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Disables gradients for the given tensor.
This may switch off the gradients for <code>value</code> itself or create a copy of <code>value</code> with disabled gradients.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor for which gradients should be disabled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>value</code> or <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_gradient(value: Tensor):
    &#34;&#34;&#34;
    Disables gradients for the given tensor.
    This may switch off the gradients for `value` itself or create a copy of `value` with disabled gradients.

    Args:
        value: tensor for which gradients should be disabled.

    Returns:
        Copy of `value` or `value`.
    &#34;&#34;&#34;
    return value._op1(lambda native: choose_backend(native).stop_gradient(native))</code></pre>
</details>
</dd>
<dt id="phi.math.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.sum(native, dim),
                   collapsed_function=lambda inner, red_shape: inner * red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.tensor"><code class="name flex">
<span>def <span class="ident">tensor</span></span>(<span>data: phi.math._tensors.Tensor, names: str = None, convert: bool = True) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a Tensor from the specified <code>data</code>.
If <code>convert=True</code>, converts <code>data</code> to the preferred format of the default backend.</p>
<p><code>data</code> must be one of the following:</p>
<ul>
<li>Number: returns a dimensionless Tensor.</li>
<li>Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.</li>
<li><code>tuple</code> or <code>list</code> of numbers: backs the Tensor with native tensor.</li>
<li><code>tuple</code> or <code>list</code> of non-numbers: creates tensors for the items and stacks them.</li>
<li>Tensor: renames dimensions and dimension types if <code>names</code> is specified. Converts all internal native values of the tensor if <code>convert=True</code>.</li>
<li>Shape: creates a 1D tensor listing the dimension sizes.</li>
</ul>
<p>While specifying <code>names</code> is optional in some cases, it is recommended to always specify them.</p>
<p>Dimension types are always inferred from the dimension names if specified.</p>
<p>See Also:
<code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> which uses <code>convert=False</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>native tensor, scalar, sequence, Shape or Tensor</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Dimension names. Dimension types are inferred from the names.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>if dimension names are not provided and cannot automatically be inferred</dd>
<dt><code>ValueError</code></dt>
<dd>if <code>data</code> is not tensor-like</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor containing same values as data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensor(data: Tensor or Shape or tuple or list or numbers.Number,
         names: str or tuple or list = None,
         convert: bool = True) -&gt; Tensor:
    &#34;&#34;&#34;
    Create a Tensor from the specified `data`.
    If `convert=True`, converts `data` to the preferred format of the default backend.

    `data` must be one of the following:
    
    * Number: returns a dimensionless Tensor.
    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.
    * `tuple` or `list` of numbers: backs the Tensor with native tensor.
    * `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.
    * Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.
    * Shape: creates a 1D tensor listing the dimension sizes.
    
    While specifying `names` is optional in some cases, it is recommended to always specify them.
    
    Dimension types are always inferred from the dimension names if specified.

    See Also:
        `phi.math.wrap()` which uses `convert=False`.

    Args:
      data: native tensor, scalar, sequence, Shape or Tensor
      names: Dimension names. Dimension types are inferred from the names.
      convert: If True, converts the data to the native format of the current default backend.
        If False, wraps the data in a `Tensor` but keeps the given data reference if possible.

    Raises:
      AssertionError: if dimension names are not provided and cannot automatically be inferred
      ValueError: if `data` is not tensor-like

    Returns:
      Tensor containing same values as data
    &#34;&#34;&#34;
    if isinstance(data, Tensor):
        if convert:
            backend = choose_backend(*data._natives())
            if backend != default_backend():
                data = data._op1(lambda native: default_backend().as_tensor(backend.numpy(native), convert_external=True))
        if names is None:
            return data
        else:
            names = _shape.parse_dim_names(names, data.rank)
            names = [n if n is not None else o for n, o in zip(names, data.shape.names)]
            types = [_shape._infer_dim_type_from_name(n) if n is not None else o for n, o in zip(names, data.shape.types)]
            new_shape = Shape(data.shape.sizes, names, types)
            return data._with_shape_replaced(new_shape)
    elif isinstance(data, Shape):
        assert names is not None
        data = data.sizes
    elif isinstance(data, (numbers.Number, bool, str)):
        assert not names, f&#34;Trying to create a zero-dimensional Tensor from value &#39;{data}&#39; but names={names}&#34;
        if convert:
            data = default_backend().as_tensor(data, convert_external=True)
        return NativeTensor(data, EMPTY_SHAPE)
    if isinstance(data, (tuple, list)):
        array = np.array(data)
        if array.dtype != np.object:
            data = array
        else:
            elements = tensors(*data, names=None if names is None else names[1:], convert=convert)
            common_shape = _shape.combine_safe(*[e.shape for e in elements])
            rank = 1 + common_shape.rank
            stack_dim = &#39;vector&#39; if names is None else _shape.parse_dim_names(names, rank)[0]
            assert all(stack_dim not in t.shape for t in elements), f&#34;Cannot stack tensors with dimension &#39;{stack_dim}&#39; because a tensor already has that dimension.&#34;
            elements = [CollapsedTensor(e, common_shape) if e.shape.rank &lt; common_shape.rank else e for e in elements]
            from ._functions import cast_same
            elements = cast_same(*elements)
            return TensorStack(elements, dim_name=stack_dim, dim_type=_shape._infer_dim_type_from_name(stack_dim))
    backend = choose_backend(data, raise_error=False)
    if backend:
        if names is None:
            assert data.ndim &lt;= 1, &#34;Specify dimension names for tensors with more than 1 dimension&#34;
            names = [&#39;vector&#39;] * backend.ndims(data)  # [] or [&#39;vector&#39;]
            types = [CHANNEL_DIM] * backend.ndims(data)
        else:
            names = _shape.parse_dim_names(names, len(data.shape))
            assert None not in names, f&#34;All names must be specified but got {names}&#34;
            types = [_shape._infer_dim_type_from_name(n) for n in names]
        shape = Shape(data.shape, names, types)
        if convert and backend != default_backend():
            data = backend.numpy(data)
            data = default_backend().as_tensor(data, convert_external=True)
        return NativeTensor(data, shape)
    raise ValueError(f&#34;{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.tensors"><code class="name flex">
<span>def <span class="ident">tensors</span></span>(<span>*objects: phi.math._tensors.Tensor, names: str = None, convert: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code> on multiple arguments independently.</p>
<h2 id="example">Example</h2>
<p>scalar_tensor, vector_tensor = tensors(0, (1, 2, 3))</p>
<h2 id="returns">Returns</h2>
<p>Sequence of same length as <code>objects</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensors(*objects: Tensor or Shape or tuple or list or numbers.Number,
            names: str or tuple or list = None,
            convert: bool = True):
    &#34;&#34;&#34;
    Calls `tensor()` on multiple arguments independently.

    Example:

        scalar_tensor, vector_tensor = tensors(0, (1, 2, 3))

    Returns:
        Sequence of same length as `objects`.
    &#34;&#34;&#34;
    return [tensor(obj, names, convert) for obj in objects]</code></pre>
</details>
</dd>
<dt id="phi.math.tile"><code class="name flex">
<span>def <span class="ident">tile</span></span>(<span>value, multiples)</span>
</code></dt>
<dd>
<div class="desc"><p>Not yet implemented.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tile(value, multiples):
    &#34;&#34;&#34; Not yet implemented. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_complex(x: Tensor) -&gt; Tensor:
    return _backend_op1(x, Backend.to_complex)</code></pre>
</details>
</dd>
<dt id="phi.math.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>x: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>values to convert</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of same shape as <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_float(x: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Converts the given tensor to floating point format with the currently specified precision.
    
    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
    
    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html

    Args:
      x: values to convert
      x: Tensor: 

    Returns:
      Tensor of same shape as `x`

    &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_float)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int"><code class="name flex">
<span>def <span class="ident">to_int</span></span>(<span>x: phi.math._tensors.Tensor, int64=False) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int(x: Tensor, int64=False) -&gt; Tensor:
    return x._op1(lambda native: choose_backend(native).to_int(native, int64=int64))</code></pre>
</details>
</dd>
<dt id="phi.math.trace_function"><code class="name flex">
<span>def <span class="ident">trace_function</span></span>(<span>f: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Compiles a graph based on the function <code>f</code>.
This action might be performed immediately or when the traced function is called for the first time (JIT).</p>
<p>The traced function will compute the same result as <code>f</code> but may run much faster.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be traced.
All arguments must be of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> returning a single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or a <code>tuple</code> or <code>list</code> of tensors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>. However, the returned function does not support keyword arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trace_function(f: Callable) -&gt; Callable:
    &#34;&#34;&#34;
    Compiles a graph based on the function `f`.
    This action might be performed immediately or when the traced function is called for the first time (JIT).

    The traced function will compute the same result as `f` but may run much faster.

    Args:
        f: Function to be traced.
            All arguments must be of type `Tensor` returning a single `Tensor` or a `tuple` or `list` of tensors.

    Returns:
        Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
    &#34;&#34;&#34;
    INPUT_TENSORS = []
    OUTPUT_TENSORS = []

    def native_function(*natives):
        natives = list(natives)
        values = [t._op1(lambda _: natives.pop(0)) for t in INPUT_TENSORS]
        assert len(natives) == 0, &#34;Not all arguments were converted&#34;
        result = f(*values)
        results = [result] if not isinstance(result, (tuple, list)) else result
        OUTPUT_TENSORS.clear()
        OUTPUT_TENSORS.extend(results)
        return sum([v._natives() for v in results], ())

    backend = default_backend()
    traced = backend.trace_function(native_function)
    if traced is NotImplemented:
        warnings.warn(f&#34;Backend &#39;{backend}&#39; does not support function tracing. Returning original function.&#34;)
        return f

    def wrapper(*values: Tensor):
        INPUT_TENSORS.clear()
        INPUT_TENSORS.extend(values)
        for v in values:
            v._expand()
        natives = sum([v._natives() for v in values], ())
        results_native = list(traced(*natives))
        results = [t._with_natives_replaced(results_native) for t in OUTPUT_TENSORS]
        assert len(results_native) == 0
        return results[0] if len(results) == 1 else results

    return wrapper</code></pre>
</details>
</dd>
<dt id="phi.math.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>value, axes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose(value, axes):
    if isinstance(value, Tensor):
        return CollapsedTensor(value, value.shape[axes])
    else:
        return choose_backend(value).transpose(value, axes)</code></pre>
</details>
</dd>
<dt id="phi.math.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>value: phi.math._tensors.Tensor, dim: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for <code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">Tensor.unstack()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(value: Tensor, dim: str):
    &#34;&#34;&#34; Alias for `Tensor.unstack()` &#34;&#34;&#34;
    return value.unstack(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.upsample2x"><code class="name flex">
<span>def <span class="ident">upsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to double the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>half-size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which up-sampling is applied. If None, up-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>double-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upsample2x(grid: Tensor,
               padding: Extrapolation = extrapolation.BOUNDARY,
               dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to double the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: half-size grid
      padding: grid extrapolation
      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      double-size grid

    &#34;&#34;&#34;
    for i, dim in enumerate(grid.shape.spatial.only(dims).names):
        left, center, right = shift(grid, (-1, 0, 1), (dim,), padding, None)
        interp_left = 0.25 * left + 0.75 * center
        interp_right = 0.75 * center + 0.25 * right
        stacked = math.spatial_stack([interp_left, interp_right], &#39;_interleave&#39;)
        grid = math.join_dimensions(stacked, (dim, &#39;_interleave&#39;), dim)
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.vec_abs"><code class="name flex">
<span>def <span class="ident">vec_abs</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_abs(vec: Tensor):
    return math.sqrt(math.sum_(vec ** 2, dim=vec.shape.channel.names))</code></pre>
</details>
</dd>
<dt id="phi.math.vec_squared"><code class="name flex">
<span>def <span class="ident">vec_squared</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_squared(vec: Tensor):
    return math.sum_(vec ** 2, dim=vec.shape.channel.names)</code></pre>
</details>
</dd>
<dt id="phi.math.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>condition: phi.math._tensors.Tensor, value_true: phi.math._tensors.Tensor, value_false: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a tensor by choosing either values from <code>value_true</code> or <code>value_false</code> depending on <code>condition</code>.
If <code>condition</code> is not of type boolean, non-zero values are interpreted as True.</p>
<p>This function requires non-None values for <code>value_true</code> and <code>value_false</code>.
To get the indices of True / non-zero values, use :func:<code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong></dt>
<dd>determines where to choose values from value_true or from value_false</dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>values to pick where condition != 0 / True</dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>values to pick where condition == 0 / False</dd>
<dt><strong><code>condition</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>Tensor or float or int: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor containing dimensions of all inputs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def where(condition: Tensor or float or int, value_true: Tensor or float or int, value_false: Tensor or float or int):
    &#34;&#34;&#34;
    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
    If `condition` is not of type boolean, non-zero values are interpreted as True.
    
    This function requires non-None values for `value_true` and `value_false`.
    To get the indices of True / non-zero values, use :func:`nonzero`.

    Args:
      condition: determines where to choose values from value_true or from value_false
      value_true: values to pick where condition != 0 / True
      value_false: values to pick where condition == 0 / False
      condition: Tensor or float or int: 
      value_true: Tensor or float or int: 
      value_false: Tensor or float or int: 

    Returns:
      tensor containing dimensions of all inputs

    &#34;&#34;&#34;
    condition, value_true, value_false = tensors(condition, value_true, value_false)
    shape, (c, vt, vf) = broadcastable_native_tensors(condition, value_true, value_false)
    result = choose_backend(c, vt, vf).where(c, vt, vf)
    return NativeTensor(result, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.wrap"><code class="name flex">
<span>def <span class="ident">wrap</span></span>(<span>data: phi.math._tensors.Tensor, names: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Short for <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code> with <code>convert=False</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap(data: Tensor or Shape or tuple or list or numbers.Number,
         names: str or tuple or list = None) -&gt; Tensor:
    &#34;&#34;&#34; Short for `phi.math.tensor()` with `convert=False`. &#34;&#34;&#34;
    return tensor(data, names=names, convert=False)</code></pre>
</details>
</dd>
<dt id="phi.math.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 0 / False everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 0 / False everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().zeros((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>tensor: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_like(tensor: Tensor):
    return zeros(tensor.shape, dtype=tensor.dtype)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phi.math.DType"><code class="flex name class">
<span>class <span class="ident">DType</span></span>
<span>(</span><span>kind: type, bits: int = 8)</span>
</code></dt>
<dd>
<div class="desc"><p>Data type for tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong></dt>
<dd>Python type, one of <code>(bool, int, float, complex, str)</code></dd>
<dt><strong><code>bits</code></strong></dt>
<dd>number of bits, typically a multiple of 8.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DType:

    def __init__(self, kind: type, bits: int = 8):
        &#34;&#34;&#34;
        Data type for tensors.

        Args:
          kind: Python type, one of `(bool, int, float, complex, str)`
          bits: number of bits, typically a multiple of 8.
        &#34;&#34;&#34;
        assert kind in (bool, int, float, complex, str)
        if kind is bool:
            assert bits == 8
        else:
            assert isinstance(bits, int)
        self.kind = kind
        &#34;&#34;&#34; Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex) &#34;&#34;&#34;
        self.bits = bits
        &#34;&#34;&#34; Number of bits used to store a single value of this type. See `DType.itemsize`. &#34;&#34;&#34;

    @property
    def precision(self):
        &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
        if self.kind == float:
            return self.bits
        if self.kind == complex:
            return self.bits // 2
        else:
            return None

    @property
    def itemsize(self):
        &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
        assert self.bits % 8 == 0
        return self.bits // 8

    def __eq__(self, other):
        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self.kind) + hash(self.bits)

    def __repr__(self):
        return f&#34;{self.kind.__name__}{self.bits}&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.DType.bits"><code class="name">var <span class="ident">bits</span></code></dt>
<dd>
<div class="desc"><p>Number of bits used to store a single value of this type. See <code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">DType.itemsize</a></code>.</p></div>
</dd>
<dt id="phi.math.DType.itemsize"><code class="name">var <span class="ident">itemsize</span></code></dt>
<dd>
<div class="desc"><p>Number of bytes used to storea single value of this type. See <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def itemsize(self):
    &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
    assert self.bits % 8 == 0
    return self.bits // 8</code></pre>
</details>
</dd>
<dt id="phi.math.DType.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"><p>Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex)</p></div>
</dd>
<dt id="phi.math.DType.precision"><code class="name">var <span class="ident">precision</span></code></dt>
<dd>
<div class="desc"><p>Floating point precision. Only defined if <code>kind in (float, complex)</code>. For complex values, returns half of <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self):
    &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
    if self.kind == float:
        return self.bits
    if self.kind == complex:
        return self.bits // 2
    else:
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Extrapolation"><code class="flex name class">
<span>class <span class="ident">Extrapolation</span></span>
<span>(</span><span>pad_rank)</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolations are used to determine values of grids or other structures outside the sampled bounds.</p>
<p>They play a vital role in padding and sampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pad_rank</code></strong></dt>
<dd>low-ranking extrapolations are handled first during mixed-extrapolation padding.</dd>
</dl>
<p>The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Extrapolation:

    def __init__(self, pad_rank):
        &#34;&#34;&#34;
        Extrapolations are used to determine values of grids or other structures outside the sampled bounds.

        They play a vital role in padding and sampling.

        Args:
          pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.
        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.

        Returns:

        &#34;&#34;&#34;
        self.pad_rank = pad_rank

    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
        
        Use `from_dict()` to restore the Extrapolation object.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def gradient(self) -&gt; &#39;Extrapolation&#39;:
        &#34;&#34;&#34;Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
        raise NotImplementedError()

    def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
        &#34;&#34;&#34;
        Pads a tensor using values from self.pad_values()

        Args:
          value: tensor to be padded
          widths: name: str -&gt; (lower: int, upper: int)}
          value: Tensor: 
          widths: dict: 

        Returns:

        &#34;&#34;&#34;
        for dim in widths:
            values = []
            if widths[dim][False] &gt; 0:
                values.append(self.pad_values(value, widths[dim][False], dim, False))
            values.append(value)
            if widths[dim][True] &gt; 0:
                values.append(self.pad_values(value, widths[dim][True], dim, True))
            value = math.concat(values, dim)
        return value

    def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
        &#34;&#34;&#34;
        Determines the values with which the given tensor would be padded at the specified using this extrapolation.

        Args:
          value: tensor to be padded
          width: number of cells to pad perpendicular to the face. Must be larger than zero.
          dimension: axis in which to pad
          upper_edge: True for upper edge, False for lower edge
          value: Tensor: 
          width: int: 
          dimension: str: 
          upper_edge: bool: 

        Returns:
          tensor that can be concatenated to value for padding

        &#34;&#34;&#34;
        raise NotImplementedError()

    def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
        &#34;&#34;&#34;
        If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
        
        Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
        Coordinates are then snapped to the valid index range.
        This is the default implementation.

        Args:
          coordinates: integer coordinates in index space
          shape: tensor shape
          coordinates: Tensor: 
          shape: Shape: 

        Returns:
          transformed coordinates

        &#34;&#34;&#34;
        return math.clip(coordinates, 0, math.wrap(shape.spatial - 1, &#39;vector&#39;))

    @property
    def is_copy_pad(self):
        &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
        return False

    @property
    def native_grid_sample_mode(self) -&gt; Union[str, None]:
        return None

    def __getitem__(self, item):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="phi.math.extrapolation.ConstantExtrapolation" href="extrapolation.html#phi.math.extrapolation.ConstantExtrapolation">ConstantExtrapolation</a></li>
<li>phi.math.extrapolation._CopyExtrapolation</li>
<li>phi.math.extrapolation._MixedExtrapolation</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Extrapolation.is_copy_pad"><code class="name">var <span class="ident">is_copy_pad</span></code></dt>
<dd>
<div class="desc"><p>:return: True if all pad values are copies of existing values in the tensor to be padded</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_copy_pad(self):
    &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.native_grid_sample_mode"><code class="name">var <span class="ident">native_grid_sample_mode</span> : Union[str, NoneType]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def native_grid_sample_mode(self) -&gt; Union[str, None]:
    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Extrapolation.gradient"><code class="name flex">
<span>def <span class="ident">gradient</span></span>(<span>self) ‑> <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradient(self) -&gt; &#39;Extrapolation&#39;:
    &#34;&#34;&#34;Returns the extrapolation for the spatial gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, value: phi.math._tensors.Tensor, widths: dict) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor using values from self.pad_values()</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)}</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor using values from self.pad_values()

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)}
      value: Tensor: 
      widths: dict: 

    Returns:

    &#34;&#34;&#34;
    for dim in widths:
        values = []
        if widths[dim][False] &gt; 0:
            values.append(self.pad_values(value, widths[dim][False], dim, False))
        values.append(value)
        if widths[dim][True] &gt; 0:
            values.append(self.pad_values(value, widths[dim][True], dim, True))
        value = math.concat(values, dim)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.pad_values"><code class="name flex">
<span>def <span class="ident">pad_values</span></span>(<span>self, value: phi.math._tensors.Tensor, width: int, dimension: str, upper_edge: bool) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the values with which the given tensor would be padded at the specified using this extrapolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>width</code></strong></dt>
<dd>number of cells to pad perpendicular to the face. Must be larger than zero.</dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>axis in which to pad</dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>True for upper edge, False for lower edge</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>width</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>bool: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor that can be concatenated to value for padding</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
    &#34;&#34;&#34;
    Determines the values with which the given tensor would be padded at the specified using this extrapolation.

    Args:
      value: tensor to be padded
      width: number of cells to pad perpendicular to the face. Must be larger than zero.
      dimension: axis in which to pad
      upper_edge: True for upper edge, False for lower edge
      value: Tensor: 
      width: int: 
      dimension: str: 
      upper_edge: bool: 

    Returns:
      tensor that can be concatenated to value for padding

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Serialize this extrapolation to a dictionary that is serializable (JSON-writable).</p>
<p>Use <code>from_dict()</code> to restore the Extrapolation object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
    
    Use `from_dict()` to restore the Extrapolation object.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.transform_coordinates"><code class="name flex">
<span>def <span class="ident">transform_coordinates</span></span>(<span>self, coordinates: phi.math._tensors.Tensor, shape: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.</p>
<p>Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
Coordinates are then snapped to the valid index range.
This is the default implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>coordinates</code></strong></dt>
<dd>integer coordinates in index space</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tensor shape</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>transformed coordinates</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
    
    Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
    Coordinates are then snapped to the valid index range.
    This is the default implementation.

    Args:
      coordinates: integer coordinates in index space
      shape: tensor shape
      coordinates: Tensor: 
      shape: Shape: 

    Returns:
      transformed coordinates

    &#34;&#34;&#34;
    return math.clip(coordinates, 0, math.wrap(shape.spatial - 1, &#39;vector&#39;))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.LinearSolve"><code class="flex name class">
<span>class <span class="ident">LinearSolve</span></span>
<span>(</span><span>solver: str = None, relative_tolerance=1e-05, absolute_tolerance=0, max_iterations=1000, bake='sparse', gradient_solve: <a title="phi.math.Solve" href="#phi.math.Solve">Solve</a> = None, **solver_arguments)</span>
</code></dt>
<dd>
<div class="desc"><p>Specifies parameters and stopping criteria for solving a system of linear equations.</p>
<p>Extends <code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> by the property <code>bake</code> which determines whether and how the equations are stored.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearSolve(Solve):
    &#34;&#34;&#34;
    Specifies parameters and stopping criteria for solving a system of linear equations.

    Extends `Solve` by the property `bake` which determines whether and how the equations are stored.
    &#34;&#34;&#34;

    def __init__(self,
                 solver: str = None,
                 relative_tolerance=1e-5,
                 absolute_tolerance=0,
                 max_iterations=1000,
                 bake=&#39;sparse&#39;,
                 gradient_solve: &#39;Solve&#39; or None = None,
                 **solver_arguments):
        Solve.__init__(self, solver, relative_tolerance, absolute_tolerance, max_iterations, gradient_solve, **solver_arguments)
        self.bake = bake
        &#34;&#34;&#34; Baking method: None to use original function, `&#39;sparse&#39;` to create a sparse matrix. &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math.backend._optim.Solve</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.LinearSolve.bake"><code class="name">var <span class="ident">bake</span></code></dt>
<dd>
<div class="desc"><p>Baking method: None to use original function, <code>'sparse'</code> to create a sparse matrix.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
<span>(</span><span>sizes: tuple, names: tuple, types: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Shapes enumerate dimensions, each consisting of a name, size and type.</p>
<p>To construct a Shape manually, use <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> instead.
This constructor is meant for internal use only.</p>
<p>Construct a Shape from sizes, names and types sequences.
All arguments must have same length.</p>
<p>To create a Shape with inferred dimension types, use :func:<code>shape(**dims)</code> instead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>Ordered dimension sizes</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Ordered dimension names, either strings (spatial, batch) or integers (channel)</dd>
<dt><strong><code>types</code></strong></dt>
<dd>Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape:
    &#34;&#34;&#34;Shapes enumerate dimensions, each consisting of a name, size and type.&#34;&#34;&#34;

    def __init__(self, sizes: tuple or list, names: tuple or list, types: tuple or list):
        &#34;&#34;&#34;
        To construct a Shape manually, use `shape()` instead.
        This constructor is meant for internal use only.

        Construct a Shape from sizes, names and types sequences.
        All arguments must have same length.

        To create a Shape with inferred dimension types, use :func:`shape(**dims)` instead.

        Args:
            sizes: Ordered dimension sizes
            names: Ordered dimension names, either strings (spatial, batch) or integers (channel)
            types: Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)
        &#34;&#34;&#34;
        assert len(sizes) == len(names) == len(types), f&#34;sizes={sizes} ({len(sizes)}), names={names} ({len(names)}), types={types} ({len(types)})&#34;
        self.sizes = tuple(sizes)
        &#34;&#34;&#34; Ordered dimension sizes as `tuple`  &#34;&#34;&#34;
        self.names = tuple(names)
        &#34;&#34;&#34; Ordered dimension names as `tuple` of `str` &#34;&#34;&#34;
        assert all(isinstance(n, str) for n in names), f&#34;All names must be of type string but got {names}&#34;
        self.types = tuple(types)  # undocumented, may be private

    @property
    def named_sizes(self):
        &#34;&#34;&#34;
        For iterating over names and sizes

            for name, size in shape.named_sizes:

        Returns:
            iterable
        &#34;&#34;&#34;
        return zip(self.names, self.sizes)

    @property
    def spatial_dict(self) -&gt; dict:
        &#34;&#34;&#34; Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions. &#34;&#34;&#34;
        return {n: s for s, n, t in zip(self.sizes, self.names, self.types) if t == SPATIAL_DIM}

    @property
    def dimensions(self):
        &#34;&#34;&#34;
        For iterating over sizes, names and types.
        Meant for internal use.

        See `Shape.named_sizes()`.
        &#34;&#34;&#34;
        return zip(self.sizes, self.names, self.types)

    def __len__(self):
        return len(self.sizes)

    def __contains__(self, item):
        return item in self.names

    def index(self, name: str or list or tuple or &#39;Shape&#39; or None):
        &#34;&#34;&#34;
        Finds the index of the dimension(s) within this Shape.

        Args:
          name: dimension name or sequence thereof, including Shape object
          name: str or list or tuple or Shape: 

        Returns:
          single index or sequence of indices

        &#34;&#34;&#34;
        if name is None:
            return None
        if isinstance(name, (list, tuple)):
            return tuple(self.index(n) for n in name)
        if isinstance(name, Shape):
            return tuple(self.index(n) for n in name.names)
        for idx, dim_name in enumerate(self.names):
            if dim_name == name:
                return idx
        raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))

    def indices(self, names: tuple or list or &#39;Shape&#39;):
        if isinstance(names, (list, tuple)):
            return tuple(self.index(n) for n in names)
        if isinstance(names, Shape):
            return tuple(self.index(n) for n in names.names)
        else:
            raise ValueError(names)

    def get_size(self, dim: str or tuple or list):
        &#34;&#34;&#34;
        Args:
            dim: dimension name or sequence of dimension names

        Returns:
            size associated with `dim`
        &#34;&#34;&#34;
        if isinstance(dim, str):
            return self.sizes[self.names.index(dim)]
        elif isinstance(dim, (tuple, list)):
            return tuple(self.get_size(n) for n in dim)
        else:
            raise ValueError(dim)

    def __getattr__(self, name):
        if name == &#39;names&#39;:
            raise AssertionError(&#34;Attribute missing: %s&#34; % name)
        if name in self.names:
            return self.get_size(name)
        raise AttributeError(&#34;Shape has no attribute &#39;%s&#39;&#34; % (name,))

    def get_type(self, name: str or tuple or list or &#39;Shape&#39;):
        if isinstance(name, str):
            return self.types[self.names.index(name)]
        elif isinstance(name, (tuple, list)):
            return tuple(self.get_type(n) for n in name)
        elif isinstance(name, Shape):
            return tuple(self.get_type(n) for n in name.names)
        else:
            raise ValueError(name)

    def __getitem__(self, selection):
        if isinstance(selection, int):
            return self.sizes[selection]
        elif isinstance(selection, slice):
            return Shape(self.sizes[selection], self.names[selection], self.types[selection])
        return Shape([self.sizes[i] for i in selection], [self.names[i] for i in selection], [self.types[i] for i in selection])

    @property
    def batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]

    @property
    def non_batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial and channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]

    @property
    def spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch and channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the channel dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]

    @property
    def non_channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch and spatial dimensions as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]

    @property
    def singleton(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of 1 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size == 1]]

    @property
    def non_singleton(self)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from 1 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size != 1]]

    @property
    def zero(self)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of 0 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size == 0]]

    @property
    def non_zero(self)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from 0 as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size != 0]]

    @property
    def undefined(self)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size of `None` as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size is None]]

    @property
    def defined(self)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the dimensions with a size different from `None` as a new `Shape` object.

        See also:

        * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
        * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, size in enumerate(self.sizes) if size is not None]]

    def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
        &#34;&#34;&#34;
        Slices this `Shape` along a dimension.
        The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
        https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

        Args:
            dim: dimension to unstack

        Returns:
            slices of this shape
        &#34;&#34;&#34;
        if dim == &#39;dims&#39;:
            return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
        if dim not in self:
            return tuple([self])
        else:
            from ._tensors import Tensor
            inner = self.without(dim)
            sizes = []
            dim_size = self.get_size(dim)
            for size in inner.sizes:
                if isinstance(size, Tensor) and dim in size.shape:
                    sizes.append(size.unstack(dim))
                    dim_size = size.shape.get_size(dim)
                else:
                    sizes.append(size)
            assert isinstance(dim_size, int)
            shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
            return shapes

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
        assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
        return self.names[0]

    @property
    def is_batch(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *batch* &#34;&#34;&#34;
        return all([t == BATCH_DIM for t in self.types])

    @property
    def is_spatial(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *spatial* &#34;&#34;&#34;
        return all([t == SPATIAL_DIM for t in self.types])

    @property
    def is_channel(self) -&gt; bool:
        &#34;&#34;&#34; Tests if all dimensions are of type *channel* &#34;&#34;&#34;
        return all([t == CHANNEL_DIM for t in self.types])

    def mask(self, names: tuple or list or set):
        &#34;&#34;&#34;
        Returns a binary sequence corresponding to the names of this Shape.
        A value of 1 means that a dimension of this Shape is contained in `names`.

        Args:
          names: collection of dimension
          names: tuple or list or set: 

        Returns:
          binary sequence

        &#34;&#34;&#34;
        if isinstance(names, str):
            names = [names]
        mask = [1 if name in names else 0 for name in self.names]
        return tuple(mask)

    def __repr__(self):
        strings = [&#39;%s=%s&#39; % (name, size) for size, name, _ in self.dimensions]
        return &#39;(&#39; + &#39;, &#39;.join(strings) + &#39;)&#39;

    def __eq__(self, other):
        if not isinstance(other, Shape):
            return False
        return self.names == other.names and self.types == other.types and self.sizes == other.sizes

    def __ne__(self, other):
        return not self == other

    def normal_order(self):
        sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
        names = self.batch.names + self.spatial.names + self.channel.names
        types = self.batch.types + self.spatial.types + self.channel.types
        return Shape(sizes, names, types)

    def reorder(self, names: tuple or list):
        assert len(names) == self.rank
        order = [self.index(n) for n in names]
        return self[order]

    def order_group(self, names: tuple or list or &#39;Shape&#39;):
        if isinstance(names, Shape):
            names = names.names
        order = []
        for name in self.names:
            if name not in order:
                if name in names:
                    order.extend(names)
                else:
                    order.append(name)
        return order

    def alphabetically(self):
        return self.reorder(sorted(self.names))

    def combined(self, other: &#39;Shape&#39;, combine_spatial=False)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a Shape object that both `self` and `other` can be broadcast to.
        If `self` and `other` are incompatible, raises a ValueError.

        Args:
          other: Shape
          other: Shape: 
          combine_spatial:  (Default value = False)

        Returns:
          combined shape
          :raise: ValueError if shapes don&#39;t match

        &#34;&#34;&#34;
        return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])

    def __and__(self, other):
        return combine_safe(self, other, check_exact=[SPATIAL_DIM])

    def expand_batch(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
        return self.expand(size, name, BATCH_DIM, pos)

    def expand_spatial(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
        return self.expand(size, name, SPATIAL_DIM, pos)

    def expand_channel(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
        return self.expand(size, name, CHANNEL_DIM, pos)

    def expand(self, size, name: str, dim_type: str, pos=None)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Add a dimension to the shape.
        
        The resulting shape has linear indices.

        Args:
          size: 
          name: str: 
          dim_type: str: 
          pos:  (Default value = None)

        Returns:

        &#34;&#34;&#34;
        if pos is None:
            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
            if len(same_type_dims) &gt; 0:
                pos = self.index(same_type_dims.names[0])
            else:
                pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
        elif pos &lt; 0:
            pos += self.rank + 1
        sizes = list(self.sizes)
        names = list(self.names)
        types = list(self.types)
        sizes.insert(pos, size)
        names.insert(pos, name)
        types.insert(pos, dim_type)
        return Shape(sizes, names, types)

    def extend(self, other: &#39;Shape&#39;, pos=-1)-&gt; &#39;Shape&#39;:
        if pos == -1:
            return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)
        elif pos == None:
            result = self
            for size, name, dim_type in other.dimensions:
                result = result.expand(size, name, dim_type)
            return result
        else:
            raise NotImplementedError(pos)

    def without(self, dims: str or tuple or list or &#39;Shape&#39; or None)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Builds a new shape from this one that is missing all given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.only`.

        Args:
          dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: str or tuple or list or Shape or None: 

        Returns:
          Shape without specified dimensions

        &#34;&#34;&#34;
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] != dims]]
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
        elif dims is None:  # subtract all
            return EMPTY_SHAPE
        else:
            raise ValueError(dims)

    reduce = without

    def only(self, dims: str or tuple or list or &#39;Shape&#39;):
        &#34;&#34;&#34;
        Builds a new shape from this one that only contains the given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.without`.

        Args:
          dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: str or tuple or list or Shape: 

        Returns:
          Shape containing only specified dimensions

        &#34;&#34;&#34;
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] == dims]]
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
        elif dims is None:  # keep all
            return self
        else:
            raise ValueError(dims)

    def select(self, *names):
        indices = [self.index(name) for name in names]
        return self[indices]

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions.
        Equal to `len(shape)`.

        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
        &#34;&#34;&#34;
        return len(self.sizes)

    @property
    def batch_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == BATCH_DIM:
                r += 1
        return r

    @property
    def spatial_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == SPATIAL_DIM:
                r += 1
        return r

    @property
    def channel_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == CHANNEL_DIM:
                r += 1
        return r

    def to_batch(self, dims: tuple or list or None = None)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a shape like this Shape but with `dims` being of type `batch`.
        
        Leaves this Shape object untouched.

        Args:
          dims: sequence of dimension names to convert or None to convert all dimensions
          dims: tuple or list or None:  (Default value = None)

        Returns:
          new Shape object

        &#34;&#34;&#34;
        if dims is None:
            return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
        else:
            return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])

    @property
    def well_defined(self):
        &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
        return None not in self.sizes

    @property
    def shape(self, list_dim=&#39;dims&#39;)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns the shape of this `Shape`.
        The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

        Sizes of type `Tensor` can cause the result to have additional dimensions.

        Args:
            list_dim: name of dimension listing the dimensions of this shape

        Returns:
            second order shape
        &#34;&#34;&#34;
        from phi.math import Tensor
        shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    @property
    def is_non_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is non-uniform if the size of any dimension varies along another dimension.

        See `Shape.shape`.
        &#34;&#34;&#34;
        from phi.math import Tensor
        for size in self.sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                return True
        return False

    def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;):
        if isinstance(sizes, Shape):
            sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
            return Shape(sizes, self.names, self.types)
        else:
            assert len(sizes) == len(self.sizes)
            return Shape(sizes, self.names, self.types)

    def with_size(self, name, size):
        new_sizes = list(self.sizes)
        new_sizes[self.index(name)] = size
        return self.with_sizes(new_sizes)

    def with_names(self, names: str or tuple or list):
        if isinstance(names, str):
            names = parse_dim_names(names, self.rank)
            names = [n if n is not None else o for n, o in zip(names, self.names)]
        return Shape(self.sizes, names, self.types)

    def with_types(self, types: &#39;Shape&#39;):
        return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])

    def perm(self, names):
        assert set(names) == set(self.names), &#39;names must match existing dimensions %s but got %s&#39; % (self.names, names)
        perm = [self.names.index(name) for name in names]
        return perm

    @property
    def volume(self) -&gt; int or None:
        &#34;&#34;&#34;
        Returns the total number of values contained in a tensor of this shape.
        This is the product of all dimension sizes.

        Returns:
            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
        &#34;&#34;&#34;
        result = 1
        for size in self.sizes:
            if size is None:
                return None
            result *= size
        return result

    @property
    def is_empty(self) -&gt; bool:
        &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
        return len(self.sizes) == 0

    def order(self, sequence, default=None)-&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        If sequence is a dict with dimension names as keys, orders its values according to this shape.
        
        Otherwise, the sequence is returned unchanged.

        Args:
          sequence(dict or list or tuple): sequence or dict to be ordered
          default: default value used for dimensions not contained in sequence

        Returns:
          ordered sequence of values
        &#34;&#34;&#34;
        if isinstance(sequence, dict):
            result = [sequence.get(name, default) for name in self.names]
            return result
        if isinstance(sequence, (tuple, list)):
            assert len(sequence) == self.rank
            return sequence
        else:  # just a constant
            return sequence

    def sequence_get(self, sequence, name):
        if isinstance(sequence, dict):
            return sequence[name]
        if isinstance(sequence, (tuple, list)):
            assert len(sequence) == self.rank
            return sequence[self.names.index(name)]
        if math.is_tensor(sequence):
            assert math.staticshape(sequence) == (self.rank,)
            return sequence[self.names.index(name)]
        else:  # just a constant
            return sequence

    def after_pad(self, widths: dict):
        sizes = list(self.sizes)
        for dim, (lo, up) in widths.items():
            sizes[self.index(dim)] += lo + up
        return Shape(sizes, self.names, self.types)

    def after_gather(self, selection: dict):
        result = self
        for name, selection in selection.items():
            if isinstance(selection, int):
                result = result.without(name)
            elif isinstance(selection, slice):
                start = selection.start or 0
                stop = selection.stop or self.get_size(name)
                step = selection.step or 1
                if stop &lt; 0:
                    stop += self.get_size(name)
                    assert stop &gt;= 0
                new_size = math.to_int(math.ceil(math.wrap((stop - start) / step)))
                if new_size.rank == 0:
                    new_size = int(new_size)  # NumPy array not allowed because not hashable
                result = result.with_size(name, new_size)
            else:
                raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
        return result

    def meshgrid(self):
        &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
        indices = [0] * self.rank
        while True:
            yield {name: index for name, index in zip(self.names, indices)}
            for i in range(self.rank-1, -1, -1):
                indices[i] = (indices[i] + 1) % self.sizes[i]
                if indices[i] != 0:
                    break
            else:
                return

    product = meshgrid

    def __add__(self, other):
        return self._op1(other, lambda s, o: s + o)

    def __radd__(self, other):
        return self._op1(other, lambda s, o: o + s)

    def __sub__(self, other):
        return self._op1(other, lambda s, o: s - o)

    def __rsub__(self, other):
        return self._op1(other, lambda s, o: o - s)

    def __mul__(self, other):
        return self._op1(other, lambda s, o: s * o)

    def __rmul__(self, other):
        return self._op1(other, lambda s, o: o * s)

    def _op1(self, other, fun):
        if isinstance(other, int):
            return Shape([fun(s, other) for s in self.sizes], self.names, self.types)
        elif isinstance(other, Shape):
            assert self.names == other.names, f&#34;{self.names, other.names}&#34;
            return Shape([fun(s, o) for s, o in zip(self.sizes, other.sizes)], self.names, self.types)
        else:
            return NotImplemented

    def __hash__(self):
        return hash(self.sizes)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Shape.batch"><code class="name">var <span class="ident">batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.batch_rank"><code class="name">var <span class="ident">batch_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == BATCH_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel"><code class="name">var <span class="ident">channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel_rank"><code class="name">var <span class="ident">channel_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of channel dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == CHANNEL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.defined"><code class="name">var <span class="ident">defined</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from <code>None</code> as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def defined(self)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from `None` as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size is not None]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.dimensions"><code class="name">var <span class="ident">dimensions</span></code></dt>
<dd>
<div class="desc"><p>For iterating over sizes, names and types.
Meant for internal use.</p>
<p>See <code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">Shape.named_sizes</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dimensions(self):
    &#34;&#34;&#34;
    For iterating over sizes, names and types.
    Meant for internal use.

    See `Shape.named_sizes()`.
    &#34;&#34;&#34;
    return zip(self.sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_batch"><code class="name">var <span class="ident">is_batch</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>batch</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_batch(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *batch* &#34;&#34;&#34;
    return all([t == BATCH_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_channel"><code class="name">var <span class="ident">is_channel</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>channel</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_channel(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *channel* &#34;&#34;&#34;
    return all([t == CHANNEL_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_empty"><code class="name">var <span class="ident">is_empty</span> : bool</code></dt>
<dd>
<div class="desc"><p>True if this shape has no dimensions. Equivalent to <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> <code>== 0</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_empty(self) -&gt; bool:
    &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
    return len(self.sizes) == 0</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_non_uniform"><code class="name">var <span class="ident">is_non_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is non-uniform if the size of any dimension varies along another dimension.</p>
<p>See <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_non_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is non-uniform if the size of any dimension varies along another dimension.

    See `Shape.shape`.
    &#34;&#34;&#34;
    from phi.math import Tensor
    for size in self.sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_spatial"><code class="name">var <span class="ident">is_spatial</span> : bool</code></dt>
<dd>
<div class="desc"><p>Tests if all dimensions are of type <em>spatial</em></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_spatial(self) -&gt; bool:
    &#34;&#34;&#34; Tests if all dimensions are of type *spatial* &#34;&#34;&#34;
    return all([t == SPATIAL_DIM for t in self.types])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for shapes with a single dimension. Returns the name of the dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
    assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
    return self.names[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.named_sizes"><code class="name">var <span class="ident">named_sizes</span></code></dt>
<dd>
<div class="desc"><p>For iterating over names and sizes</p>
<pre><code>for name, size in shape.named_sizes:
</code></pre>
<h2 id="returns">Returns</h2>
<p>iterable</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def named_sizes(self):
    &#34;&#34;&#34;
    For iterating over names and sizes

        for name, size in shape.named_sizes:

    Returns:
        iterable
    &#34;&#34;&#34;
    return zip(self.names, self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension names as <code>tuple</code> of <code>str</code></p></div>
</dd>
<dt id="phi.math.Shape.non_batch"><code class="name">var <span class="ident">non_batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial and channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial and channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_channel"><code class="name">var <span class="ident">non_channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch and spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch and spatial dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_singleton"><code class="name">var <span class="ident">non_singleton</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from 1 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_singleton(self)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from 1 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size != 1]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_spatial"><code class="name">var <span class="ident">non_spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch and channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch and channel dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_zero"><code class="name">var <span class="ident">non_zero</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size different from 0 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_zero(self)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size different from 0 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size != 0]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the number of dimensions.
Equal to <code>len(<a title="phi.math.shape" href="#phi.math.shape">shape()</a>)</code>.</p>
<p>See <code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">Shape.is_empty</a></code>, <code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">Shape.batch_rank</a></code>, <code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">Shape.spatial_rank</a></code>, <code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">Shape.channel_rank</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions.
    Equal to `len(shape)`.

    See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
    &#34;&#34;&#34;
    return len(self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Returns the shape of this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
The returned shape will always contain the dimension <code>list_dim</code> with a size equal to the <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> of this shape.</p>
<p>Sizes of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> can cause the result to have additional dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing the dimensions of this shape</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>second order shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self, list_dim=&#39;dims&#39;)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns the shape of this `Shape`.
    The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

    Sizes of type `Tensor` can cause the result to have additional dimensions.

    Args:
        list_dim: name of dimension listing the dimensions of this shape

    Returns:
        second order shape
    &#34;&#34;&#34;
    from phi.math import Tensor
    shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.singleton"><code class="name">var <span class="ident">singleton</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of 1 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def singleton(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of 1 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size == 1]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sizes"><code class="name">var <span class="ident">sizes</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension sizes as <code>tuple</code></p></div>
</dd>
<dt id="phi.math.Shape.spatial"><code class="name">var <span class="ident">spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial dimensions as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_dict"><code class="name">var <span class="ident">spatial_dict</span> : dict</code></dt>
<dd>
<div class="desc"><p>Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_dict(self) -&gt; dict:
    &#34;&#34;&#34; Ordered dictionary mapping dimension names to their respective sizes for all spatial dimensions. &#34;&#34;&#34;
    return {n: s for s, n, t in zip(self.sizes, self.names, self.types) if t == SPATIAL_DIM}</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_rank"><code class="name">var <span class="ident">spatial_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == SPATIAL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.undefined"><code class="name">var <span class="ident">undefined</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of <code>None</code> as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def undefined(self)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of `None` as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size is None]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.volume"><code class="name">var <span class="ident">volume</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the total number of values contained in a tensor of this shape.
This is the product of all dimension sizes.</p>
<h2 id="returns">Returns</h2>
<p>volume as <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>None</code> if the shape is not <code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">Shape.well_defined</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def volume(self) -&gt; int or None:
    &#34;&#34;&#34;
    Returns the total number of values contained in a tensor of this shape.
    This is the product of all dimension sizes.

    Returns:
        volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
    &#34;&#34;&#34;
    result = 1
    for size in self.sizes:
        if size is None:
            return None
        result *= size
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.well_defined"><code class="name">var <span class="ident">well_defined</span></code></dt>
<dd>
<div class="desc"><p>Returns True if no dimension is <code>None</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def well_defined(self):
    &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
    return None not in self.sizes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.zero"><code class="name">var <span class="ident">zero</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the dimensions with a size of 0 as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:</p>
<ul>
<li>Dimension type filters: <code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code></li>
<li>Dimension size filters: <code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">Shape.singleton</a></code>, <code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">Shape.non_singleton</a></code>, <code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">Shape.zero</a></code>, <code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">Shape.non_zero</a></code>, <code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">Shape.undefined</a></code>, <code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">Shape.defined</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def zero(self)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the dimensions with a size of 0 as a new `Shape` object.

    See also:

    * Dimension type filters: `Shape.batch`, `Shape.spatial`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_channel`
    * Dimension size filters: `Shape.singleton`, `Shape.non_singleton`, `Shape.zero`, `Shape.non_zero`, `Shape.undefined`, `Shape.defined`

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, size in enumerate(self.sizes) if size == 0]]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Shape.after_gather"><code class="name flex">
<span>def <span class="ident">after_gather</span></span>(<span>self, selection: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_gather(self, selection: dict):
    result = self
    for name, selection in selection.items():
        if isinstance(selection, int):
            result = result.without(name)
        elif isinstance(selection, slice):
            start = selection.start or 0
            stop = selection.stop or self.get_size(name)
            step = selection.step or 1
            if stop &lt; 0:
                stop += self.get_size(name)
                assert stop &gt;= 0
            new_size = math.to_int(math.ceil(math.wrap((stop - start) / step)))
            if new_size.rank == 0:
                new_size = int(new_size)  # NumPy array not allowed because not hashable
            result = result.with_size(name, new_size)
        else:
            raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.after_pad"><code class="name flex">
<span>def <span class="ident">after_pad</span></span>(<span>self, widths: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_pad(self, widths: dict):
    sizes = list(self.sizes)
    for dim, (lo, up) in widths.items():
        sizes[self.index(dim)] += lo + up
    return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.alphabetically"><code class="name flex">
<span>def <span class="ident">alphabetically</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alphabetically(self):
    return self.reorder(sorted(self.names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.combined"><code class="name flex">
<span>def <span class="ident">combined</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, combine_spatial=False) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Shape object that both <code>self</code> and <code>other</code> can be broadcast to.
If <code>self</code> and <code>other</code> are incompatible, raises a ValueError.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>Shape</dd>
<dt><strong><code>other</code></strong></dt>
<dd>Shape: </dd>
<dt><strong><code>combine_spatial</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>combined shape
:raise: ValueError if shapes don't match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combined(self, other: &#39;Shape&#39;, combine_spatial=False)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns a Shape object that both `self` and `other` can be broadcast to.
    If `self` and `other` are incompatible, raises a ValueError.

    Args:
      other: Shape
      other: Shape: 
      combine_spatial:  (Default value = False)

    Returns:
      combined shape
      :raise: ValueError if shapes don&#39;t match

    &#34;&#34;&#34;
    return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>self, size, name: str, dim_type: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Add a dimension to the shape.</p>
<p>The resulting shape has linear indices.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>dim_type</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>pos</code></strong></dt>
<dd>(Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(self, size, name: str, dim_type: str, pos=None)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Add a dimension to the shape.
    
    The resulting shape has linear indices.

    Args:
      size: 
      name: str: 
      dim_type: str: 
      pos:  (Default value = None)

    Returns:

    &#34;&#34;&#34;
    if pos is None:
        same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
        if len(same_type_dims) &gt; 0:
            pos = self.index(same_type_dims.names[0])
        else:
            pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
    elif pos &lt; 0:
        pos += self.rank + 1
    sizes = list(self.sizes)
    names = list(self.names)
    types = list(self.types)
    sizes.insert(pos, size)
    names.insert(pos, name)
    types.insert(pos, dim_type)
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_batch"><code class="name flex">
<span>def <span class="ident">expand_batch</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_batch(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
    return self.expand(size, name, BATCH_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_channel"><code class="name flex">
<span>def <span class="ident">expand_channel</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_channel(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
    return self.expand(size, name, CHANNEL_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand_spatial"><code class="name flex">
<span>def <span class="ident">expand_spatial</span></span>(<span>self, size, name: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_spatial(self, size, name: str, pos=None)-&gt; &#39;Shape&#39;:
    return self.expand(size, name, SPATIAL_DIM, pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, pos=-1) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, other: &#39;Shape&#39;, pos=-1)-&gt; &#39;Shape&#39;:
    if pos == -1:
        return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)
    elif pos == None:
        result = self
        for size, name, dim_type in other.dimensions:
            result = result.expand(size, name, dim_type)
        return result
    else:
        raise NotImplementedError(pos)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_size"><code class="name flex">
<span>def <span class="ident">get_size</span></span>(<span>self, dim: str)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension name or sequence of dimension names</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>size associated with <code>dim</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_size(self, dim: str or tuple or list):
    &#34;&#34;&#34;
    Args:
        dim: dimension name or sequence of dimension names

    Returns:
        size associated with `dim`
    &#34;&#34;&#34;
    if isinstance(dim, str):
        return self.sizes[self.names.index(dim)]
    elif isinstance(dim, (tuple, list)):
        return tuple(self.get_size(n) for n in dim)
    else:
        raise ValueError(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_type(self, name: str or tuple or list or &#39;Shape&#39;):
    if isinstance(name, str):
        return self.types[self.names.index(name)]
    elif isinstance(name, (tuple, list)):
        return tuple(self.get_type(n) for n in name)
    elif isinstance(name, Shape):
        return tuple(self.get_type(n) for n in name.names)
    else:
        raise ValueError(name)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the index of the dimension(s) within this Shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name or sequence thereof, including Shape object</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str or list or tuple or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>single index or sequence of indices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index(self, name: str or list or tuple or &#39;Shape&#39; or None):
    &#34;&#34;&#34;
    Finds the index of the dimension(s) within this Shape.

    Args:
      name: dimension name or sequence thereof, including Shape object
      name: str or list or tuple or Shape: 

    Returns:
      single index or sequence of indices

    &#34;&#34;&#34;
    if name is None:
        return None
    if isinstance(name, (list, tuple)):
        return tuple(self.index(n) for n in name)
    if isinstance(name, Shape):
        return tuple(self.index(n) for n in name.names)
    for idx, dim_name in enumerate(self.names):
        if dim_name == name:
            return idx
    raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.indices"><code class="name flex">
<span>def <span class="ident">indices</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def indices(self, names: tuple or list or &#39;Shape&#39;):
    if isinstance(names, (list, tuple)):
        return tuple(self.index(n) for n in names)
    if isinstance(names, Shape):
        return tuple(self.index(n) for n in names.names)
    else:
        raise ValueError(names)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.mask"><code class="name flex">
<span>def <span class="ident">mask</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a binary sequence corresponding to the names of this Shape.
A value of 1 means that a dimension of this Shape is contained in <code>names</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>collection of dimension</dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list or set: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>binary sequence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mask(self, names: tuple or list or set):
    &#34;&#34;&#34;
    Returns a binary sequence corresponding to the names of this Shape.
    A value of 1 means that a dimension of this Shape is contained in `names`.

    Args:
      names: collection of dimension
      names: tuple or list or set: 

    Returns:
      binary sequence

    &#34;&#34;&#34;
    if isinstance(names, str):
        names = [names]
    mask = [1 if name in names else 0 for name in self.names]
    return tuple(mask)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.normal_order"><code class="name flex">
<span>def <span class="ident">normal_order</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_order(self):
    sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
    names = self.batch.names + self.spatial.names + self.channel.names
    types = self.batch.types + self.spatial.types + self.channel.types
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.only"><code class="name flex">
<span>def <span class="ident">only</span></span>(<span>self, dims: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that only contains the given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">Shape.without()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def only(self, dims: str or tuple or list or &#39;Shape&#39;):
    &#34;&#34;&#34;
    Builds a new shape from this one that only contains the given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.without`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape: 

    Returns:
      Shape containing only specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] == dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
    elif dims is None:  # keep all
        return self
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order"><code class="name flex">
<span>def <span class="ident">order</span></span>(<span>self, sequence, default=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>If sequence is a dict with dimension names as keys, orders its values according to this shape.</p>
<p>Otherwise, the sequence is returned unchanged.</p>
<h2 id="args">Args</h2>
<dl>
<dt>sequence(dict or list or tuple): sequence or dict to be ordered</dt>
<dt><strong><code>default</code></strong></dt>
<dd>default value used for dimensions not contained in sequence</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ordered sequence of values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order(self, sequence, default=None)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    If sequence is a dict with dimension names as keys, orders its values according to this shape.
    
    Otherwise, the sequence is returned unchanged.

    Args:
      sequence(dict or list or tuple): sequence or dict to be ordered
      default: default value used for dimensions not contained in sequence

    Returns:
      ordered sequence of values
    &#34;&#34;&#34;
    if isinstance(sequence, dict):
        result = [sequence.get(name, default) for name in self.names]
        return result
    if isinstance(sequence, (tuple, list)):
        assert len(sequence) == self.rank
        return sequence
    else:  # just a constant
        return sequence</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order_group"><code class="name flex">
<span>def <span class="ident">order_group</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order_group(self, names: tuple or list or &#39;Shape&#39;):
    if isinstance(names, Shape):
        names = names.names
    order = []
    for name in self.names:
        if name not in order:
            if name in names:
                order.extend(names)
            else:
                order.append(name)
    return order</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.perm"><code class="name flex">
<span>def <span class="ident">perm</span></span>(<span>self, names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def perm(self, names):
    assert set(names) == set(self.names), &#39;names must match existing dimensions %s but got %s&#39; % (self.names, names)
    perm = [self.names.index(name) for name in names]
    return perm</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.product"><code class="name flex">
<span>def <span class="ident">product</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, dims: str) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape or None: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or &#39;Shape&#39; or None)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.only`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape or None: 

    Returns:
      Shape without specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    elif dims is None:  # subtract all
        return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reorder"><code class="name flex">
<span>def <span class="ident">reorder</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder(self, names: tuple or list):
    assert len(names) == self.rank
    order = [self.index(n) for n in names]
    return self[order]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, *names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(self, *names):
    indices = [self.index(name) for name in names]
    return self[indices]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sequence_get"><code class="name flex">
<span>def <span class="ident">sequence_get</span></span>(<span>self, sequence, name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sequence_get(self, sequence, name):
    if isinstance(sequence, dict):
        return sequence[name]
    if isinstance(sequence, (tuple, list)):
        assert len(sequence) == self.rank
        return sequence[self.names.index(name)]
    if math.is_tensor(sequence):
        assert math.staticshape(sequence) == (self.rank,)
        return sequence[self.names.index(name)]
    else:  # just a constant
        return sequence</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.to_batch"><code class="name flex">
<span>def <span class="ident">to_batch</span></span>(<span>self, dims: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shape like this Shape but with <code>dims</code> being of type <code>batch</code>.</p>
<p>Leaves this Shape object untouched.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>sequence of dimension names to convert or None to convert all dimensions</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or list or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>new Shape object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_batch(self, dims: tuple or list or None = None)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns a shape like this Shape but with `dims` being of type `batch`.
    
    Leaves this Shape object untouched.

    Args:
      dims: sequence of dimension names to convert or None to convert all dimensions
      dims: tuple or list or None:  (Default value = None)

    Returns:
      new Shape object

    &#34;&#34;&#34;
    if dims is None:
        return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
    else:
        return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dim='dims') ‑> Tuple[phi.math._shape.Shape]</span>
</code></dt>
<dd>
<div class="desc"><p>Slices this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> along a dimension.
The dimension listing the sizes of the shape is referred to as <code>'dims'</code>.</p>
<p>Non-uniform tensor shapes may be unstacked along other dimensions as well, see
<a href="https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors">https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension to unstack</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>slices of this shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
    &#34;&#34;&#34;
    Slices this `Shape` along a dimension.
    The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

    Non-uniform tensor shapes may be unstacked along other dimensions as well, see
    https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

    Args:
        dim: dimension to unstack

    Returns:
        slices of this shape
    &#34;&#34;&#34;
    if dim == &#39;dims&#39;:
        return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
    if dim not in self:
        return tuple([self])
    else:
        from ._tensors import Tensor
        inner = self.without(dim)
        sizes = []
        dim_size = self.get_size(dim)
        for size in inner.sizes:
            if isinstance(size, Tensor) and dim in size.shape:
                sizes.append(size.unstack(dim))
                dim_size = size.shape.get_size(dim)
            else:
                sizes.append(size)
        assert isinstance(dim_size, int)
        shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
        return shapes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_names"><code class="name flex">
<span>def <span class="ident">with_names</span></span>(<span>self, names: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_names(self, names: str or tuple or list):
    if isinstance(names, str):
        names = parse_dim_names(names, self.rank)
        names = [n if n is not None else o for n, o in zip(names, self.names)]
    return Shape(self.sizes, names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_size"><code class="name flex">
<span>def <span class="ident">with_size</span></span>(<span>self, name, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_size(self, name, size):
    new_sizes = list(self.sizes)
    new_sizes[self.index(name)] = size
    return self.with_sizes(new_sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_sizes"><code class="name flex">
<span>def <span class="ident">with_sizes</span></span>(<span>self, sizes: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;):
    if isinstance(sizes, Shape):
        sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
        return Shape(sizes, self.names, self.types)
    else:
        assert len(sizes) == len(self.sizes)
        return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_types"><code class="name flex">
<span>def <span class="ident">with_types</span></span>(<span>self, types: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_types(self, types: &#39;Shape&#39;):
    return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.without"><code class="name flex">
<span>def <span class="ident">without</span></span>(<span>self, dims: str) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape or None: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or &#39;Shape&#39; or None)-&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.only`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape or None: 

    Returns:
      Shape without specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    elif dims is None:  # subtract all
        return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Solve"><code class="flex name class">
<span>class <span class="ident">Solve</span></span>
<span>(</span><span>solver: str = None, relative_tolerance: float = 1e-05, absolute_tolerance: float = 0, max_iterations: int = 1000, gradient_solve: <a title="phi.math.Solve" href="#phi.math.Solve">Solve</a> = None, **solver_arguments)</span>
</code></dt>
<dd>
<div class="desc"><p>Specifies parameters and stopping criteria for solving a system of equations or minimization problem.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solve:
    &#34;&#34;&#34;
    Specifies parameters and stopping criteria for solving a system of equations or minimization problem.
    &#34;&#34;&#34;

    def __init__(self,
                 solver: str = None,
                 relative_tolerance: float = 1e-5,
                 absolute_tolerance: float = 0,
                 max_iterations: int = 1000,
                 gradient_solve: &#39;Solve&#39; or None = None,
                 **solver_arguments):
        self.solver = solver
        &#34;&#34;&#34; (Optional) Name of method to use. &#34;&#34;&#34;
        self.relative_tolerance: float = relative_tolerance
        &#34;&#34;&#34; The final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.absolute_tolerance: float = absolute_tolerance
        &#34;&#34;&#34; The final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.max_iterations: int = max_iterations
        &#34;&#34;&#34; Maximum number of iterations to perform before terminating with `converged=False`. &#34;&#34;&#34;
        self._gradient_solve = gradient_solve
        if gradient_solve is not None:
            assert gradient_solve.solver == solver
        self.solver_arguments: dict = solver_arguments
        &#34;&#34;&#34; Additional solver-dependent arguments. &#34;&#34;&#34;
        self.result: SolveResult = None
        &#34;&#34;&#34; `SolveResult` storing information about the found solution and the performed solving process. This variable is assigned during the solve. &#34;&#34;&#34;

    @property
    def gradient_solve(self) -&gt; &#39;Solve&#39;:
        &#34;&#34;&#34;
        Parameters to use for the gradient pass when an implicit gradient is computed. The implicit gradient must use the same solver.

        If this property is initialized with `None`, its first evaluation will create a duplicate `Solve` object for the gradient solve.
        Gradient solve information will be stored in `gradient_solve.result`.
        &#34;&#34;&#34;
        if self._gradient_solve is None:
            self._gradient_solve = copy(self)
        return self._gradient_solve</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math.backend._optim.LinearSolve</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Solve.absolute_tolerance"><code class="name">var <span class="ident">absolute_tolerance</span></code></dt>
<dd>
<div class="desc"><p>The final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.gradient_solve"><code class="name">var <span class="ident">gradient_solve</span> : phi.math.backend._optim.Solve</code></dt>
<dd>
<div class="desc"><p>Parameters to use for the gradient pass when an implicit gradient is computed. The implicit gradient must use the same solver.</p>
<p>If this property is initialized with <code>None</code>, its first evaluation will create a duplicate <code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object for the gradient solve.
Gradient solve information will be stored in <code>gradient_solve.result</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def gradient_solve(self) -&gt; &#39;Solve&#39;:
    &#34;&#34;&#34;
    Parameters to use for the gradient pass when an implicit gradient is computed. The implicit gradient must use the same solver.

    If this property is initialized with `None`, its first evaluation will create a duplicate `Solve` object for the gradient solve.
    Gradient solve information will be stored in `gradient_solve.result`.
    &#34;&#34;&#34;
    if self._gradient_solve is None:
        self._gradient_solve = copy(self)
    return self._gradient_solve</code></pre>
</details>
</dd>
<dt id="phi.math.Solve.max_iterations"><code class="name">var <span class="ident">max_iterations</span></code></dt>
<dd>
<div class="desc"><p>Maximum number of iterations to perform before terminating with <code>converged=False</code>.</p></div>
</dd>
<dt id="phi.math.Solve.relative_tolerance"><code class="name">var <span class="ident">relative_tolerance</span></code></dt>
<dd>
<div class="desc"><p>The final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p><code>SolveResult</code> storing information about the found solution and the performed solving process. This variable is assigned during the solve.</p></div>
</dd>
<dt id="phi.math.Solve.solver"><code class="name">var <span class="ident">solver</span></code></dt>
<dd>
<div class="desc"><p>(Optional) Name of method to use.</p></div>
</dd>
<dt id="phi.math.Solve.solver_arguments"><code class="name">var <span class="ident">solver_arguments</span></code></dt>
<dd>
<div class="desc"><p>Additional solver-dependent arguments.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class to represent structured data of one data type.</p>
<p>Unlike with <code>numpy.ndarray</code>, the dimensions of Tensors have names and types.
Additionally, tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.</p>
<p>To check whether a value is a tensor, use <code>isinstance(value, <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>)</code>.</p>
<p>To construct a Tensor, use <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> or one of the basic tensor creation functions,
see <a href="https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation">https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation</a> .</p>
<p>Tensors are not editable.
When backed by an editable native tensor, e.g. a <code>numpy.ndarray</code>, do not edit the underlying data structure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
    &#34;&#34;&#34;
    Abstract base class to represent structured data of one data type.

    Unlike with `numpy.ndarray`, the dimensions of Tensors have names and types.
    Additionally, tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.

    To check whether a value is a tensor, use `isinstance(value, Tensor)`.

    To construct a Tensor, use `phi.math.tensor()`, `phi.math.wrap()` or one of the basic tensor creation functions,
    see https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation .

    Tensors are not editable.
    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
    &#34;&#34;&#34;

    def native(self, order: str or tuple or list = None):
        &#34;&#34;&#34;
        Returns a native tensor object with the dimensions ordered according to `order`.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
          order: optional) list of dimension names. If not given, the current order is kept.
          order: str or tuple or list:  (Default value = None)

        Returns:
          native tensor object
          :raise: ValueError if the tensor cannot be transposed to match target_shape

        &#34;&#34;&#34;
        raise NotImplementedError()

    def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Returns this tensor as a NumPy ndarray object with dimensions ordered according to `order`.
        
        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
        To get a differentiable tensor, use :func:`Tensor.native` instead.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
          order: optional) list of dimension names. If not given, the current order is kept.
          order: str or tuple or list:  (Default value = None)

        Returns:
          NumPy representation
          :raise: ValueError if the tensor cannot be transposed to match target_shape

        &#34;&#34;&#34;
        native = self.native(order=order)
        return choose_backend(native).numpy(native)

    @property
    def dtype(self) -&gt; DType:
        &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def shape(self) -&gt; Shape:
        &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
        raise NotImplementedError()

    def _with_shape_replaced(self, new_shape):
        raise NotImplementedError()

    def _with_natives_replaced(self, natives: list):
        &#34;&#34;&#34; Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34; Equal to `tensor.shape.rank`. &#34;&#34;&#34;
        return self.shape.rank

    @property
    def _is_special(self) -&gt; bool:
        &#34;&#34;&#34;
        Special tensors store additional internal information.
        They should not be converted to native() in intermediate operations.
        
        Tracking tensors are special tensors.
        
        TensorStack prevents performing the actual stack operation if one of its component tensors is special.

        Args:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __len__(self):
        return self.shape.volume if self.rank == 1 else NotImplemented

    def __bool__(self):
        if self.rank == 0:
            return bool(self.native())
        else:
            from phi.math._functions import all_
            return bool(all_(self))

    def __int__(self):
        return int(self.native()) if self.rank == 0 else NotImplemented

    def __float__(self):
        return float(self.native()) if self.rank == 0 else NotImplemented

    def __complex__(self):
        return complex(self.native()) if self.rank == 0 else NotImplemented

    def __index__(self):
        return int(self.native()) if self.rank == 0 and np.issubdtype(self.dtype, int) else NotImplemented

    def _summary_str(self) -&gt; str:
        try:
            from ._functions import all_available, min_, max_
            if all_available(self):
                if self.rank == 0:
                    return str(self.numpy())
                elif self.shape.volume is not None and self.shape.volume &lt;= 6:
                    content = list(np.reshape(self.numpy(), [-1]))
                    content = &#39;, &#39;.join([repr(number) for number in content])
                    if self.shape.rank == 1 and (self.dtype.kind in (bool, int) or self.dtype.precision == get_precision()):
                        return f&#34;({content}) along {self.shape.name}&#34;
                    return f&#34;{self.shape} {self.dtype}  {content}&#34;
                else:
                    min_val, max_val = min_(self), max_(self)
                    return f&#34;{self.shape} {self.dtype}  {min_val} &lt; ... &lt; {max_val}&#34;
            else:
                if self.rank == 0:
                    return f&#34;scalar {self.dtype}&#34;
                else:
                    return f&#34;{self.shape} {self.dtype}&#34;
        except BaseException as err:
            return f&#34;{self.shape}, failed to fetch values with error {err}&#34;

    def __repr__(self):
        return self._summary_str()

    def __getitem__(self, item):
        if isinstance(item, Tensor):
            from ._functions import gather
            return gather(self, item)
        if isinstance(item, (int, slice)):
            assert self.rank == 1
            item = {self.shape.names[0]: item}
        if isinstance(item, (tuple, list)):
            if item[0] == Ellipsis:
                assert len(item) - 1 == self.shape.channel.rank
                item = {name: selection for name, selection in zip(self.shape.channel.names, item[1:])}
            elif len(item) == self.shape.channel.rank:
                item = {name: selection for name, selection in zip(self.shape.channel.names, item)}
            elif len(item) == self.shape.rank:  # legacy indexing
                warnings.warn(&#34;Slicing with sequence should only be used for channel dimensions.&#34;)
                item = {name: selection for name, selection in zip(self.shape.names, item)}
        assert isinstance(item, dict)  # dict mapping name -&gt; slice/int
        return self._getitem(item)

    def _getitem(self, selection: dict) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Slice the tensor along specified dimensions.

        Args:
          selection: dim_name: str -&gt; int or slice
          selection: dict: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Reverses the order of elements along one or multiple dimensions.

        Args:
            *dims: dimensions to flip

        Returns:
            `Tensor` of the same `Shape`
        &#34;&#34;&#34;
        raise NotImplementedError()

    # def __setitem__(self, key, value):
    #     &#34;&#34;&#34;
    #     All tensors are editable.
    #
    #     :param key: list/tuple of slices / indices
    #     :param value:
    #     :return:
    #     &#34;&#34;&#34;
    #     raise NotImplementedError()

    def unstack(self, dimension: str):
        &#34;&#34;&#34;
        Splits this tensor along the specified dimension.
        The returned tensors have the same dimensions as this tensor save the unstacked dimension.

        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

        See Also:
            `TensorDim.unstack()`

        Args:
          dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

        Returns:
          tuple of tensors

        &#34;&#34;&#34;
        raise NotImplementedError()

    def dimension(self, name) -&gt; &#39;TensorDim&#39;:
        &#34;&#34;&#34;
        Returns a reference to a specific dimension of this tensor.
        This is equivalent to the syntax `tensor.&lt;name&gt;`.

        The dimension need not be part of the `Tensor.shape` in which case its size is 1.

        Args:
            name: dimension name

        Returns:
            `TensorDim` corresponding to a dimension of this tensor
        &#34;&#34;&#34;
        return TensorDim(self, name)

    def __getattr__(self, name):
        if name.startswith(&#39;_&#39;):
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        assert name not in (&#39;shape&#39;, &#39;_shape&#39;, &#39;tensor&#39;), name
        return TensorDim(self, name)

    def __add__(self, other):
        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y))

    def __radd__(self, other):
        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x))

    def __sub__(self, other):
        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y))

    def __rsub__(self, other):
        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x))

    def __and__(self, other):
        return self._op2(other, lambda x, y: x &amp; y, lambda x, y: x &amp; y)

    def __or__(self, other):
        return self._op2(other, lambda x, y: x | y, lambda x, y: x | y)

    def __xor__(self, other):
        return self._op2(other, lambda x, y: x ^ y, lambda x, y: x ^ y)

    def __mul__(self, other):
        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y))

    def __rmul__(self, other):
        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x))

    def __truediv__(self, other):
        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y))

    def __rtruediv__(self, other):
        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x))

    def __divmod__(self, other):
        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y))

    def __rdivmod__(self, other):
        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x))

    def __floordiv__(self, other):
        return self._op2(other, lambda x, y: x // y, lambda x, y: x // y)

    def __rfloordiv__(self, other):
        return self._op2(other, lambda x, y: y // x, lambda x, y: y // x)

    def __pow__(self, power, modulo=None):
        assert modulo is None
        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y))

    def __rpow__(self, other):
        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x))

    def __mod__(self, other):
        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y))

    def __rmod__(self, other):
        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x))

    def __eq__(self, other):
        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y))

    def __ne__(self, other):
        return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y))

    def __lt__(self, other):
        return self._op2(other, lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x))

    def __le__(self, other):
        return self._op2(other, lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x))

    def __gt__(self, other):
        return self._op2(other, lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y))

    def __ge__(self, other):
        return self._op2(other, lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y))

    def __abs__(self):
        return self._op1(lambda t: choose_backend(t).abs(t))

    def __copy__(self):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))

    def __deepcopy__(self, memodict={}):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))

    def __neg__(self):
        return self._op1(lambda t: -t)

    def __invert__(self):
        return self._op1(lambda t: ~t)

    def __reversed__(self):
        assert self.shape.channel.rank == 1
        return self[::-1]

    def __iter__(self):
        assert self.rank == 1, f&#34;Can only iterate over 1D tensors but got {self.shape}&#34;
        return iter(self.native())

    def _tensor(self, other):
        if isinstance(other, Tensor):
            return other
        elif isinstance(other, Shape):
            assert self.shape.channel.rank == 1, &#34;Only single-channel tensors support implicit casting from Shape to tensor&#34;
            assert other.rank == self.shape.channel.volume
            return wrap(other.spatial.sizes, names=self.shape.channel.names)
        else:
            backend = choose_backend(other)
            try:
                other_tensor = backend.as_tensor(other, convert_external=True)
                shape = backend.staticshape(other_tensor)
            except ValueError as e:
                raise ValueError(e)
            if len(shape) == 0:
                return NativeTensor(other_tensor, EMPTY_SHAPE)
            elif len(shape) == self.rank:
                return NativeTensor(other_tensor, self.shape.with_sizes(shape))
            elif len(shape) == self.shape.channel.rank:
                other_tensor = wrap(other, names=self.shape.channel.names)
                return other_tensor
            elif len(shape) == 1 and self.shape.channel.rank == 0:
                return NativeTensor(other_tensor, Shape(shape, [&#39;vector&#39;], [CHANNEL_DIM]))
            else:
                raise ValueError(&#34;Cannot broadcast object of rank %d to tensor with shape %s&#34; % (backend.ndims(other), self.shape))

    def _op1(self, native_function):
        &#34;&#34;&#34;
        Transform the values of this tensor given a function that can be applied to any native tensor.

        Args:
          native_function:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _op2(self, other: &#39;Tensor&#39;, operator: Callable, native_function: Callable) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Apply a broadcast operation on two tensors.

        Args:
          other: second argument
          operator: function (Tensor, Tensor) -&gt; Tensor, used to propagate the operation to children tensors to have Python choose the callee
          native_function: function (native tensor, native tensor) -&gt; native tensor
          other: &#39;Tensor&#39;: 
          operator: Callable:
          native_function: Callable:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def _natives(self) -&gt; tuple:
        raise NotImplementedError(self.__class__)

    def _expand(self):
        &#34;&#34;&#34; Expands all compressed tensors to their defined size as if they were being used in `Tensor.native()`. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def __tensor_reduce__(self,
                dims: Tuple[str],
                native_function: Callable,
                collapsed_function: Callable = lambda inner_reduced, collapsed_dims_to_reduce: inner_reduced,
                unaffected_function: Callable = lambda value: value):
        raise NotImplementedError(self.__class__)

    def __simplify__(self):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._tensors.CollapsedTensor</li>
<li>phi.math._tensors.NativeTensor</li>
<li>phi.math._tensors.TensorStack</li>
<li>phi.math._track.ShiftLinOp</li>
<li>phi.math._track.SparseLinearOperation</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Tensor.dtype"><code class="name">var <span class="ident">dtype</span> : phi.math.backend._dtype.DType</code></dt>
<dd>
<div class="desc"><p>Data type of the elements of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self) -&gt; DType:
    &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Equal to <code>tensor.shape.rank</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34; Equal to `tensor.shape.rank`. &#34;&#34;&#34;
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>The <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> lists the dimensions with their sizes, names and types.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; Shape:
    &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Tensor.dimension"><code class="name flex">
<span>def <span class="ident">dimension</span></span>(<span>self, name) ‑> phi.math._tensors.TensorDim</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a reference to a specific dimension of this tensor.
This is equivalent to the syntax <code>tensor.&lt;name&gt;</code>.</p>
<p>The dimension need not be part of the <code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">Tensor.shape</a></code> in which case its size is 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code> corresponding to a dimension of this tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dimension(self, name) -&gt; &#39;TensorDim&#39;:
    &#34;&#34;&#34;
    Returns a reference to a specific dimension of this tensor.
    This is equivalent to the syntax `tensor.&lt;name&gt;`.

    The dimension need not be part of the `Tensor.shape` in which case its size is 1.

    Args:
        name: dimension name

    Returns:
        `TensorDim` corresponding to a dimension of this tensor
    &#34;&#34;&#34;
    return TensorDim(self, name)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self, *dims: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Reverses the order of elements along one or multiple dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*dims</code></strong></dt>
<dd>dimensions to flip</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of the same <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Reverses the order of elements along one or multiple dimensions.

    Args:
        *dims: dimensions to flip

    Returns:
        `Tensor` of the same `Shape`
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>self, order: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native tensor object with the dimensions ordered according to <code>order</code>.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.</p>
<p>If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>optional) list of dimension names. If not given, the current order is kept.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>str or tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>native tensor object
:raise: ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(self, order: str or tuple or list = None):
    &#34;&#34;&#34;
    Returns a native tensor object with the dimensions ordered according to `order`.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
      order: optional) list of dimension names. If not given, the current order is kept.
      order: str or tuple or list:  (Default value = None)

    Returns:
      native tensor object
      :raise: ValueError if the tensor cannot be transposed to match target_shape

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, order: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Returns this tensor as a NumPy ndarray object with dimensions ordered according to <code>order</code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use :func:<code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.</p>
<p>If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>optional) list of dimension names. If not given, the current order is kept.</dd>
<dt><strong><code>order</code></strong></dt>
<dd>str or tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation
:raise: ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Returns this tensor as a NumPy ndarray object with dimensions ordered according to `order`.
    
    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use :func:`Tensor.native` instead.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
      order: optional) list of dimension names. If not given, the current order is kept.
      order: str or tuple or list:  (Default value = None)

    Returns:
      NumPy representation
      :raise: ValueError if the tensor cannot be transposed to match target_shape

    &#34;&#34;&#34;
    native = self.native(order=order)
    return choose_backend(native).numpy(native)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dimension: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits this tensor along the specified dimension.
The returned tensors have the same dimensions as this tensor save the unstacked dimension.</p>
<p>Raises an error if the dimension is not part of the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>See Also:
<code><a title="phi.math.TensorDim.unstack" href="#phi.math.TensorDim.unstack">TensorDim.unstack()</a></code></p>
<h2 id="args">Args</h2>
<p>dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension</p>
<h2 id="returns">Returns</h2>
<p>tuple of tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dimension: str):
    &#34;&#34;&#34;
    Splits this tensor along the specified dimension.
    The returned tensors have the same dimensions as this tensor save the unstacked dimension.

    Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

    See Also:
        `TensorDim.unstack()`

    Args:
      dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

    Returns:
      tuple of tensors

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.TensorDim"><code class="flex name class">
<span>class <span class="ident">TensorDim</span></span>
<span>(</span><span>tensor: phi.math._tensors.Tensor, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Reference to a specific dimension of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>To obtain a <code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code>, use <code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">Tensor.dimension()</a></code> or the syntax <code>tensor.&lt;dim&gt;</code>.</p>
<p>Indexing a <code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code> as <code>tdim[start:stop:step]</code> returns a sliced <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking">https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking</a> .</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorDim:
    &#34;&#34;&#34;
    Reference to a specific dimension of a `Tensor`.

    To obtain a `TensorDim`, use `Tensor.dimension()` or the syntax `tensor.&lt;dim&gt;`.

    Indexing a `TensorDim` as `tdim[start:stop:step]` returns a sliced `Tensor`.

    See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking .
    &#34;&#34;&#34;

    def __init__(self, tensor: Tensor, name: str):
        self.tensor = tensor
        self.name = name

    @property
    def exists(self):
        &#34;&#34;&#34; Whether the dimension is listed in the `Shape` of the `Tensor`. &#34;&#34;&#34;
        return self.name in self.tensor.shape

    def __str__(self):
        &#34;&#34;&#34; Dimension name. &#34;&#34;&#34;
        return self.name

    def __repr__(self):
        return f&#34;Dimension &#39;{self.name}&#39; of {self.tensor.shape}&#34;

    def unstack(self, size: int or None = None, to_numpy=False, to_python=False) -&gt; tuple:
        &#34;&#34;&#34;
        See `unstack_spatial()`.

        Args:
            size: (optional)
                None: unstack along this dimension, error if dimension does not exist
                int: repeating unstack if dimension does not exist
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            sliced tensors
        &#34;&#34;&#34;
        if size is None:
            result = self.tensor.unstack(self.name)
        else:
            if self.exists:
                unstacked = self.tensor.unstack(self.name)
                assert len(unstacked) == size, f&#34;Size of dimension {self.name} does not match {size}.&#34;
                result = unstacked
            else:
                result = (self.tensor,) * size
        if to_numpy or to_python:
            result = tuple(component.numpy() for component in result)
            if to_python:
                result = tuple(component.tolist() for component in result)
        return result

    def optional_unstack(self, to_numpy=False, to_python=False):
        &#34;&#34;&#34;
        Unstacks the `Tensor` along this dimension if the dimension is listed in the `Shape`.
        Otherwise returns the original `Tensor`.

        Args:
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            `tuple` of sliced tensors or original `Tensor`
        &#34;&#34;&#34;
        if self.exists:
            return self.unstack(to_numpy=to_numpy, to_python=to_python)
        else:
            if to_numpy or to_python:
                result = self.tensor.numpy()
                if to_python:
                    return result.tolist()
                return result
            return self.tensor

    def unstack_spatial(self, components: str or tuple or list, to_numpy=False, to_python=False) -&gt; tuple:
        &#34;&#34;&#34;
        Slices the tensor along this dimension, returning only the selected components in the specified order.

        Args:
            components:
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            selected components
        &#34;&#34;&#34;
        if isinstance(components, str):
            components = _shape.parse_dim_order(components)
        if self.exists:
            spatial = self.tensor.shape.spatial
            result = []
            if spatial.is_empty:
                spatial = [GLOBAL_AXIS_ORDER.axis_name(i, len(components)) for i in range(len(components))]
            for dim in components:
                component_index = spatial.index(dim)
                result.append(self.tensor[{self.name: component_index}])
        else:
            result = [self.tensor] * len(components)
        if to_numpy or to_python:
            result = tuple(component.numpy() for component in result)
            if to_python:
                result = tuple(component.tolist() for component in result)
        return tuple(result)

    @property
    def index(self):
        &#34;&#34;&#34; The index of this dimension in the `Shape` of the `Tensor`. &#34;&#34;&#34;
        return self.tensor.shape.index(self.name)

    def __int__(self):
        return self.index

    def __len__(self):
        return self.tensor.shape.get_size(self.name)

    @property
    def size(self):
        &#34;&#34;&#34; Length of this tensor dimension as listed in the `Shape`, otherwise `1`. &#34;&#34;&#34;
        if self.exists:
            return self.tensor.shape.get_size(self.name)
        else:
            return 1

    def as_batch(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. &#34;&#34;&#34;
        return self._as(BATCH_DIM, name)

    def as_spatial(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. &#34;&#34;&#34;
        return self._as(SPATIAL_DIM, name)

    def as_channel(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. &#34;&#34;&#34;
        return self._as(CHANNEL_DIM, name)

    def _as(self, dim_type: int, name: str or None):
        shape = self.tensor.shape
        new_types = list(shape.types)
        new_types[self.index] = dim_type
        new_names = shape.names
        if name is not None:
            new_names = list(new_names)
            new_names[self.index] = name
        new_shape = Shape(shape.sizes, new_names, new_types)
        return self.tensor._with_shape_replaced(new_shape)

    @property
    def _dim_type(self):
        return self.tensor.shape.get_type(self.name)

    @property
    def is_spatial(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *spatial*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == SPATIAL_DIM

    @property
    def is_batch(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *batch*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == BATCH_DIM

    @property
    def is_channel(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *channel*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == CHANNEL_DIM

    def __getitem__(self, item):
        if isinstance(item, str):
            item = self.tensor.shape.spatial.index(item)
        return self.tensor[{self.name: item}]

    def flip(self):
        &#34;&#34;&#34; Flips the element order along this dimension and returns the result as a `Tensor`. &#34;&#34;&#34;
        return self.tensor.flip(self.name)

    def split(self, split_dimensions: Shape):
        &#34;&#34;&#34; See `phi.math.split_dimension()` &#34;&#34;&#34;
        from ._functions import split_dimension
        return split_dimension(self, split_dimensions)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.TensorDim.exists"><code class="name">var <span class="ident">exists</span></code></dt>
<dd>
<div class="desc"><p>Whether the dimension is listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def exists(self):
    &#34;&#34;&#34; Whether the dimension is listed in the `Shape` of the `Tensor`. &#34;&#34;&#34;
    return self.name in self.tensor.shape</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.index"><code class="name">var <span class="ident">index</span></code></dt>
<dd>
<div class="desc"><p>The index of this dimension in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def index(self):
    &#34;&#34;&#34; The index of this dimension in the `Shape` of the `Tensor`. &#34;&#34;&#34;
    return self.tensor.shape.index(self.name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_batch"><code class="name">var <span class="ident">is_batch</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>batch</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_batch(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *batch*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == BATCH_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_channel"><code class="name">var <span class="ident">is_channel</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>channel</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_channel(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *channel*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == CHANNEL_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_spatial"><code class="name">var <span class="ident">is_spatial</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>spatial</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_spatial(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *spatial*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == SPATIAL_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.size"><code class="name">var <span class="ident">size</span></code></dt>
<dd>
<div class="desc"><p>Length of this tensor dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, otherwise <code>1</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self):
    &#34;&#34;&#34; Length of this tensor dimension as listed in the `Shape`, otherwise `1`. &#34;&#34;&#34;
    if self.exists:
        return self.tensor.shape.get_size(self.name)
    else:
        return 1</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.TensorDim.as_batch"><code class="name flex">
<span>def <span class="ident">as_batch</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>batch</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_batch(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. &#34;&#34;&#34;
    return self._as(BATCH_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.as_channel"><code class="name flex">
<span>def <span class="ident">as_channel</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>channel</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_channel(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. &#34;&#34;&#34;
    return self._as(CHANNEL_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.as_spatial"><code class="name flex">
<span>def <span class="ident">as_spatial</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>spatial</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_spatial(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. &#34;&#34;&#34;
    return self._as(SPATIAL_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Flips the element order along this dimension and returns the result as a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self):
    &#34;&#34;&#34; Flips the element order along this dimension and returns the result as a `Tensor`. &#34;&#34;&#34;
    return self.tensor.flip(self.name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.optional_unstack"><code class="name flex">
<span>def <span class="ident">optional_unstack</span></span>(<span>self, to_numpy=False, to_python=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Unstacks the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> along this dimension if the dimension is listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
Otherwise returns the original <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of sliced tensors or original <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optional_unstack(self, to_numpy=False, to_python=False):
    &#34;&#34;&#34;
    Unstacks the `Tensor` along this dimension if the dimension is listed in the `Shape`.
    Otherwise returns the original `Tensor`.

    Args:
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        `tuple` of sliced tensors or original `Tensor`
    &#34;&#34;&#34;
    if self.exists:
        return self.unstack(to_numpy=to_numpy, to_python=to_python)
    else:
        if to_numpy or to_python:
            result = self.tensor.numpy()
            if to_python:
                return result.tolist()
            return result
        return self.tensor</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, split_dimensions: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split(self, split_dimensions: Shape):
    &#34;&#34;&#34; See `phi.math.split_dimension()` &#34;&#34;&#34;
    from ._functions import split_dimension
    return split_dimension(self, split_dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, size: int = None, to_numpy=False, to_python=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>See <code>unstack_spatial()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>(optional)
None: unstack along this dimension, error if dimension does not exist
int: repeating unstack if dimension does not exist</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>sliced tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, size: int or None = None, to_numpy=False, to_python=False) -&gt; tuple:
    &#34;&#34;&#34;
    See `unstack_spatial()`.

    Args:
        size: (optional)
            None: unstack along this dimension, error if dimension does not exist
            int: repeating unstack if dimension does not exist
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        sliced tensors
    &#34;&#34;&#34;
    if size is None:
        result = self.tensor.unstack(self.name)
    else:
        if self.exists:
            unstacked = self.tensor.unstack(self.name)
            assert len(unstacked) == size, f&#34;Size of dimension {self.name} does not match {size}.&#34;
            result = unstacked
        else:
            result = (self.tensor,) * size
    if to_numpy or to_python:
        result = tuple(component.numpy() for component in result)
        if to_python:
            result = tuple(component.tolist() for component in result)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.unstack_spatial"><code class="name flex">
<span>def <span class="ident">unstack_spatial</span></span>(<span>self, components: str, to_numpy=False, to_python=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Slices the tensor along this dimension, returning only the selected components in the specified order.</p>
<h2 id="args">Args</h2>
<dl>
<dt>components:</dt>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>selected components</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack_spatial(self, components: str or tuple or list, to_numpy=False, to_python=False) -&gt; tuple:
    &#34;&#34;&#34;
    Slices the tensor along this dimension, returning only the selected components in the specified order.

    Args:
        components:
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        selected components
    &#34;&#34;&#34;
    if isinstance(components, str):
        components = _shape.parse_dim_order(components)
    if self.exists:
        spatial = self.tensor.shape.spatial
        result = []
        if spatial.is_empty:
            spatial = [GLOBAL_AXIS_ORDER.axis_name(i, len(components)) for i in range(len(components))]
        for dim in components:
            component_index = spatial.index(dim)
            result.append(self.tensor[{self.name: component_index}])
    else:
        result = [self.tensor] * len(components)
    if to_numpy or to_python:
        result = tuple(component.numpy() for component in result)
        if to_python:
            result = tuple(component.tolist() for component in result)
    return tuple(result)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi" href="../index.html">phi</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></li>
<li><code><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="phi.math.NUMPY_BACKEND" href="#phi.math.NUMPY_BACKEND">NUMPY_BACKEND</a></code></li>
<li><code><a title="phi.math.PI" href="#phi.math.PI">PI</a></code></li>
<li><code><a title="phi.math.SCIPY_BACKEND" href="#phi.math.SCIPY_BACKEND">SCIPY_BACKEND</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phi.math.abs" href="#phi.math.abs">abs</a></code></li>
<li><code><a title="phi.math.abs_square" href="#phi.math.abs_square">abs_square</a></code></li>
<li><code><a title="phi.math.all" href="#phi.math.all">all</a></code></li>
<li><code><a title="phi.math.all_available" href="#phi.math.all_available">all_available</a></code></li>
<li><code><a title="phi.math.any" href="#phi.math.any">any</a></code></li>
<li><code><a title="phi.math.assert_close" href="#phi.math.assert_close">assert_close</a></code></li>
<li><code><a title="phi.math.batch_shape" href="#phi.math.batch_shape">batch_shape</a></code></li>
<li><code><a title="phi.math.batch_stack" href="#phi.math.batch_stack">batch_stack</a></code></li>
<li><code><a title="phi.math.boolean_mask" href="#phi.math.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phi.math.cast" href="#phi.math.cast">cast</a></code></li>
<li><code><a title="phi.math.ceil" href="#phi.math.ceil">ceil</a></code></li>
<li><code><a title="phi.math.channel_shape" href="#phi.math.channel_shape">channel_shape</a></code></li>
<li><code><a title="phi.math.channel_stack" href="#phi.math.channel_stack">channel_stack</a></code></li>
<li><code><a title="phi.math.choose_backend" href="#phi.math.choose_backend">choose_backend</a></code></li>
<li><code><a title="phi.math.clip" href="#phi.math.clip">clip</a></code></li>
<li><code><a title="phi.math.close" href="#phi.math.close">close</a></code></li>
<li><code><a title="phi.math.closest_grid_values" href="#phi.math.closest_grid_values">closest_grid_values</a></code></li>
<li><code><a title="phi.math.concat" href="#phi.math.concat">concat</a></code></li>
<li><code><a title="phi.math.conv" href="#phi.math.conv">conv</a></code></li>
<li><code><a title="phi.math.cos" href="#phi.math.cos">cos</a></code></li>
<li><code><a title="phi.math.cross_product" href="#phi.math.cross_product">cross_product</a></code></li>
<li><code><a title="phi.math.divide_no_nan" href="#phi.math.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="phi.math.dot" href="#phi.math.dot">dot</a></code></li>
<li><code><a title="phi.math.downsample2x" href="#phi.math.downsample2x">downsample2x</a></code></li>
<li><code><a title="phi.math.dtype" href="#phi.math.dtype">dtype</a></code></li>
<li><code><a title="phi.math.einsum" href="#phi.math.einsum">einsum</a></code></li>
<li><code><a title="phi.math.exp" href="#phi.math.exp">exp</a></code></li>
<li><code><a title="phi.math.expand" href="#phi.math.expand">expand</a></code></li>
<li><code><a title="phi.math.expand_batch" href="#phi.math.expand_batch">expand_batch</a></code></li>
<li><code><a title="phi.math.expand_channel" href="#phi.math.expand_channel">expand_channel</a></code></li>
<li><code><a title="phi.math.expand_spatial" href="#phi.math.expand_spatial">expand_spatial</a></code></li>
<li><code><a title="phi.math.extrapolate_valid_values" href="#phi.math.extrapolate_valid_values">extrapolate_valid_values</a></code></li>
<li><code><a title="phi.math.fft" href="#phi.math.fft">fft</a></code></li>
<li><code><a title="phi.math.fftfreq" href="#phi.math.fftfreq">fftfreq</a></code></li>
<li><code><a title="phi.math.flatten" href="#phi.math.flatten">flatten</a></code></li>
<li><code><a title="phi.math.floor" href="#phi.math.floor">floor</a></code></li>
<li><code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace</a></code></li>
<li><code><a title="phi.math.fourier_poisson" href="#phi.math.fourier_poisson">fourier_poisson</a></code></li>
<li><code><a title="phi.math.frequency_loss" href="#phi.math.frequency_loss">frequency_loss</a></code></li>
<li><code><a title="phi.math.gather" href="#phi.math.gather">gather</a></code></li>
<li><code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision</a></code></li>
<li><code><a title="phi.math.gradient" href="#phi.math.gradient">gradient</a></code></li>
<li><code><a title="phi.math.gradient_function" href="#phi.math.gradient_function">gradient_function</a></code></li>
<li><code><a title="phi.math.gradients" href="#phi.math.gradients">gradients</a></code></li>
<li><code><a title="phi.math.grid_sample" href="#phi.math.grid_sample">grid_sample</a></code></li>
<li><code><a title="phi.math.ifft" href="#phi.math.ifft">ifft</a></code></li>
<li><code><a title="phi.math.imag" href="#phi.math.imag">imag</a></code></li>
<li><code><a title="phi.math.isfinite" href="#phi.math.isfinite">isfinite</a></code></li>
<li><code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions</a></code></li>
<li><code><a title="phi.math.l1_loss" href="#phi.math.l1_loss">l1_loss</a></code></li>
<li><code><a title="phi.math.l2_loss" href="#phi.math.l2_loss">l2_loss</a></code></li>
<li><code><a title="phi.math.l_n_loss" href="#phi.math.l_n_loss">l_n_loss</a></code></li>
<li><code><a title="phi.math.laplace" href="#phi.math.laplace">laplace</a></code></li>
<li><code><a title="phi.math.linspace" href="#phi.math.linspace">linspace</a></code></li>
<li><code><a title="phi.math.map" href="#phi.math.map">map</a></code></li>
<li><code><a title="phi.math.matmul" href="#phi.math.matmul">matmul</a></code></li>
<li><code><a title="phi.math.max" href="#phi.math.max">max</a></code></li>
<li><code><a title="phi.math.maximum" href="#phi.math.maximum">maximum</a></code></li>
<li><code><a title="phi.math.mean" href="#phi.math.mean">mean</a></code></li>
<li><code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.min" href="#phi.math.min">min</a></code></li>
<li><code><a title="phi.math.minimize" href="#phi.math.minimize">minimize</a></code></li>
<li><code><a title="phi.math.minimum" href="#phi.math.minimum">minimum</a></code></li>
<li><code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero</a></code></li>
<li><code><a title="phi.math.normalize_to" href="#phi.math.normalize_to">normalize_to</a></code></li>
<li><code><a title="phi.math.ones" href="#phi.math.ones">ones</a></code></li>
<li><code><a title="phi.math.ones_like" href="#phi.math.ones_like">ones_like</a></code></li>
<li><code><a title="phi.math.pad" href="#phi.math.pad">pad</a></code></li>
<li><code><a title="phi.math.precision" href="#phi.math.precision">precision</a></code></li>
<li><code><a title="phi.math.print" href="#phi.math.print">print</a></code></li>
<li><code><a title="phi.math.prod" href="#phi.math.prod">prod</a></code></li>
<li><code><a title="phi.math.random_normal" href="#phi.math.random_normal">random_normal</a></code></li>
<li><code><a title="phi.math.random_uniform" href="#phi.math.random_uniform">random_uniform</a></code></li>
<li><code><a title="phi.math.real" href="#phi.math.real">real</a></code></li>
<li><code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients</a></code></li>
<li><code><a title="phi.math.round" href="#phi.math.round">round</a></code></li>
<li><code><a title="phi.math.sample_subgrid" href="#phi.math.sample_subgrid">sample_subgrid</a></code></li>
<li><code><a title="phi.math.scatter" href="#phi.math.scatter">scatter</a></code></li>
<li><code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision</a></code></li>
<li><code><a title="phi.math.shape" href="#phi.math.shape">shape</a></code></li>
<li><code><a title="phi.math.shift" href="#phi.math.shift">shift</a></code></li>
<li><code><a title="phi.math.sign" href="#phi.math.sign">sign</a></code></li>
<li><code><a title="phi.math.sin" href="#phi.math.sin">sin</a></code></li>
<li><code><a title="phi.math.solve" href="#phi.math.solve">solve</a></code></li>
<li><code><a title="phi.math.spatial_pad" href="#phi.math.spatial_pad">spatial_pad</a></code></li>
<li><code><a title="phi.math.spatial_shape" href="#phi.math.spatial_shape">spatial_shape</a></code></li>
<li><code><a title="phi.math.spatial_stack" href="#phi.math.spatial_stack">spatial_stack</a></code></li>
<li><code><a title="phi.math.spatial_sum" href="#phi.math.spatial_sum">spatial_sum</a></code></li>
<li><code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension</a></code></li>
<li><code><a title="phi.math.sqrt" href="#phi.math.sqrt">sqrt</a></code></li>
<li><code><a title="phi.math.std" href="#phi.math.std">std</a></code></li>
<li><code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="phi.math.sum" href="#phi.math.sum">sum</a></code></li>
<li><code><a title="phi.math.tensor" href="#phi.math.tensor">tensor</a></code></li>
<li><code><a title="phi.math.tensors" href="#phi.math.tensors">tensors</a></code></li>
<li><code><a title="phi.math.tile" href="#phi.math.tile">tile</a></code></li>
<li><code><a title="phi.math.to_complex" href="#phi.math.to_complex">to_complex</a></code></li>
<li><code><a title="phi.math.to_float" href="#phi.math.to_float">to_float</a></code></li>
<li><code><a title="phi.math.to_int" href="#phi.math.to_int">to_int</a></code></li>
<li><code><a title="phi.math.trace_function" href="#phi.math.trace_function">trace_function</a></code></li>
<li><code><a title="phi.math.transpose" href="#phi.math.transpose">transpose</a></code></li>
<li><code><a title="phi.math.unstack" href="#phi.math.unstack">unstack</a></code></li>
<li><code><a title="phi.math.upsample2x" href="#phi.math.upsample2x">upsample2x</a></code></li>
<li><code><a title="phi.math.vec_abs" href="#phi.math.vec_abs">vec_abs</a></code></li>
<li><code><a title="phi.math.vec_squared" href="#phi.math.vec_squared">vec_squared</a></code></li>
<li><code><a title="phi.math.where" href="#phi.math.where">where</a></code></li>
<li><code><a title="phi.math.wrap" href="#phi.math.wrap">wrap</a></code></li>
<li><code><a title="phi.math.zeros" href="#phi.math.zeros">zeros</a></code></li>
<li><code><a title="phi.math.zeros_like" href="#phi.math.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code></h4>
<ul class="">
<li><code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">bits</a></code></li>
<li><code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">itemsize</a></code></li>
<li><code><a title="phi.math.DType.kind" href="#phi.math.DType.kind">kind</a></code></li>
<li><code><a title="phi.math.DType.precision" href="#phi.math.DType.precision">precision</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Extrapolation" href="#phi.math.Extrapolation">Extrapolation</a></code></h4>
<ul class="">
<li><code><a title="phi.math.Extrapolation.gradient" href="#phi.math.Extrapolation.gradient">gradient</a></code></li>
<li><code><a title="phi.math.Extrapolation.is_copy_pad" href="#phi.math.Extrapolation.is_copy_pad">is_copy_pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.native_grid_sample_mode" href="#phi.math.Extrapolation.native_grid_sample_mode">native_grid_sample_mode</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad" href="#phi.math.Extrapolation.pad">pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad_values" href="#phi.math.Extrapolation.pad_values">pad_values</a></code></li>
<li><code><a title="phi.math.Extrapolation.to_dict" href="#phi.math.Extrapolation.to_dict">to_dict</a></code></li>
<li><code><a title="phi.math.Extrapolation.transform_coordinates" href="#phi.math.Extrapolation.transform_coordinates">transform_coordinates</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.LinearSolve" href="#phi.math.LinearSolve">LinearSolve</a></code></h4>
<ul class="">
<li><code><a title="phi.math.LinearSolve.bake" href="#phi.math.LinearSolve.bake">bake</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Shape.after_gather" href="#phi.math.Shape.after_gather">after_gather</a></code></li>
<li><code><a title="phi.math.Shape.after_pad" href="#phi.math.Shape.after_pad">after_pad</a></code></li>
<li><code><a title="phi.math.Shape.alphabetically" href="#phi.math.Shape.alphabetically">alphabetically</a></code></li>
<li><code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">batch</a></code></li>
<li><code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">batch_rank</a></code></li>
<li><code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">channel</a></code></li>
<li><code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">channel_rank</a></code></li>
<li><code><a title="phi.math.Shape.combined" href="#phi.math.Shape.combined">combined</a></code></li>
<li><code><a title="phi.math.Shape.defined" href="#phi.math.Shape.defined">defined</a></code></li>
<li><code><a title="phi.math.Shape.dimensions" href="#phi.math.Shape.dimensions">dimensions</a></code></li>
<li><code><a title="phi.math.Shape.expand" href="#phi.math.Shape.expand">expand</a></code></li>
<li><code><a title="phi.math.Shape.expand_batch" href="#phi.math.Shape.expand_batch">expand_batch</a></code></li>
<li><code><a title="phi.math.Shape.expand_channel" href="#phi.math.Shape.expand_channel">expand_channel</a></code></li>
<li><code><a title="phi.math.Shape.expand_spatial" href="#phi.math.Shape.expand_spatial">expand_spatial</a></code></li>
<li><code><a title="phi.math.Shape.extend" href="#phi.math.Shape.extend">extend</a></code></li>
<li><code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">get_size</a></code></li>
<li><code><a title="phi.math.Shape.get_type" href="#phi.math.Shape.get_type">get_type</a></code></li>
<li><code><a title="phi.math.Shape.index" href="#phi.math.Shape.index">index</a></code></li>
<li><code><a title="phi.math.Shape.indices" href="#phi.math.Shape.indices">indices</a></code></li>
<li><code><a title="phi.math.Shape.is_batch" href="#phi.math.Shape.is_batch">is_batch</a></code></li>
<li><code><a title="phi.math.Shape.is_channel" href="#phi.math.Shape.is_channel">is_channel</a></code></li>
<li><code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">is_empty</a></code></li>
<li><code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">is_non_uniform</a></code></li>
<li><code><a title="phi.math.Shape.is_spatial" href="#phi.math.Shape.is_spatial">is_spatial</a></code></li>
<li><code><a title="phi.math.Shape.mask" href="#phi.math.Shape.mask">mask</a></code></li>
<li><code><a title="phi.math.Shape.meshgrid" href="#phi.math.Shape.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.Shape.name" href="#phi.math.Shape.name">name</a></code></li>
<li><code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">named_sizes</a></code></li>
<li><code><a title="phi.math.Shape.names" href="#phi.math.Shape.names">names</a></code></li>
<li><code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">non_batch</a></code></li>
<li><code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">non_channel</a></code></li>
<li><code><a title="phi.math.Shape.non_singleton" href="#phi.math.Shape.non_singleton">non_singleton</a></code></li>
<li><code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">non_spatial</a></code></li>
<li><code><a title="phi.math.Shape.non_zero" href="#phi.math.Shape.non_zero">non_zero</a></code></li>
<li><code><a title="phi.math.Shape.normal_order" href="#phi.math.Shape.normal_order">normal_order</a></code></li>
<li><code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">only</a></code></li>
<li><code><a title="phi.math.Shape.order" href="#phi.math.Shape.order">order</a></code></li>
<li><code><a title="phi.math.Shape.order_group" href="#phi.math.Shape.order_group">order_group</a></code></li>
<li><code><a title="phi.math.Shape.perm" href="#phi.math.Shape.perm">perm</a></code></li>
<li><code><a title="phi.math.Shape.product" href="#phi.math.Shape.product">product</a></code></li>
<li><code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">rank</a></code></li>
<li><code><a title="phi.math.Shape.reduce" href="#phi.math.Shape.reduce">reduce</a></code></li>
<li><code><a title="phi.math.Shape.reorder" href="#phi.math.Shape.reorder">reorder</a></code></li>
<li><code><a title="phi.math.Shape.select" href="#phi.math.Shape.select">select</a></code></li>
<li><code><a title="phi.math.Shape.sequence_get" href="#phi.math.Shape.sequence_get">sequence_get</a></code></li>
<li><code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">shape</a></code></li>
<li><code><a title="phi.math.Shape.singleton" href="#phi.math.Shape.singleton">singleton</a></code></li>
<li><code><a title="phi.math.Shape.sizes" href="#phi.math.Shape.sizes">sizes</a></code></li>
<li><code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">spatial</a></code></li>
<li><code><a title="phi.math.Shape.spatial_dict" href="#phi.math.Shape.spatial_dict">spatial_dict</a></code></li>
<li><code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">spatial_rank</a></code></li>
<li><code><a title="phi.math.Shape.to_batch" href="#phi.math.Shape.to_batch">to_batch</a></code></li>
<li><code><a title="phi.math.Shape.undefined" href="#phi.math.Shape.undefined">undefined</a></code></li>
<li><code><a title="phi.math.Shape.unstack" href="#phi.math.Shape.unstack">unstack</a></code></li>
<li><code><a title="phi.math.Shape.volume" href="#phi.math.Shape.volume">volume</a></code></li>
<li><code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">well_defined</a></code></li>
<li><code><a title="phi.math.Shape.with_names" href="#phi.math.Shape.with_names">with_names</a></code></li>
<li><code><a title="phi.math.Shape.with_size" href="#phi.math.Shape.with_size">with_size</a></code></li>
<li><code><a title="phi.math.Shape.with_sizes" href="#phi.math.Shape.with_sizes">with_sizes</a></code></li>
<li><code><a title="phi.math.Shape.with_types" href="#phi.math.Shape.with_types">with_types</a></code></li>
<li><code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">without</a></code></li>
<li><code><a title="phi.math.Shape.zero" href="#phi.math.Shape.zero">zero</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Solve.absolute_tolerance" href="#phi.math.Solve.absolute_tolerance">absolute_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.gradient_solve" href="#phi.math.Solve.gradient_solve">gradient_solve</a></code></li>
<li><code><a title="phi.math.Solve.max_iterations" href="#phi.math.Solve.max_iterations">max_iterations</a></code></li>
<li><code><a title="phi.math.Solve.relative_tolerance" href="#phi.math.Solve.relative_tolerance">relative_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.result" href="#phi.math.Solve.result">result</a></code></li>
<li><code><a title="phi.math.Solve.solver" href="#phi.math.Solve.solver">solver</a></code></li>
<li><code><a title="phi.math.Solve.solver_arguments" href="#phi.math.Solve.solver_arguments">solver_arguments</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">dimension</a></code></li>
<li><code><a title="phi.math.Tensor.dtype" href="#phi.math.Tensor.dtype">dtype</a></code></li>
<li><code><a title="phi.math.Tensor.flip" href="#phi.math.Tensor.flip">flip</a></code></li>
<li><code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">native</a></code></li>
<li><code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">numpy</a></code></li>
<li><code><a title="phi.math.Tensor.rank" href="#phi.math.Tensor.rank">rank</a></code></li>
<li><code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">shape</a></code></li>
<li><code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">unstack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.TensorDim.as_batch" href="#phi.math.TensorDim.as_batch">as_batch</a></code></li>
<li><code><a title="phi.math.TensorDim.as_channel" href="#phi.math.TensorDim.as_channel">as_channel</a></code></li>
<li><code><a title="phi.math.TensorDim.as_spatial" href="#phi.math.TensorDim.as_spatial">as_spatial</a></code></li>
<li><code><a title="phi.math.TensorDim.exists" href="#phi.math.TensorDim.exists">exists</a></code></li>
<li><code><a title="phi.math.TensorDim.flip" href="#phi.math.TensorDim.flip">flip</a></code></li>
<li><code><a title="phi.math.TensorDim.index" href="#phi.math.TensorDim.index">index</a></code></li>
<li><code><a title="phi.math.TensorDim.is_batch" href="#phi.math.TensorDim.is_batch">is_batch</a></code></li>
<li><code><a title="phi.math.TensorDim.is_channel" href="#phi.math.TensorDim.is_channel">is_channel</a></code></li>
<li><code><a title="phi.math.TensorDim.is_spatial" href="#phi.math.TensorDim.is_spatial">is_spatial</a></code></li>
<li><code><a title="phi.math.TensorDim.optional_unstack" href="#phi.math.TensorDim.optional_unstack">optional_unstack</a></code></li>
<li><code><a title="phi.math.TensorDim.size" href="#phi.math.TensorDim.size">size</a></code></li>
<li><code><a title="phi.math.TensorDim.split" href="#phi.math.TensorDim.split">split</a></code></li>
<li><code><a title="phi.math.TensorDim.unstack" href="#phi.math.TensorDim.unstack">unstack</a></code></li>
<li><code><a title="phi.math.TensorDim.unstack_spatial" href="#phi.math.TensorDim.unstack_spatial">unstack_spatial</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>