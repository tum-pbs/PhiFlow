<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>phi.math API documentation</title>
<meta name="description" content="Vectorized operations, tensors with named dimensions …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.math</code></h1>
</header>
<section id="section-intro">
<p>Vectorized operations, tensors with named dimensions.</p>
<p>This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.</p>
<p>Main classes: <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, <code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code>, <code><a title="phi.math.Extrapolation" href="#phi.math.Extrapolation">Extrapolation</a></code>.</p>
<p>The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Vectorized operations, tensors with named dimensions.

This package provides a common interface for tensor operations.
Is internally uses NumPy, TensorFlow or PyTorch.

Main classes: `Tensor`, `Shape`, `DType`, `Extrapolation`.

The provided operations are not implemented directly.
Instead, they delegate the actual computation to either NumPy, TensorFlow or PyTorch, depending on the configuration.
This allows the user to write simulation code once and have it run with various computation backends.

See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html
&#34;&#34;&#34;

from .backend._dtype import DType
from .backend import NUMPY_BACKEND, precision, set_global_precision, get_precision

from ._config import GLOBAL_AXIS_ORDER
from ._shape import Shape, spatial_shape, EMPTY_SHAPE, batch_shape, channel_shape, shape
from ._tensors import wrap, tensor, tensors, Tensor, TensorDim, TensorLike
from .extrapolation import Extrapolation
from ._ops import (
    choose_backend_t as choose_backend, all_available, convert, seed,
    native, numpy, reshaped_native, reshaped_tensor, copy, native_call,
    print_ as print,
    map_ as map,
    zeros, ones, fftfreq, random_normal, random_uniform, meshgrid, linspace, arange as range, range_tensor,  # creation operators (use default backend)
    zeros_like, ones_like,
    batch_stack, spatial_stack, channel_stack, unstack, concat,
    pad,
    join_dimensions, split_dimension, flatten, expand, expand_batch, expand_spatial, expand_channel, transpose,  # reshape operations
    divide_no_nan,
    where, nonzero,
    sum_ as sum, mean, std, prod, max_ as max, min_ as min, any_ as any, all_ as all,  # reduce
    dot,
    abs_ as abs, sign,
    round_ as round, ceil, floor,
    maximum, minimum, clip,
    sqrt, exp, sin, cos, tan, log, log2, log10,
    to_float, to_int32, to_int64, to_complex, imag, real,
    boolean_mask,
    isfinite,
    closest_grid_values, grid_sample, scatter, gather,
    fft, ifft, convolve,
    dtype, cast,
    close, assert_close,
    record_gradients, gradients, stop_gradient
)
from ._nd import (
    shift,
    spatial_sum, vec_abs, vec_squared, cross_product,
    normalize_to,
    l1_loss, l2_loss, frequency_loss,
    spatial_gradient, laplace,
    fourier_laplace, fourier_poisson, abs_square,
    downsample2x, upsample2x, sample_subgrid,
    extrapolate_valid_values,
)
from ._functional import (
    LinearFunction, jit_compile_linear, jit_compile,
    functional_gradient, custom_gradient, print_gradient,
    solve_linear, solve_nonlinear, minimize, Solve, SolveInfo, ConvergenceException, NotConverged, Diverged, SolveTape,
)


PI = 3.14159265358979323846
&#34;&#34;&#34;Value of π to double precision &#34;&#34;&#34;
pi = PI

NUMPY_BACKEND = NUMPY_BACKEND  # to show up in pdoc
&#34;&#34;&#34;Default backend for NumPy arrays and SciPy objects.&#34;&#34;&#34;

__all__ = [key for key in globals().keys() if not key.startswith(&#39;_&#39;)]

__pdoc__ = {
    &#39;Extrapolation.__init__&#39;: False,
    &#39;Shape.__init__&#39;: False,
    &#39;SolveInfo.__init__&#39;: False,
    &#39;TensorDim.__init__&#39;: False,
    &#39;ConvergenceException.__init__&#39;: False,
    &#39;Diverged.__init__&#39;: False,
    &#39;NotConverged.__init__&#39;: False,
    &#39;LinearFunction.__init__&#39;: False,
    &#39;TensorLike.__variable_attrs__&#39;: True,
    &#39;TensorLike.__value_attrs__&#39;: True,
    &#39;TensorLike.__with_tattrs__&#39;: True,
}</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></dt>
<dd>
<div class="desc"><p>Low-level library wrappers for delegating vector operations.</p></div>
</dd>
<dt><code class="name"><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></dt>
<dd>
<div class="desc"><p>Defines standard extrapolations …</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="phi.math.NUMPY_BACKEND"><code class="name">var <span class="ident">NUMPY_BACKEND</span></code></dt>
<dd>
<div class="desc"><p>Default backend for NumPy arrays and SciPy objects.</p></div>
</dd>
<dt id="phi.math.PI"><code class="name">var <span class="ident">PI</span></code></dt>
<dd>
<div class="desc"><p>Value of π to double precision</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.math.abs"><code class="name flex">
<span>def <span class="ident">abs</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>||x||<sub>1</sub></em>.
Complex <code>x</code> result in matching precision float values.</p>
<p><em>Note</em>: The gradient of this operation is undefined for <em>x=0</em>.
TensorFlow and PyTorch return 0 while Jax returns 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Absolute value of <code>x</code> of same type as <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_(x) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes *||x||&lt;sub&gt;1&lt;/sub&gt;*.
    Complex `x` result in matching precision float values.

    *Note*: The gradient of this operation is undefined for *x=0*.
    TensorFlow and PyTorch return 0 while Jax returns 1.

    Args:
        x: `Tensor` or `TensorLike`

    Returns:
        Absolute value of `x` of same type as `x`.
    &#34;&#34;&#34;
    return _backend_op1(x, Backend.abs)</code></pre>
</details>
</dd>
<dt id="phi.math.abs_square"><code class="name flex">
<span>def <span class="ident">abs_square</span></span>(<span>complex_values: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Squared magnitude of complex values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>complex_values</code></strong></dt>
<dd>complex <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dt>
<dd>real valued magnitude squared</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def abs_square(complex_values: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Squared magnitude of complex values.

    Args:
      complex_values: complex `Tensor`

    Returns:
        Tensor: real valued magnitude squared

    &#34;&#34;&#34;
    return math.imag(complex_values) ** 2 + math.real(complex_values) ** 2</code></pre>
</details>
</dd>
<dt id="phi.math.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.all(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.all_available"><code class="name flex">
<span>def <span class="ident">all_available</span></span>(<span>*values: phi.math._tensors.Tensor) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Tests if the values of all given tensors are known and can be read at this point.</p>
<p>Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to check.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>True</code> if no value is a placeholder or being traced, <code>False</code> otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_available(*values: Tensor) -&gt; bool:
    &#34;&#34;&#34;
    Tests if the values of all given tensors are known and can be read at this point.
    
    Tensors are typically available when the backend operates in eager mode and is not currently tracing a function.

    Args:
      values: Tensors to check.

    Returns:
        `True` if no value is a placeholder or being traced, `False` otherwise.
    &#34;&#34;&#34;
    from phi.math._functional import is_tracer
    for value in values:
        if is_tracer(value):
            return False
        natives = value._natives()
        natives_available = [choose_backend(native).is_available(native) for native in natives]
        if not all(natives_available):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>boolean_tensor: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any_(boolean_tensor: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(boolean_tensor, dim,
                   native_function=lambda backend, native, dim: backend.any(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.assert_close"><code class="name flex">
<span>def <span class="ident">assert_close</span></span>(<span>*values, rel_tolerance: float = 1e-05, abs_tolerance: float = 0, msg: str = '', verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks that all given tensors have equal values within the specified tolerance.
Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.</p>
<p>Does not check that the shapes match as long as they can be broadcast to a common shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors or native tensors or numbers or sequences of numbers.</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>Relative tolerance.</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>Absolute tolerance.</dd>
<dt><strong><code>msg</code></strong></dt>
<dd>Optional error message.</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Whether to print conflicting values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assert_close(*values,
                 rel_tolerance: float = 1e-5,
                 abs_tolerance: float = 0,
                 msg: str = &#34;&#34;,
                 verbose: bool = True):
    &#34;&#34;&#34;
    Checks that all given tensors have equal values within the specified tolerance.
    Raises an AssertionError if the values of this tensor are not within tolerance of any of the other tensors.
    
    Does not check that the shapes match as long as they can be broadcast to a common shape.

    Args:
      values: Tensors or native tensors or numbers or sequences of numbers.
      rel_tolerance: Relative tolerance.
      abs_tolerance: Absolute tolerance.
      msg: Optional error message.
      verbose: Whether to print conflicting values.
    &#34;&#34;&#34;
    any_tensor = next(filter(lambda t: isinstance(t, Tensor), values))
    if any_tensor is None:
        values = [wrap(t) for t in values]
    else:  # use Tensor to infer dimensions
        values = [compatible_tensor(t, any_tensor.shape)._simplify() for t in values]
    for other in values[1:]:
        _assert_close(values[0], other, rel_tolerance, abs_tolerance, msg, verbose)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_shape"><code class="name flex">
<span>def <span class="ident">batch_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'batch'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None):
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;batch&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.batch_stack"><code class="name flex">
<span>def <span class="ident">batch_stack</span></span>(<span>values, dim: str = 'batch')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_stack(values, dim: str = &#39;batch&#39;):
    return _stack(values, dim, BATCH_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.boolean_mask"><code class="name flex">
<span>def <span class="ident">boolean_mask</span></span>(<span>x: phi.math._tensors.Tensor, dim: str, mask: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Discards values <code>x.dim[i]</code> where <code>mask.dim[i]=False</code>.</p>
<p>All dimensions of <code>mask</code> that are not <code>dim</code> are treated as batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of values.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Dimension of <code>x</code> to along which to discard slices.</dd>
<dt><strong><code>mask</code></strong></dt>
<dd>Boolean <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> marking which values to keep. Must have the dimension <code>dim</code> matching `x´.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Selected values of <code>x</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with dimensions from <code>x</code> and <code>mask</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_mask(x: Tensor, dim: str, mask: Tensor):
    &#34;&#34;&#34;
    Discards values `x.dim[i]` where `mask.dim[i]=False`.

    All dimensions of `mask` that are not `dim` are treated as batch dimensions.

    Args:
        x: `Tensor` of values.
        dim: Dimension of `x` to along which to discard slices.
        mask: Boolean `Tensor` marking which values to keep. Must have the dimension `dim` matching `x´.

    Returns:
        Selected values of `x` as `Tensor` with dimensions from `x` and `mask`.
    &#34;&#34;&#34;
    def uniform_boolean_mask(x: Tensor, mask_1d: Tensor):
        if dim in x.shape:
            x_native = x.native()
            mask_native = mask_1d.native()
            backend = choose_backend(x_native, mask_native)
            result_native = backend.boolean_mask(x_native, mask_native, axis=x.shape.index(dim))
            new_shape = x.shape.with_sizes(backend.staticshape(result_native))
            return NativeTensor(result_native, new_shape)
        else:
            total = int(sum_(to_int64(mask_1d)))
            new_shape = mask_1d.shape.with_sizes([total])
            return _expand_dims(x, new_shape)

    return broadcast_op(uniform_boolean_mask, [x, mask], iter_dims=mask.shape.without(dim))</code></pre>
</details>
</dd>
<dt id="phi.math.cast"><code class="name flex">
<span>def <span class="ident">cast</span></span>(<span>x: phi.math._tensors.Tensor, dtype: phi.math.backend._dtype.DType) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cast(x: Tensor, dtype: DType) -&gt; Tensor:
    return x._op1(lambda native: choose_backend(native).cast(native, dtype=dtype))</code></pre>
</details>
</dd>
<dt id="phi.math.ceil"><code class="name flex">
<span>def <span class="ident">ceil</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ceil(x) -&gt; Tensor:
    return _backend_op1(x, Backend.ceil)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_shape"><code class="name flex">
<span>def <span class="ident">channel_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'channel'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or list or tuple: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_shape(sizes: Shape or dict or list or tuple, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;channel&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or list or tuple: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.channel_stack"><code class="name flex">
<span>def <span class="ident">channel_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def channel_stack(values, dim: str):
    return _stack(values, dim, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.choose_backend"><code class="name flex">
<span>def <span class="ident">choose_backend</span></span>(<span>*values, prefer_default=False) ‑> phi.math.backend._backend.Backend</span>
</code></dt>
<dd>
<div class="desc"><p>Choose backend for given <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def choose_backend_t(*values, prefer_default=False) -&gt; Backend:
    &#34;&#34;&#34; Choose backend for given `Tensor` or native tensor values. &#34;&#34;&#34;
    natives = sum([v._natives() if isinstance(v, Tensor) else (v,) for v in values], ())
    return choose_backend(*natives, prefer_default=prefer_default)</code></pre>
</details>
</dd>
<dt id="phi.math.clip"><code class="name flex">
<span>def <span class="ident">clip</span></span>(<span>x: phi.math._tensors.Tensor, lower_limit: float, upper_limit: float)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clip(x: Tensor, lower_limit: float or Tensor, upper_limit: float or Tensor):
    if isinstance(lower_limit, Number) and isinstance(upper_limit, Number):

        def clip_(x):
            return x._op1(lambda native: choose_backend(native).clip(native, lower_limit, upper_limit))

        return broadcast_op(clip_, [x])
    else:
        return maximum(lower_limit, minimum(x, upper_limit))</code></pre>
</details>
</dd>
<dt id="phi.math.close"><code class="name flex">
<span>def <span class="ident">close</span></span>(<span>*tensors, rel_tolerance=1e-05, abs_tolerance=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether all tensors have equal values within the specified tolerance.</p>
<p>Does not check that the shapes exactly match.
Tensors with different shapes are reshaped before comparing.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tensors</code></strong></dt>
<dd>tensor or tensor-like (constant) each</dd>
<dt><strong><code>rel_tolerance</code></strong></dt>
<dd>relative tolerance (Default value = 1e-5)</dd>
<dt><strong><code>abs_tolerance</code></strong></dt>
<dd>absolute tolerance (Default value = 0)</dd>
<dt><strong><code>*tensors</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close(*tensors, rel_tolerance=1e-5, abs_tolerance=0):
    &#34;&#34;&#34;
    Checks whether all tensors have equal values within the specified tolerance.
    
    Does not check that the shapes exactly match.
    Tensors with different shapes are reshaped before comparing.

    Args:
      tensors: tensor or tensor-like (constant) each
      rel_tolerance: relative tolerance (Default value = 1e-5)
      abs_tolerance: absolute tolerance (Default value = 0)
      *tensors: 

    Returns:

    &#34;&#34;&#34;
    tensors = [wrap(t) for t in tensors]
    for other in tensors[1:]:
        if not _close(tensors[0], other, rel_tolerance=rel_tolerance, abs_tolerance=abs_tolerance):
            return False
    return True</code></pre>
</details>
</dd>
<dt id="phi.math.closest_grid_values"><code class="name flex">
<span>def <span class="ident">closest_grid_values</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: extrapolation_.Extrapolation, stack_dim_prefix='closest_')</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the neighboring grid points in all spatial directions and returns their values.
The result will have 2^d values for each vector in coordiantes in d dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid data. The grid is spanned by the spatial dimensions of the tensor</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>tensor with 1 channel dimension holding vectors pointing to locations in grid index space</dd>
<dt><strong><code>extrap</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>stack_dim_prefix</code></strong></dt>
<dd>For each spatial dimension <code>dim</code>, stacks lower and upper closest values along dimension <code>stack_dim_prefix+dim</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,&hellip;), grid_channel)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def closest_grid_values(grid: Tensor,
                        coordinates: Tensor,
                        extrap: &#39;extrapolation_.Extrapolation&#39;,
                        stack_dim_prefix=&#39;closest_&#39;):
    &#34;&#34;&#34;
    Finds the neighboring grid points in all spatial directions and returns their values.
    The result will have 2^d values for each vector in coordiantes in d dimensions.

    Args:
      grid: grid data. The grid is spanned by the spatial dimensions of the tensor
      coordinates: tensor with 1 channel dimension holding vectors pointing to locations in grid index space
      extrap: grid extrapolation
      stack_dim_prefix: For each spatial dimension `dim`, stacks lower and upper closest values along dimension `stack_dim_prefix+dim`.

    Returns:
      Tensor of shape (batch, coord_spatial, grid_spatial=(2, 2,...), grid_channel)

    &#34;&#34;&#34;
    return broadcast_op(functools.partial(_closest_grid_values, extrap=extrap, stack_dim_prefix=stack_dim_prefix), [grid, coordinates])</code></pre>
</details>
</dd>
<dt id="phi.math.concat"><code class="name flex">
<span>def <span class="ident">concat</span></span>(<span>values: tuple, dim: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenates a sequence of tensors along one dimension.
The shapes of all values must be equal, except for the size of the concat dimension.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to concatenate</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>concat dimension, must be present in all values</dd>
<dt><strong><code>values</code></strong></dt>
<dd>tuple or list: </dd>
<dt><strong><code>dim</code></strong></dt>
<dd>str: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>concatenated tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concat(values: tuple or list, dim: str) -&gt; Tensor:
    &#34;&#34;&#34;
    Concatenates a sequence of tensors along one dimension.
    The shapes of all values must be equal, except for the size of the concat dimension.

    Args:
      values: Tensors to concatenate
      dim: concat dimension, must be present in all values
      values: tuple or list: 
      dim: str: 

    Returns:
      concatenated tensor

    &#34;&#34;&#34;
    assert len(values) &gt; 0, &#34;concat() got empty sequence&#34;
    broadcast_shape = values[0].shape
    natives = [v.native(order=broadcast_shape.names) for v in values]
    backend = choose_backend(*natives)
    concatenated = backend.concat(natives, broadcast_shape.index(dim))
    return NativeTensor(concatenated, broadcast_shape.with_sizes(backend.staticshape(concatenated)))</code></pre>
</details>
</dd>
<dt id="phi.math.convert"><code class="name flex">
<span>def <span class="ident">convert</span></span>(<span>value: phi.math._tensors.Tensor, backend: phi.math.backend._backend.Backend = None, use_dlpack=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the native representation of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to the native format of <code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code>.</p>
<p><em>Warning</em>: This operation breaks the automatic differentiation chain.</p>
<p>See Also:
<code><a title="phi.math.backend.convert" href="backend/index.html#phi.math.backend.convert">convert()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to convert.</dd>
<dt><strong><code>backend</code></strong></dt>
<dd>Target backend. If <code>None</code>, uses the current default backend, see <code><a title="phi.math.backend.default_backend" href="backend/index.html#phi.math.backend.default_backend">default_backend()</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with native representation belonging to <code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert(value: Tensor, backend: Backend = None, use_dlpack=True):
    &#34;&#34;&#34;
    Convert the native representation of a `Tensor` to the native format of `backend`.

    *Warning*: This operation breaks the automatic differentiation chain.

    See Also:
        `phi.math.backend.convert()`.

    Args:
        value: `Tensor` to convert.
        backend: Target backend. If `None`, uses the current default backend, see `phi.math.backend.default_backend()`.

    Returns:
        `Tensor` with native representation belonging to `backend`.
    &#34;&#34;&#34;
    return value._op1(lambda native: b_convert(native, backend, use_dlpack=use_dlpack))</code></pre>
</details>
</dd>
<dt id="phi.math.convolve"><code class="name flex">
<span>def <span class="ident">convolve</span></span>(<span>value: phi.math._tensors.Tensor, kernel: phi.math._tensors.Tensor, extrapolation: extrapolation_.Extrapolation = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the convolution of <code>value</code> and <code>kernel</code> along the spatial axes of <code>kernel</code>.</p>
<p>The channel dimensions of <code>value</code> are reduced against the equally named dimensions of <code>kernel</code>.
The result will have the non-reduced channel dimensions of <code>kernel</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> whose shape includes all spatial dimensions of <code>kernel</code>.</dd>
<dt><strong><code>kernel</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> used as convolutional filter.</dd>
<dt><strong><code>extrapolation</code></strong></dt>
<dd>If not None, pads <code>value</code> so that the result has the same shape as <code>value</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolve(value: Tensor,
             kernel: Tensor,
             extrapolation: &#39;extrapolation_.Extrapolation&#39; = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the convolution of `value` and `kernel` along the spatial axes of `kernel`.

    The channel dimensions of `value` are reduced against the equally named dimensions of `kernel`.
    The result will have the non-reduced channel dimensions of `kernel`.

    Args:
        value: `Tensor` whose shape includes all spatial dimensions of `kernel`.
        kernel: `Tensor` used as convolutional filter.
        extrapolation: If not None, pads `value` so that the result has the same shape as `value`.

    Returns:
        `Tensor`
    &#34;&#34;&#34;
    assert all(dim in value.shape for dim in kernel.shape.spatial.names), f&#34;Value must have all spatial dimensions of kernel but got value {value} kernel {kernel}&#34;
    conv_shape = kernel.shape.spatial
    in_channels = value.shape.channel
    out_channels = kernel.shape.channel.without(in_channels)
    batch = value.shape.batch &amp; kernel.shape.batch
    if extrapolation is not None and extrapolation != e_.ZERO:
        value = pad(value, {dim: (kernel.shape.get_size(dim) // 2, (kernel.shape.get_size(dim) - 1) // 2)
                            for dim in conv_shape.name}, extrapolation)
    native_kernel = reshaped_native(kernel, (batch, out_channels, in_channels, *conv_shape.names), force_expand=in_channels)
    native_value = reshaped_native(value, (batch, in_channels, *conv_shape.names), force_expand=batch)
    backend = choose_backend(native_value, native_kernel)
    native_result = backend.conv(native_value, native_kernel, zero_padding=extrapolation == e_.ZERO)
    result = reshaped_tensor(native_result, (batch, out_channels, *conv_shape.names))
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Copies the data buffer and encapsulating <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to be copied.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(value: Tensor):
    &#34;&#34;&#34;
    Copies the data buffer and encapsulating `Tensor` object.

    Args:
        value: `Tensor` to be copied.

    Returns:
        Copy of `value`.
    &#34;&#34;&#34;
    if value._is_special:
        warnings.warn(&#34;Tracing tensors cannot be copied.&#34;)
        return value
    return value._op1(lambda native: choose_backend(native).copy(native))</code></pre>
</details>
</dd>
<dt id="phi.math.cos"><code class="name flex">
<span>def <span class="ident">cos</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cos(x) -&gt; Tensor:
    return _backend_op1(x, Backend.cos)</code></pre>
</details>
</dd>
<dt id="phi.math.cross_product"><code class="name flex">
<span>def <span class="ident">cross_product</span></span>(<span>vec1: phi.math._tensors.Tensor, vec2: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_product(vec1: Tensor, vec2: Tensor):
    vec1, vec2 = math.tensors(vec1, vec2)
    spatial_rank = vec1.vector.size if &#39;vector&#39; in vec1.shape else vec2.vector.size
    if spatial_rank == 2:  # Curl in 2D
        assert vec2.vector.exists
        if vec1.vector.exists:
            v1_x, v1_y = vec1.vector.unstack()
            v2_x, v2_y = vec2.vector.unstack()
            if GLOBAL_AXIS_ORDER.is_x_first:
                return v1_x * v2_y - v1_y * v2_x
            else:
                return - v1_x * v2_y + v1_y * v2_x
        else:
            v2_x, v2_y = vec2.vector.unstack()
            if GLOBAL_AXIS_ORDER.is_x_first:
                return vec1 * math.channel_stack([-v2_y, v2_x], &#39;vector&#39;)
            else:
                return vec1 * math.channel_stack([v2_y, -v2_x], &#39;vector&#39;)
    elif spatial_rank == 3:  # Curl in 3D
        raise NotImplementedError(f&#39;spatial_rank={spatial_rank} not yet implemented&#39;)
    else:
        raise AssertionError(f&#39;dims = {spatial_rank}. Vector product not available in &gt; 3 dimensions&#39;)</code></pre>
</details>
</dd>
<dt id="phi.math.custom_gradient"><code class="name flex">
<span>def <span class="ident">custom_gradient</span></span>(<span>f: Callable, gradient: Callable)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function based on <code>f</code> that uses a custom gradient for the backpropagation pass.</p>
<p><em>Warning</em> This method can lead to memory leaks if the gradient funcion is not called.
Make sure to pass tensors without gradients if the gradient is not required, see <code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Forward function mapping <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments <code>x</code> to a single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> output or sequence of tensors <code>y</code>.</dd>
<dt><strong><code>gradient</code></strong></dt>
<dd>Function to compute the vector-Jacobian product for backpropropagation. Will be called as <code>gradient(*x, *y, *dy) -&gt; *dx</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>. However, the returned function does not support keyword arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def custom_gradient(f: Callable, gradient: Callable):
    &#34;&#34;&#34;
    Creates a function based on `f` that uses a custom gradient for the backpropagation pass.

    *Warning* This method can lead to memory leaks if the gradient funcion is not called.
    Make sure to pass tensors without gradients if the gradient is not required, see `stop_gradient()`.

    Args:
        f: Forward function mapping `Tensor` arguments `x` to a single `Tensor` output or sequence of tensors `y`.
        gradient: Function to compute the vector-Jacobian product for backpropropagation. Will be called as `gradient(*x, *y, *dy) -&gt; *dx`.

    Returns:
        Function with similar signature and return values as `f`. However, the returned function does not support keyword arguments.
    &#34;&#34;&#34;
    return CustomGradientFunction(f, gradient)</code></pre>
</details>
</dd>
<dt id="phi.math.divide_no_nan"><code class="name flex">
<span>def <span class="ident">divide_no_nan</span></span>(<span>x, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def divide_no_nan(x, y):
    return custom_op2(x, y, divide_no_nan, lambda x_, y_: choose_backend(x_, y_).divide_no_nan(x_, y_), lambda y_, x_: divide_no_nan(x_, y_), lambda y_, x_: choose_backend(x_, y_).divide_no_nan(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.dot"><code class="name flex">
<span>def <span class="ident">dot</span></span>(<span>x: phi.math._tensors.Tensor, x_dims: str, y: phi.math._tensors.Tensor, y_dims: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the dot product along the specified dimensions.
Contracts <code>x_dims</code> with <code>y_dims</code> by first multiplying the elements and then summing them up.</p>
<p>For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>First <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>x_dims</code></strong></dt>
<dd>Dimensions of <code>x</code> to reduce against <code>y</code></dd>
<dt><strong><code>y</code></strong></dt>
<dd>Second <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>y_dims</code></strong></dt>
<dd>Dimensions of <code>y</code> to reduce against <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dot product as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dot(x: Tensor,
        x_dims: str or tuple or list or Shape,
        y: Tensor,
        y_dims: str or tuple or list or Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes the dot product along the specified dimensions.
    Contracts `x_dims` with `y_dims` by first multiplying the elements and then summing them up.

    For one dimension, this is equal to matrix-matrix or matrix-vector multiplication.

    Args:
        x: First `Tensor`
        x_dims: Dimensions of `x` to reduce against `y`
        y: Second `Tensor`
        y_dims: Dimensions of `y` to reduce against `x`.

    Returns:
        Dot product as `Tensor`.
    &#34;&#34;&#34;
    x_dims = _resolve_dims(x_dims, x.shape)
    y_dims = _resolve_dims(y_dims, y.shape)
    x_native = x.native()
    y_native = y.native()
    backend = choose_backend(x_native, y_native)
    remaining_shape_x = x.shape.without(x_dims)
    remaining_shape_y = y.shape.without(y_dims)
    if remaining_shape_y.only(remaining_shape_x).is_empty:  # no shared batch dimensions -&gt; tensordot
        result_native = backend.tensordot(x_native, x.shape.index(x_dims), y_native, y.shape.index(y_dims))
    else:  # shared batch dimensions -&gt; einsum
        REDUCE_LETTERS = list(&#39;ijklmn&#39;)
        KEEP_LETTERS = list(&#39;abcdefgh&#39;)
        x_letters = [(REDUCE_LETTERS if dim in x_dims else KEEP_LETTERS).pop(0) for dim in x.shape.names]
        x_letter_map = {dim: letter for dim, letter in zip(x.shape.names, x_letters)}
        REDUCE_LETTERS = list(&#39;ijklmn&#39;)
        y_letters = []
        for dim in y.shape.names:
            if dim in y_dims:
                y_letters.append(REDUCE_LETTERS.pop(0))
            else:
                if dim in x_letter_map:
                    y_letters.append(x_letter_map[dim])
                else:
                    y_letters.append(KEEP_LETTERS.pop(0))
        keep_letters = list(&#39;abcdefgh&#39;)[:-len(KEEP_LETTERS)]
        subscripts = f&#39;{&#34;&#34;.join(x_letters)},{&#34;&#34;.join(y_letters)}-&gt;{&#34;&#34;.join(keep_letters)}&#39;
        result_native = backend.einsum(subscripts, x_native, y_native)
    result_shape = combine_safe(x.shape.without(x_dims), y.shape.without(y_dims))  # don&#39;t check group match
    return NativeTensor(result_native, result_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.downsample2x"><code class="name flex">
<span>def <span class="ident">downsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to half the number of spatial sample points per dimension.
The grid values at the new points are determined via mean (linear interpolation).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>full size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation. Used to insert an additional value for odd spatial dims</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which down-sampling is applied. If None, down-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>half-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def downsample2x(grid: Tensor,
                 padding: Extrapolation = extrapolation.BOUNDARY,
                 dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to half the number of spatial sample points per dimension.
    The grid values at the new points are determined via mean (linear interpolation).

    Args:
      grid: full size grid
      padding: grid extrapolation. Used to insert an additional value for odd spatial dims
      dims: dims along which down-sampling is applied. If None, down-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      half-size grid

    &#34;&#34;&#34;
    dims = grid.shape.spatial.only(dims).names
    odd_dimensions = [dim for dim in dims if grid.shape.get_size(dim) % 2 != 0]
    grid = math.pad(grid, {dim: (0, 1) for dim in odd_dimensions}, padding)
    for dim in dims:
        grid = (grid[{dim: slice(1, None, 2)}] + grid[{dim: slice(0, None, 2)}]) / 2
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.dtype"><code class="name flex">
<span>def <span class="ident">dtype</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dtype(x):
    if isinstance(x, Tensor):
        return x.dtype
    else:
        return choose_backend(x).dtype(x)</code></pre>
</details>
</dd>
<dt id="phi.math.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(x) -&gt; Tensor:
    return _backend_op1(x, Backend.exp)</code></pre>
</details>
</dd>
<dt id="phi.math.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>value: phi.math._tensors.Tensor, add_shape: phi.math._shape.Shape = (), **dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(value: Tensor, add_shape: Shape = EMPTY_SHAPE, **dims):
    return _expand_dims(value, add_shape &amp; shape_(**dims))</code></pre>
</details>
</dd>
<dt id="phi.math.expand_batch"><code class="name flex">
<span>def <span class="ident">expand_batch</span></span>(<span>value: phi.math._tensors.Tensor, **dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_batch(value: Tensor, **dims):
    return _expand_dims(value, batch_shape(dims))</code></pre>
</details>
</dd>
<dt id="phi.math.expand_channel"><code class="name flex">
<span>def <span class="ident">expand_channel</span></span>(<span>value: phi.math._tensors.Tensor, **dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_channel(value: Tensor, **dims):
    return _expand_dims(value, channel_shape(dims))</code></pre>
</details>
</dd>
<dt id="phi.math.expand_spatial"><code class="name flex">
<span>def <span class="ident">expand_spatial</span></span>(<span>value: phi.math._tensors.Tensor, **dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_spatial(value: Tensor, **dims):
    return _expand_dims(value, spatial_shape(dims))</code></pre>
</details>
</dd>
<dt id="phi.math.extrapolate_valid_values"><code class="name flex">
<span>def <span class="ident">extrapolate_valid_values</span></span>(<span>values: phi.math._tensors.Tensor, valid: phi.math._tensors.Tensor, distance_cells: int = 1) ‑> Tuple[phi.math._tensors.Tensor, phi.math._tensors.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolates the values of <code>values</code> which are marked by the nonzero values of <code>valid</code> for <code>distance_cells</code> steps in all spatial directions.
Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.</p>
<p>Examples (1-step extrapolation), x marks the values for extrapolation:
200
000
111
004
00x
044
102
000
144
010 + 0x0 =&gt; 111
000 + 000 =&gt; 234
004 + 00x =&gt; 234
040
000
111
200
x00
220
200
x00
234</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Tensor which holds the values for extrapolation</dd>
<dt><strong><code>valid</code></strong></dt>
<dd>Tensor with same size as <code>x</code> marking the values for extrapolation with nonzero values</dd>
<dt><strong><code>distance_cells</code></strong></dt>
<dd>Number of extrapolation steps</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>values</code></dt>
<dd>Extrapolation result</dd>
<dt><code>valid</code></dt>
<dd>mask marking all valid values after extrapolation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extrapolate_valid_values(values: Tensor, valid: Tensor, distance_cells: int = 1) -&gt; Tuple[Tensor, Tensor]:
    &#34;&#34;&#34;
    Extrapolates the values of `values` which are marked by the nonzero values of `valid` for `distance_cells` steps in all spatial directions.
    Overlapping extrapolated values get averaged. Extrapolation also includes diagonals.

    Examples (1-step extrapolation), x marks the values for extrapolation:
        200   000    111        004   00x    044        102   000    144
        010 + 0x0 =&gt; 111        000 + 000 =&gt; 234        004 + 00x =&gt; 234
        040   000    111        200   x00    220        200   x00    234

    Args:
        values: Tensor which holds the values for extrapolation
        valid: Tensor with same size as `x` marking the values for extrapolation with nonzero values
        distance_cells: Number of extrapolation steps

    Returns:
        values: Extrapolation result
        valid: mask marking all valid values after extrapolation
    &#34;&#34;&#34;

    def binarize(x):
        return math.divide_no_nan(x, x)

    distance_cells = min(distance_cells, max(values.shape))
    for _ in range(distance_cells):
        valid = binarize(valid)
        valid_values = valid * values
        overlap = valid
        for dim in values.shape.spatial.names:
            values_l, values_r = shift(valid_values, (-1, 1), dims=dim, padding=extrapolation.ZERO)
            valid_values = math.sum_(values_l + values_r + valid_values, dim=&#39;shift&#39;)
            mask_l, mask_r = shift(overlap, (-1, 1), dims=dim, padding=extrapolation.ZERO)
            overlap = math.sum_(mask_l + mask_r + overlap, dim=&#39;shift&#39;)
        extp = math.divide_no_nan(valid_values, overlap)  # take mean where extrapolated values overlap
        values = math.where(valid, values, math.where(binarize(overlap), extp, values))
        valid = overlap
    return values, binarize(valid)</code></pre>
</details>
</dd>
<dt id="phi.math.fft"><code class="name flex">
<span>def <span class="ident">fft</span></span>(<span>x: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a fast Fourier transform (FFT) on all spatial dimensions of x.</p>
<p>The inverse operation is :func:<code><a title="phi.math.ifft" href="#phi.math.ifft">ifft()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>tensor of type float or complex</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>FFT(x) of type complex</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fft(x: Tensor):
    &#34;&#34;&#34;
    Performs a fast Fourier transform (FFT) on all spatial dimensions of x.
    
    The inverse operation is :func:`ifft`.

    Args:
      x: tensor of type float or complex
      x: Tensor: 

    Returns:
      FFT(x) of type complex

    &#34;&#34;&#34;
    native, assemble = _invertible_standard_form(x)
    result = choose_backend(native).fft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.fftfreq"><code class="name flex">
<span>def <span class="ident">fftfreq</span></span>(<span>resolution: phi.math._shape.Shape, dx: phi.math._tensors.Tensor = 1, dtype: phi.math.backend._dtype.DType = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the discrete Fourier transform sample frequencies.
These are the frequencies corresponding to the components of the result of <code>math.fft</code> on a tensor of shape <code>resolution</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>resolution</code></strong></dt>
<dd>Grid resolution measured in cells</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Distance between sampling points in real space.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Data type of the returned tensor (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> holding the frequencies of the corresponding values computed by math.fft</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fftfreq(resolution: Shape, dx: Tensor or float = 1, dtype: DType = None):
    &#34;&#34;&#34;
    Returns the discrete Fourier transform sample frequencies.
    These are the frequencies corresponding to the components of the result of `math.fft` on a tensor of shape `resolution`.

    Args:
        resolution: Grid resolution measured in cells
        dx: Distance between sampling points in real space.
        dtype: Data type of the returned tensor (Default value = None)

    Returns:
        `Tensor` holding the frequencies of the corresponding values computed by math.fft
    &#34;&#34;&#34;
    resolution = spatial_shape(resolution)
    k = meshgrid(**{dim: np.fft.fftfreq(int(n)) for dim, n in resolution.named_sizes})
    k /= dx
    return to_float(k) if dtype is None else cast(k, dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.flatten"><code class="name flex">
<span>def <span class="ident">flatten</span></span>(<span>value: phi.math._tensors.Tensor, flat_dim: str = 'flat')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatten(value: Tensor, flat_dim: str = &#39;flat&#39;):
    return join_dimensions(value, value.shape, flat_dim)</code></pre>
</details>
</dd>
<dt id="phi.math.floor"><code class="name flex">
<span>def <span class="ident">floor</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def floor(x) -&gt; Tensor:
    return _backend_op1(x, Backend.floor)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_laplace"><code class="name flex">
<span>def <span class="ident">fourier_laplace</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies the spatial laplace operator to the given tensor with periodic boundary conditions.</p>
<p><em>Note:</em> The results of <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code> and <code><a title="phi.math.laplace" href="#phi.math.laplace">laplace()</a></code> are close but not identical.</p>
<p>This implementation computes the laplace operator in Fourier space.
The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>tensor, assumed to have periodic boundary conditions</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>distance between grid points, tensor-like, scalar or vector</dd>
<dt><strong><code>times</code></strong></dt>
<dd>number of times the laplace operator is applied. The computational cost is independent of this parameter.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape as <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_laplace(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Applies the spatial laplace operator to the given tensor with periodic boundary conditions.
    
    *Note:* The results of `fourier_laplace` and `laplace` are close but not identical.
    
    This implementation computes the laplace operator in Fourier space.
    The result for periodic fields is exact, i.e. no numerical instabilities can occur, even for higher-order derivatives.

    Args:
      grid: tensor, assumed to have periodic boundary conditions
      dx: distance between grid points, tensor-like, scalar or vector
      times: number of times the laplace operator is applied. The computational cost is independent of this parameter.
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:
      tensor of same shape as `tensor`

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi) ** 2 * k_squared
    result = math.real(math.ifft(frequencies * fft_laplace ** times))
    return math.cast(result / wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.fourier_poisson"><code class="name flex">
<span>def <span class="ident">fourier_poisson</span></span>(<span>grid: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor, times: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Inverse operation to <code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or Shape or float or list or tuple: </dd>
<dt><strong><code>times</code></strong></dt>
<dd>int:
(Default value = 1)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fourier_poisson(grid: Tensor,
                    dx: Tensor or Shape or float or list or tuple,
                    times: int = 1):
    &#34;&#34;&#34;
    Inverse operation to `fourier_laplace`.

    Args:
      grid: Tensor: 
      dx: Tensor or Shape or float or list or tuple: 
      times: int:  (Default value = 1)

    Returns:

    &#34;&#34;&#34;
    frequencies = math.fft(math.to_complex(grid))
    k_squared = math.sum_(math.fftfreq(grid.shape) ** 2, &#39;vector&#39;)
    fft_laplace = -(2 * np.pi) ** 2 * k_squared
    # fft_laplace.tensor[(0,) * math.ndims(k_squared)] = math.inf  # assume NumPy array to edit
    result = math.real(math.ifft(math.divide_no_nan(frequencies, math.to_complex(fft_laplace ** times))))
    return math.cast(result * wrap(dx) ** 2, grid.dtype)</code></pre>
</details>
</dd>
<dt id="phi.math.frequency_loss"><code class="name flex">
<span>def <span class="ident">frequency_loss</span></span>(<span>x, n=2, frequency_falloff: float = 100, threshold=1e-05, batch_norm: bool = True, ignore_mean=False) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Penalizes the squared <code>values</code> in frequency (Fourier) space.
Lower frequencies are weighted more strongly then higher frequencies, depending on <code>frequency_falloff</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>values</code></strong></dt>
<dd>Values to penalize, typically <code>actual - target</code></dd>
<dt><strong><code>n</code></strong></dt>
<dd>Loss type, see <code>l_n_loss()</code>.</dd>
<dt><strong><code>frequency_falloff</code></strong></dt>
<dd>Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
<em>Note</em>: The total loss is not normalized. Varying the value will result in losses of different magnitudes.</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>Frequency amplitudes below this value are ignored.
Setting this to zero may cause infinities or NaN values during backpropagation.</dd>
<dt><strong><code>batch_norm</code></strong></dt>
<dd>Either <code>bool</code> specifying whether to divide by the product of all batch sizes or specific dimensions.</dd>
<dt><strong><code>ignore_mean</code></strong></dt>
<dd>If <code>True</code>, does not penalize the mean value (frequency=0 component).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Scalar loss value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def frequency_loss(x,
                   n=2,
                   frequency_falloff: float = 100,
                   threshold=1e-5,
                   batch_norm: bool or str or tuple or list or Shape = True,
                   ignore_mean=False) -&gt; Tensor:
    &#34;&#34;&#34;
    Penalizes the squared `values` in frequency (Fourier) space.
    Lower frequencies are weighted more strongly then higher frequencies, depending on `frequency_falloff`.

    Args:
        values: Values to penalize, typically `actual - target`
        n: Loss type, see `l_n_loss()`.
        frequency_falloff: Large values put more emphasis on lower frequencies, 1.0 weights all frequencies equally.
            *Note*: The total loss is not normalized. Varying the value will result in losses of different magnitudes.
        threshold: Frequency amplitudes below this value are ignored.
            Setting this to zero may cause infinities or NaN values during backpropagation.
        batch_norm: Either `bool` specifying whether to divide by the product of all batch sizes or specific dimensions.
        ignore_mean: If `True`, does not penalize the mean value (frequency=0 component).

    Returns:
      Scalar loss value
    &#34;&#34;&#34;
    if ignore_mean:
        values -= math.mean(values, values.shape.non_batch)
    k_squared = vec_squared(math.fftfreq(values.shape.spatial))
    weights = math.exp(-0.5 * k_squared * frequency_falloff ** 2)
    diff_fft = abs_square(math.fft(values) * weights)
    diff_fft = math.sqrt(math.maximum(diff_fft, threshold))
    return l_n_loss(diff_fft, n=n, batch_norm=batch_norm)</code></pre>
</details>
</dd>
<dt id="phi.math.functional_gradient"><code class="name flex">
<span>def <span class="ident">functional_gradient</span></span>(<span>f: Callable, wrt: tuple = (0,), get_output=True) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a function which computes the spatial_gradient of <code>f</code>.</p>
<p>Example:</p>
<pre><code class="language-python">def loss_function(x, y):
    prediction = f(x)
    loss = math.l2_loss(prediction - y)
    return loss, prediction

dx, = functional_gradient(loss_function)(x, y)

loss, prediction, dx, dy = functional_gradient(loss_function, wrt=(0, 1),
                                             get_output=True)(x, y)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be differentiated.
<code>f</code> must return a floating point <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with rank zero.
It can return additional tensors which are treated as auxiliary data and will be returned by the spatial_gradient function if <code>return_values=True</code>.
All arguments for which the spatial_gradient is computed must be of dtype float or complex.</dd>
<dt><strong><code>get_output</code></strong></dt>
<dd>Whether the spatial_gradient function should also return the return values of <code>f</code>.</dd>
<dt><strong><code>wrt</code></strong></dt>
<dd>Arguments of <code>f</code> with respect to which the spatial_gradient should be computed.
Example: <code>wrt_indices=[0]</code> computes the spatial_gradient with respect to the first argument of <code>f</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with the same arguments as <code>f</code> that returns the value of <code>f</code>, auxiliary data and spatial_gradient of <code>f</code> if <code>get_output=True</code>, else just the spatial_gradient of <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def functional_gradient(f: Callable, wrt: tuple or list = (0,), get_output=True) -&gt; Callable:
    &#34;&#34;&#34;
    Creates a function which computes the spatial_gradient of `f`.

    Example:

    ```python
    def loss_function(x, y):
        prediction = f(x)
        loss = math.l2_loss(prediction - y)
        return loss, prediction

    dx, = functional_gradient(loss_function)(x, y)

    loss, prediction, dx, dy = functional_gradient(loss_function, wrt=(0, 1),
                                                 get_output=True)(x, y)
    ```

    Args:
        f: Function to be differentiated.
            `f` must return a floating point `Tensor` with rank zero.
            It can return additional tensors which are treated as auxiliary data and will be returned by the spatial_gradient function if `return_values=True`.
            All arguments for which the spatial_gradient is computed must be of dtype float or complex.
        get_output: Whether the spatial_gradient function should also return the return values of `f`.
        wrt: Arguments of `f` with respect to which the spatial_gradient should be computed.
            Example: `wrt_indices=[0]` computes the spatial_gradient with respect to the first argument of `f`.

    Returns:
        Function with the same arguments as `f` that returns the value of `f`, auxiliary data and spatial_gradient of `f` if `get_output=True`, else just the spatial_gradient of `f`.
    &#34;&#34;&#34;
    return GradientFunction(f, wrt, get_output)</code></pre>
</details>
</dd>
<dt id="phi.math.gather"><code class="name flex">
<span>def <span class="ident">gather</span></span>(<span>values: phi.math._tensors.Tensor, indices: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gather(values: Tensor, indices: Tensor):
    b_values = join_dimensions(values, values.shape.batch, &#39;batch&#39;)
    b_values = join_dimensions(b_values, b_values.shape.channel, &#39;channel&#39;, pos=-1)
    b_indices = _expand_dims(indices, values.shape.batch)
    b_indices = join_dimensions(b_indices, values.shape.batch, &#39;batch&#39;)
    native_values = b_values.native()
    native_indices = b_indices.native()
    backend = choose_backend(native_values, native_indices)
    native_result = backend.batched_gather_nd(native_values, native_indices)
    result_shape = Shape(backend.staticshape(native_result),
                         (&#39;batch&#39;, *indices.shape.non_channel.without(values.shape.batch).names, &#39;vector&#39;),
                         (BATCH_DIM, *indices.shape.non_channel.without(values.shape.batch).types, CHANNEL_DIM))
    b_result = NativeTensor(native_result, result_shape)
    result = split_dimension(b_result, &#39;vector&#39;, values.shape.channel)
    result = split_dimension(result, &#39;batch&#39;, values.shape.batch)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.get_precision"><code class="name flex">
<span>def <span class="ident">get_precision</span></span>(<span>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the current target floating point precision in bits.
The precision can be set globally using <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code> or locally using <code>with precision(p):</code>.</p>
<p>Any Backend method may convert floating point values to this precision, even if the input had a different precision.</p>
<h2 id="returns">Returns</h2>
<p>16 for half, 32 for single, 64 for double</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_precision() -&gt; int:
    &#34;&#34;&#34;
    Gets the current target floating point precision in bits.
    The precision can be set globally using `set_global_precision()` or locally using `with precision(p):`.

    Any Backend method may convert floating point values to this precision, even if the input had a different precision.

    Returns:
        16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    return _PRECISION[-1]</code></pre>
</details>
</dd>
<dt id="phi.math.gradients"><code class="name flex">
<span>def <span class="ident">gradients</span></span>(<span>y: phi.math._tensors.Tensor, *x: phi.math._tensors.Tensor, grad_y: phi.math._tensors.Tensor = None)</span>
</code></dt>
<dd>
<div class="desc"><p><em>Deprecated. Use <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code> instead.</em></p>
<p>Computes the gradients dy/dx for each <code>x</code>.
The parameters <code>x</code> must be marked prior to all operations for which gradients should be recorded using <code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>y</code></strong></dt>
<dd>Scalar <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> computed from <code>x</code>, typically loss.</dd>
<dt><strong><code>*x</code></strong></dt>
<dd>(Optional) Parameter tensors which were previously marked in <code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients()</a></code>.
If empty, computes the gradients w.r.t. all marked tensors.</dd>
<dt><strong><code>grad_y</code></strong></dt>
<dd>(optional) Gradient at <code>y</code>, defaults to 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> if one <code>x</code> was passed, else sequence of tensors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gradients(y: Tensor,
              *x: Tensor,
              grad_y: Tensor or None = None):
    &#34;&#34;&#34;
    *Deprecated. Use `functional_gradient()` instead.*

    Computes the gradients dy/dx for each `x`.
    The parameters `x` must be marked prior to all operations for which gradients should be recorded using `record_gradients()`.

    Args:
        y: Scalar `Tensor` computed from `x`, typically loss.
        *x: (Optional) Parameter tensors which were previously marked in `record_gradients()`.
            If empty, computes the gradients w.r.t. all marked tensors.
        grad_y: (optional) Gradient at `y`, defaults to 1.

    Returns:
        Single `Tensor` if one `x` was passed, else sequence of tensors.
    &#34;&#34;&#34;
    warnings.warn(&#34;math.gradients() is deprecated. Use functional_gradient() instead.&#34;, DeprecationWarning)
    assert isinstance(y, NativeTensor), f&#34;{type(y)}&#34;
    if len(x) == 0:
        x = _PARAM_STACK[-1]
    backend = choose_backend_t(y, *x)
    x_natives = sum([x_._natives() for x_ in x], ())
    native_gradients = list(backend.gradients(y.native(), x_natives, grad_y.native() if grad_y is not None else None))
    for i, grad in enumerate(native_gradients):
        assert grad is not None, f&#34;Missing spatial_gradient for source with shape {x_natives[i].shape}&#34;
    grads = [x_._op1(lambda native: native_gradients.pop(0)) for x_ in x]
    return grads[0] if len(x) == 1 else grads</code></pre>
</details>
</dd>
<dt id="phi.math.grid_sample"><code class="name flex">
<span>def <span class="ident">grid_sample</span></span>(<span>grid: phi.math._tensors.Tensor, coordinates: phi.math._tensors.Tensor, extrap: extrapolation_.Extrapolation)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def grid_sample(grid: Tensor, coordinates: Tensor, extrap: &#39;extrapolation_.Extrapolation&#39;):
    result = broadcast_op(functools.partial(_grid_sample, extrap=extrap), [grid, coordinates])
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.ifft"><code class="name flex">
<span>def <span class="ident">ifft</span></span>(<span>k: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ifft(k: Tensor):
    native, assemble = _invertible_standard_form(k)
    result = choose_backend(native).ifft(native)
    return assemble(result)</code></pre>
</details>
</dd>
<dt id="phi.math.imag"><code class="name flex">
<span>def <span class="ident">imag</span></span>(<span>complex) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def imag(complex) -&gt; Tensor:
    return _backend_op1(complex, Backend.imag)</code></pre>
</details>
</dd>
<dt id="phi.math.isfinite"><code class="name flex">
<span>def <span class="ident">isfinite</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def isfinite(x) -&gt; Tensor:
    return _backend_op1(x, Backend.isfinite)</code></pre>
</details>
</dd>
<dt id="phi.math.jit_compile"><code class="name flex">
<span>def <span class="ident">jit_compile</span></span>(<span>f: Callable) ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Compiles a graph based on the function <code>f</code>.
The graph compilation is performed just-in-time (jit) when the returned function is called for the first time.</p>
<p>The traced function will compute the same result as <code>f</code> but may run much faster.
Some checks may be disabled in the compiled function.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile
def my_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>Compilation is implemented for the following backends:</p>
<ul>
<li>PyTorch: <a href="https://pytorch.org/docs/stable/jit.html"><code>torch.jit.trace</code></a></li>
<li>TensorFlow: <a href="https://www.tensorflow.org/guide/function"><code>tf.function</code></a></li>
<li>Jax: <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions"><code>jax.jit</code></a></li>
</ul>
<p>See Also:
<code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be traced.
All arguments must be of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> returning a single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or a <code>tuple</code> or <code>list</code> of tensors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Function with similar signature and return values as <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit_compile(f: Callable) -&gt; Callable:
    &#34;&#34;&#34;
    Compiles a graph based on the function `f`.
    The graph compilation is performed just-in-time (jit) when the returned function is called for the first time.

    The traced function will compute the same result as `f` but may run much faster.
    Some checks may be disabled in the compiled function.

    Can be used as a decorator:
    ```python
    @math.jit_compile
    def my_function(x: math.Tensor) -&gt; math.Tensor:
    ```

    Compilation is implemented for the following backends:

    * PyTorch: [`torch.jit.trace`](https://pytorch.org/docs/stable/jit.html)
    * TensorFlow: [`tf.function`](https://www.tensorflow.org/guide/function)
    * Jax: [`jax.jit`](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)

    See Also:
        `jit_compile_linear()`

    Args:
        f: Function to be traced.
            All arguments must be of type `Tensor` returning a single `Tensor` or a `tuple` or `list` of tensors.

    Returns:
        Function with similar signature and return values as `f`.
    &#34;&#34;&#34;
    return f if isinstance(f, (JitFunction, LinearFunction)) else JitFunction(f)</code></pre>
</details>
</dd>
<dt id="phi.math.jit_compile_linear"><code class="name flex">
<span>def <span class="ident">jit_compile_linear</span></span>(<span>f: Callable[[~X], ~Y]) ‑> phi.math._functional.LinearFunction[~X, ~Y]</span>
</code></dt>
<dd>
<div class="desc"><p>Compile an optimized representation of the linear function <code>f</code>.</p>
<p>Can be used as a decorator:</p>
<pre><code class="language-python">@math.jit_compile_linear
def my_linear_function(x: math.Tensor) -&gt; math.Tensor:
</code></pre>
<p>See Also:
<code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Linear function with <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> positional arguments and return value(s).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.LinearFunction" href="#phi.math.LinearFunction">LinearFunction</a></code> with similar signature and return values as <code>f</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def jit_compile_linear(f: Callable[[X], Y]) -&gt; &#39;LinearFunction[X, Y]&#39;:
    &#34;&#34;&#34;
    Compile an optimized representation of the linear function `f`.

    Can be used as a decorator:

    ```python
    @math.jit_compile_linear
    def my_linear_function(x: math.Tensor) -&gt; math.Tensor:
    ```

    See Also:
        `jit_compile()`

    Args:
        f: Linear function with `Tensor` positional arguments and return value(s).

    Returns:
        `LinearFunction` with similar signature and return values as `f`.
    &#34;&#34;&#34;
    if isinstance(f, JitFunction):
        f = f.f  # cannot trace linear function from jitted version
    return f if isinstance(f, LinearFunction) else LinearFunction(f)</code></pre>
</details>
</dd>
<dt id="phi.math.join_dimensions"><code class="name flex">
<span>def <span class="ident">join_dimensions</span></span>(<span>value: phi.math._tensors.Tensor, dims: phi.math._shape.Shape, joined_dim_name: str, pos: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compresses multiple dimensions into a single dimension by concatenating the elements.
Elements along the new dimensions are laid out according to the order of <code>dims</code>.
If the order of <code>dims</code> differs from the current dimension order, the tensor is transposed accordingly.</p>
<p>The type of the new dimension will be equal to the types of <code>dims</code>.
If <code>dims</code> have varying types, the new dimension will be a batch dimension.</p>
<p>See Also:
<code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor containing the dimensions <code>dims</code>.</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to be compressed in the specified order.</dd>
<dt><strong><code>joined_dim_name</code></strong></dt>
<dd>Name of the new dimension.</dd>
<dt><strong><code>pos</code></strong></dt>
<dd>Index of new dimension. <code>None</code> for automatic, <code>-1</code> for last, <code>0</code> for first.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with compressed shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def join_dimensions(value: Tensor,
                    dims: Shape or tuple or list,
                    joined_dim_name: str,
                    pos: int or None = None):
    &#34;&#34;&#34;
    Compresses multiple dimensions into a single dimension by concatenating the elements.
    Elements along the new dimensions are laid out according to the order of `dims`.
    If the order of `dims` differs from the current dimension order, the tensor is transposed accordingly.

    The type of the new dimension will be equal to the types of `dims`.
    If `dims` have varying types, the new dimension will be a batch dimension.

    See Also:
        `split_dimension()`

    Args:
        value: Tensor containing the dimensions `dims`.
        dims: Dimensions to be compressed in the specified order.
        joined_dim_name: Name of the new dimension.
        pos: Index of new dimension. `None` for automatic, `-1` for last, `0` for first.

    Returns:
        `Tensor` with compressed shape.
    &#34;&#34;&#34;
    dims = dims.names if isinstance(dims, Shape) else dims
    if len(dims) == 0 or all(dim not in value.shape for dim in dims):
        return CollapsedTensor(value, value.shape.expand(1, joined_dim_name, dim_type(joined_dim_name), pos))
    if len(dims) == 1:
        new_shape = value.shape.with_names([joined_dim_name if name == dims[0] else name for name in value.shape.names])
        return value._with_shape_replaced(new_shape)
    order = value.shape.order_group(dims)
    native = value.native(order)
    types = value.shape.get_type(dims)
    dim_type_ = types[0] if len(set(types)) == 1 else BATCH_DIM
    if pos is None:
        pos = min(value.shape.indices(dims))
    new_shape = value.shape.without(dims).expand(value.shape.only(dims).volume, joined_dim_name, dim_type_, pos)
    native = choose_backend(native).reshape(native, new_shape.sizes)
    return NativeTensor(native, new_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.l1_loss"><code class="name flex">
<span>def <span class="ident">l1_loss</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>1</sub></em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.
For <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> objects, only value the sum over all value attributes is computed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l1_loss(x) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes *∑&lt;sub&gt;i&lt;/sub&gt; ||x&lt;sub&gt;i&lt;/sub&gt;||&lt;sub&gt;1&lt;/sub&gt;*, summing over all non-batch dimensions.

    Args:
        x: `Tensor` or `TensorLike`.
            For `TensorLike` objects, only value the sum over all value attributes is computed.

    Returns:
        loss: `Tensor`
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return math.sum_(abs(x), x.shape.non_batch)
    elif isinstance(x, TensorLike):
        return sum([l1_loss(getattr(x, a)) for a in value_attributes(x)])
    else:
        raise ValueError(x)</code></pre>
</details>
</dd>
<dt id="phi.math.l2_loss"><code class="name flex">
<span>def <span class="ident">l2_loss</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Computes <em>∑<sub>i</sub> ||x<sub>i</sub>||<sub>2</sub><sup>2</sup> / 2</em>, summing over all non-batch dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.
For <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> objects, only value the sum over all value attributes is computed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>loss</code></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def l2_loss(x) -&gt; Tensor:
    &#34;&#34;&#34;
    Computes *∑&lt;sub&gt;i&lt;/sub&gt; ||x&lt;sub&gt;i&lt;/sub&gt;||&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; / 2*, summing over all non-batch dimensions.

    Args:
        x: `Tensor` or `TensorLike`.
            For `TensorLike` objects, only value the sum over all value attributes is computed.

    Returns:
        loss: `Tensor`
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        if x.dtype.kind == complex:
            x = abs(x)
        return math.sum_(x ** 2, x.shape.non_batch) * 0.5
    elif isinstance(x, TensorLike):
        return sum([l2_loss(getattr(x, a)) for a in value_attributes(x)])
    else:
        raise ValueError(x)</code></pre>
</details>
</dd>
<dt id="phi.math.laplace"><code class="name flex">
<span>def <span class="ident">laplace</span></span>(<span>x: phi.math._tensors.Tensor, dx: phi.math._tensors.Tensor = 1, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Laplace operator as defined for scalar fields.
If a vector field is passed, the laplace is computed component-wise.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>n-dimensional field of shape (batch, spacial dimensions&hellip;, components)</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>scalar or 1d tensor</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>The second derivative along these dimensions is summed over</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>dx</code></strong></dt>
<dd>Tensor or float:
(Default value = 1)</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of same shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def laplace(x: Tensor,
            dx: Tensor or float = 1,
            padding: Extrapolation = extrapolation.BOUNDARY,
            dims: tuple or None = None):
    &#34;&#34;&#34;
    Spatial Laplace operator as defined for scalar fields.
    If a vector field is passed, the laplace is computed component-wise.

    Args:
      x: n-dimensional field of shape (batch, spacial dimensions..., components)
      dx: scalar or 1d tensor
      padding: extrapolation
      dims: The second derivative along these dimensions is summed over
      x: Tensor: 
      dx: Tensor or float:  (Default value = 1)
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      tensor of same shape

    &#34;&#34;&#34;
    if not isinstance(dx, (int, float)):
        dx = wrap(dx, names=&#39;_laplace&#39;)
    if isinstance(x, Extrapolation):
        return x.spatial_gradient()
    left, center, right = shift(wrap(x), (-1, 0, 1), dims, padding, stack_dim=&#39;_laplace&#39;)
    result = (left + right - 2 * center) / dx
    result = math.sum_(result, &#39;_laplace&#39;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.linspace"><code class="name flex">
<span>def <span class="ident">linspace</span></span>(<span>start, stop, number: int, dim='linspace')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linspace(start, stop, number: int, dim=&#39;linspace&#39;):
    native = choose_backend(start, stop, number, prefer_default=True).linspace(start, stop, number)
    return NativeTensor(native, shape_(**{dim: number}))</code></pre>
</details>
</dd>
<dt id="phi.math.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Natural logarithm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(x) -&gt; Tensor:
    &#34;&#34;&#34; Natural logarithm. &#34;&#34;&#34;
    return _backend_op1(x, Backend.log)</code></pre>
</details>
</dd>
<dt id="phi.math.log10"><code class="name flex">
<span>def <span class="ident">log10</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log10(x) -&gt; Tensor:
    return _backend_op1(x, Backend.log10)</code></pre>
</details>
</dd>
<dt id="phi.math.log2"><code class="name flex">
<span>def <span class="ident">log2</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log2(x) -&gt; Tensor:
    return _backend_op1(x, Backend.log2)</code></pre>
</details>
</dd>
<dt id="phi.math.map"><code class="name flex">
<span>def <span class="ident">map</span></span>(<span>function, *values: phi.math._tensors.Tensor) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>function</code> on all elements of <code>value</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>function</code></strong></dt>
<dd>Function to be called on single elements contained in <code>value</code>. Must return a value that can be stored in tensors.</dd>
<dt><strong><code>values</code></strong></dt>
<dd>Tensors to iterate over. Number of tensors must match <code>function</code> signature.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of same shape as <code>value</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def map_(function, *values: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    Calls `function` on all elements of `value`.

    Args:
        function: Function to be called on single elements contained in `value`. Must return a value that can be stored in tensors.
        values: Tensors to iterate over. Number of tensors must match `function` signature.

    Returns:
        `Tensor` of same shape as `value`.
    &#34;&#34;&#34;
    shape = combine_safe(*[v.shape for v in values])
    values_reshaped = [CollapsedTensor(v, shape) for v in values]
    flat = [flatten(v) for v in values_reshaped]
    result = []
    for items in zip(*flat):
        result.append(function(*items))
    if None in result:
        assert all(r is None for r in result), f&#34;map function returned None for some elements, {result}&#34;
        return
    return wrap(result).vector.split(shape)</code></pre>
</details>
</dd>
<dt id="phi.math.max"><code class="name flex">
<span>def <span class="ident">max</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.max(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, maximum, lambda x_, y_: choose_backend(x_, y_).maximum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.mean"><code class="name flex">
<span>def <span class="ident">mean</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.mean(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>**dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>generate a TensorStack meshgrid from keyword dimensions</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(**dimensions):
    &#34;&#34;&#34;
    generate a TensorStack meshgrid from keyword dimensions

    Args:
      **dimensions: 

    Returns:

    &#34;&#34;&#34;
    assert &#39;vector&#39; not in dimensions
    dim_values = []
    dim_sizes = []
    for dim, spec in dimensions.items():
        if isinstance(spec, int):
            dim_values.append(tuple(range(spec)))
            dim_sizes.append(spec)
        elif isinstance(spec, Tensor):
            assert spec.rank == 1, f&#34;Only 1D sequences allowed, got {spec} for dimension &#39;{dim}&#39;.&#34;
            dim_values.append(spec.native())
            dim_sizes.append(spec.shape.volume)
        else:
            backend = choose_backend(spec)
            shape = backend.staticshape(spec)
            assert len(shape) == 1, &#34;Only 1D sequences allowed, got {spec} for dimension &#39;{dim}&#39;.&#34;
            dim_values.append(spec)
            dim_sizes.append(shape[0])
    backend = choose_backend(*dim_values, prefer_default=True)
    indices_list = backend.meshgrid(*dim_values)
    grid_shape = Shape(dim_sizes, dimensions.keys(), [SPATIAL_DIM] * len(dim_values))
    channels = [NativeTensor(t, grid_shape) for t in indices_list]
    return TensorStack(channels, &#39;vector&#39;, CHANNEL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.min"><code class="name flex">
<span>def <span class="ident">min</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def min_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.min(native, dim))</code></pre>
</details>
</dd>
<dt id="phi.math.minimize"><code class="name flex">
<span>def <span class="ident">minimize</span></span>(<span>f: Callable[[~X], ~Y], solve: phi.math._functional.Solve[~X, ~Y]) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Finds a minimum of the scalar function <em>f(x)</em>.
The <code>method</code> argument of <code>solve</code> determines which method is used.
All methods supported by <code>scipy.optimize.minimize</code> are supported,
see <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html</a> .</p>
<p>This method is limited to backends that support <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code>, currently PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is subject to minimization.
All positional arguments of <code>f</code> are optimized and must be <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.
The first return value of <code>f</code> must be a scalar float <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object to specify method type, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution, the minimum point <code>x</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the optimization failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimize(f: Callable[[X], Y], solve: Solve[X, Y]) -&gt; X:
    &#34;&#34;&#34;
    Finds a minimum of the scalar function *f(x)*.
    The `method` argument of `solve` determines which method is used.
    All methods supported by `scipy.optimize.minimize` are supported,
    see https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html .

    This method is limited to backends that support `functional_gradient()`, currently PyTorch, TensorFlow and Jax.

    To obtain additional information about the performed solve, use a `SolveTape`.

    See Also:
        `solve_nonlinear()`.

    Args:
        f: Function whose output is subject to minimization.
            All positional arguments of `f` are optimized and must be `Tensor` or `TensorLike`.
            The first return value of `f` must be a scalar float `Tensor` or `TensorLike`.
        solve: `Solve` object to specify method type, parameters and initial guess for `x`.

    Returns:
        x: solution, the minimum point `x`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the optimization failed prematurely.
    &#34;&#34;&#34;
    assert solve.relative_tolerance == 0, f&#34;relative_tolerance must be zero for minimize() but got {solve.relative_tolerance}&#34;
    x0_nest, x0_tensors = disassemble_nested(solve.x0)
    x0_tensors = [to_float(t) for t in x0_tensors]
    backend = choose_backend_t(*x0_tensors, prefer_default=True)
    batch = combine_safe(*[t.shape for t in x0_tensors]).batch
    x0_natives = []
    for t in x0_tensors:
        t._expand()
        assert t.shape.is_uniform
        x0_natives.append(reshaped_native(t, [batch, t.shape.non_batch], force_expand=True))
    x0_flat = backend.concat(x0_natives, -1)

    def unflatten_assemble(x_flat, additional_dims=()):
        i = 0
        x_tensors = []
        for x0_native, x0_tensor in zip(x0_natives, x0_tensors):
            vol = backend.shape(x0_native)[-1]
            flat_native = x_flat[..., i:i + vol]
            x_tensors.append(reshaped_tensor(flat_native, [*additional_dims, batch, x0_tensor.shape.non_batch]))
            i += vol
        x = assemble_nested(x0_nest, x_tensors)
        return x

    def native_function(x_flat):
        x = unflatten_assemble(x_flat)
        if isinstance(x, (tuple, list)):
            y = f(*x)
        else:
            y = f(x)
        _, y_tensors = disassemble_nested(y)
        return y_tensors[0].sum, y_tensors[0].native()

    atol = backend.to_float(reshaped_native(solve.absolute_tolerance, [batch], force_expand=True))
    maxi = backend.to_int32(reshaped_native(solve.max_iterations, [batch], force_expand=True))
    trj = _SOLVE_TAPES and any(t.record_trajectories for t in _SOLVE_TAPES)
    ret = backend.minimize(solve.method, native_function, x0_flat, atol, maxi, trj)
    if not trj:
        assert isinstance(ret, SolveResult)
        converged = reshaped_tensor(ret.converged, [batch])
        diverged = reshaped_tensor(ret.diverged, [batch])
        x = unflatten_assemble(ret.x)
        iterations = reshaped_tensor(ret.iterations, [batch])
        function_evaluations = reshaped_tensor(ret.function_evaluations, [batch])
        residual = reshaped_tensor(ret.residual, [batch])
        result = SolveInfo(solve, x, residual, iterations, function_evaluations, converged, diverged, ret.method, ret.message)
    else:  # trajectory
        assert isinstance(ret, (tuple, list)) and all(isinstance(r, SolveResult) for r in ret)
        converged = reshaped_tensor(ret[-1].converged, [batch])
        diverged = reshaped_tensor(ret[-1].diverged, [batch])
        x = unflatten_assemble(ret[-1].x)
        x_ = unflatten_assemble(backend.stack([r.x for r in ret]), additional_dims=(&#39;trajectory&#39;,))
        residual = batch_stack([reshaped_tensor(r.residual, [batch]) for r in ret], &#39;trajectory&#39;)
        iterations = reshaped_tensor(ret[-1].iterations, [batch])
        function_evaluations = batch_stack([reshaped_tensor(r.function_evaluations, [batch]) for r in ret], &#39;trajectory&#39;)
        result = SolveInfo(solve, x_, residual, iterations, function_evaluations, converged, diverged, ret[-1].method, ret[-1].message)
    for tape in _SOLVE_TAPES:
        tape._add(solve, trj, result)
    result.convergence_check(False)  # raises ConvergenceException
    return x</code></pre>
</details>
</dd>
<dt id="phi.math.minimum"><code class="name flex">
<span>def <span class="ident">minimum</span></span>(<span>x: phi.math._tensors.Tensor, y: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minimum(x: Tensor or float, y: Tensor or float):
    return custom_op2(x, y, minimum, lambda x_, y_: choose_backend(x_, y_).minimum(x_, y_))</code></pre>
</details>
</dd>
<dt id="phi.math.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the native tensor representation of <code>value</code>.
If <code>value</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code>.
Otherwise, checks that <code>value</code> is a valid tensor object and returns it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or native tensor or tensor-like.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(value: Tensor or Number or tuple or list or Any):
    &#34;&#34;&#34;
    Returns the native tensor representation of `value`.
    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.native()`.
    Otherwise, checks that `value` is a valid tensor object and returns it.

    Args:
        value: `Tensor` or native tensor or tensor-like.

    Returns:
        Native tensor representation

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    if isinstance(value, Tensor):
        return value.native()
    else:
        choose_backend(value, raise_error=True)
        return value</code></pre>
</details>
</dd>
<dt id="phi.math.native_call"><code class="name flex">
<span>def <span class="ident">native_call</span></span>(<span>f: Callable, *inputs: phi.math._tensors.Tensor, channels_last=None, channel_dim='vector')</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code>f</code> with the native representations of the <code>inputs</code> tensors in standard layout and returns the result as a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>All inputs are converted to native tensors depending on <code>channels_last</code>:</p>
<ul>
<li><code>channels_last=True</code>: Dimension layout <code>(total_batch_size, spatial_dims&hellip;, total_channel_size)</code></li>
<li><code>channels_last=False</code>: Dimension layout <code>(total_batch_size, total_channel_size, spatial_dims&hellip;)</code></li>
</ul>
<p>All batch dimensions are compressed into a single dimension with <code>total_batch_size = input.shape.batch.volume</code>.
The same is done for all channel dimensions.</p>
<p>Additionally, missing batch and spatial dimensions are added so that all <code>inputs</code> have the same batch and spatial shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function to be called on native tensors of <code>inputs</code>.
The function output must have the same dimension layout as the inputs and the batch size must be identical.</dd>
<dt><strong><code>*inputs</code></strong></dt>
<dd>Uniform <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments</dd>
<dt><strong><code>channels_last</code></strong></dt>
<dd>(Optional) Whether to put channels as the last dimension of the native representation.
If <code>None</code>, the channels are put in the default position associated with the current backend,
see <code><a title="phi.math.backend.Backend.prefers_channels_last" href="backend/index.html#phi.math.backend.Backend.prefers_channels_last">Backend.prefers_channels_last()</a></code>.</dd>
<dt><strong><code>channel_dim</code></strong></dt>
<dd>Name of the channel dimension of the result.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with batch and spatial dimensions of <code>inputs</code> and single channel dimension <code>channel_dim</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native_call(f: Callable, *inputs: Tensor, channels_last=None, channel_dim=&#39;vector&#39;):
    &#34;&#34;&#34;
    Calls `f` with the native representations of the `inputs` tensors in standard layout and returns the result as a `Tensor`.

    All inputs are converted to native tensors depending on `channels_last`:

    * `channels_last=True`: Dimension layout `(total_batch_size, spatial_dims..., total_channel_size)`
    * `channels_last=False`: Dimension layout `(total_batch_size, total_channel_size, spatial_dims...)`

    All batch dimensions are compressed into a single dimension with `total_batch_size = input.shape.batch.volume`.
    The same is done for all channel dimensions.

    Additionally, missing batch and spatial dimensions are added so that all `inputs` have the same batch and spatial shape.

    Args:
        f: Function to be called on native tensors of `inputs`.
            The function output must have the same dimension layout as the inputs and the batch size must be identical.
        *inputs: Uniform `Tensor` arguments
        channels_last: (Optional) Whether to put channels as the last dimension of the native representation.
            If `None`, the channels are put in the default position associated with the current backend,
            see `phi.math.backend.Backend.prefers_channels_last()`.
        channel_dim: Name of the channel dimension of the result.

    Returns:
        `Tensor` with batch and spatial dimensions of `inputs` and single channel dimension `channel_dim`.
    &#34;&#34;&#34;
    if channels_last is None:
        backend = choose_backend_t(*inputs, prefer_default=True)
        channels_last = backend.prefers_channels_last()
    batch = combine_safe(*[i.shape.batch for i in inputs])
    spatial = combine_safe(*[i.shape.spatial for i in inputs])
    natives = []
    for i in inputs:
        groups = (batch, *i.shape.spatial.names, i.shape.channel) if channels_last else (batch, i.shape.channel, *i.shape.spatial.names)
        natives.append(reshaped_native(i, groups))
    output = f(*natives)
    if isinstance(output, (tuple, list)):
        raise NotImplementedError()
    else:
        groups = (batch, *spatial.names, channel_dim) if channels_last else (batch, channel_dim, *spatial.names)
        return reshaped_tensor(output, groups)</code></pre>
</details>
</dd>
<dt id="phi.math.nonzero"><code class="name flex">
<span>def <span class="ident">nonzero</span></span>(<span>value: phi.math._tensors.Tensor, list_dim='nonzero', index_dim='vector')</span>
</code></dt>
<dd>
<div class="desc"><p>Get spatial indices of non-zero / True values.</p>
<p>Batch dimensions are preserved by this operation.
If channel dimensions are present, this method returns the indices where any entry is nonzero.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>spatial tensor to find non-zero / True values in.</dd>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing non-zero values (Default value = 'nonzero')</dd>
<dt><strong><code>index_dim</code></strong></dt>
<dd>name of index dimension (Default value = 'vector')</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch dims&hellip;, list_dim=#non-zero, index_dim=value.shape.spatial_rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nonzero(value: Tensor, list_dim=&#39;nonzero&#39;, index_dim=&#39;vector&#39;):
    &#34;&#34;&#34;
    Get spatial indices of non-zero / True values.
    
    Batch dimensions are preserved by this operation.
    If channel dimensions are present, this method returns the indices where any entry is nonzero.

    Args:
      value: spatial tensor to find non-zero / True values in.
      list_dim: name of dimension listing non-zero values (Default value = &#39;nonzero&#39;)
      index_dim: name of index dimension (Default value = &#39;vector&#39;)
      value: Tensor: 

    Returns:
      tensor of shape (batch dims..., list_dim=#non-zero, index_dim=value.shape.spatial_rank)

    &#34;&#34;&#34;
    if value.shape.channel_rank &gt; 0:
        value = sum_(abs(value), value.shape.channel)

    def unbatched_nonzero(value):
        native = value.native()
        backend = choose_backend(native)
        indices = backend.nonzero(native)
        indices_shape = Shape(backend.staticshape(indices), (list_dim, index_dim), (BATCH_DIM, CHANNEL_DIM))
        return NativeTensor(indices, indices_shape)

    return broadcast_op(unbatched_nonzero, [value], iter_dims=value.shape.batch.names)</code></pre>
</details>
</dd>
<dt id="phi.math.normalize_to"><code class="name flex">
<span>def <span class="ident">normalize_to</span></span>(<span>target: phi.math._tensors.Tensor, source: phi.math._tensors.Tensor, epsilon=1e-05)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiplies the target so that its total content matches the source.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>target</code></strong></dt>
<dd>a tensor</dd>
<dt><strong><code>source</code></strong></dt>
<dd>a tensor or number</dd>
<dt><strong><code>epsilon</code></strong></dt>
<dd>small number to prevent division by zero or None. (Default value = 1e-5)</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>source</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>normalized tensor of the same shape as target</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_to(target: Tensor, source: Tensor, epsilon=1e-5):
    &#34;&#34;&#34;
    Multiplies the target so that its total content matches the source.

    Args:
      target: a tensor
      source: a tensor or number
      epsilon: small number to prevent division by zero or None. (Default value = 1e-5)
      target: Tensor: 
      source: Tensor: 

    Returns:
      normalized tensor of the same shape as target

    &#34;&#34;&#34;
    target_total = math.sum_(target, dim=target.shape.non_batch.names)
    denominator = math.maximum(target_total, epsilon) if epsilon is not None else target_total
    source_total = math.sum_(source, dim=source.shape.non_batch.names)
    return target * (source_total / denominator)</code></pre>
</details>
</dd>
<dt id="phi.math.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Converts <code>value</code> to a <code>numpy.ndarray</code> where value must be a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, backend tensor or tensor-like.
If <code>value</code> is a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, this is equal to calling <code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">Tensor.numpy()</a></code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If <code>value</code> is a NumPy array, it may be returned directly.</p>
<h2 id="returns">Returns</h2>
<p>NumPy representation of <code>value</code></p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(value: Tensor or Number or tuple or list or Any):
    &#34;&#34;&#34;
    Converts `value` to a `numpy.ndarray` where value must be a `Tensor`, backend tensor or tensor-like.
    If `value` is a `phi.math.Tensor`, this is equal to calling `phi.math.Tensor.numpy()`.

    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use `Tensor.native()` instead.

    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    If `value` is a NumPy array, it may be returned directly.

    Returns:
        NumPy representation of `value`

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    if isinstance(value, Tensor):
        return value.numpy()
    else:
        backend = choose_backend(value)
        return backend.numpy(value)</code></pre>
</details>
</dd>
<dt id="phi.math.ones"><code class="name flex">
<span>def <span class="ident">ones</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 1 / True everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 1 / True everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().ones((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.ones_like"><code class="name flex">
<span>def <span class="ident">ones_like</span></span>(<span>tensor: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_like(tensor: Tensor):
    return zeros(tensor.shape, dtype=tensor.dtype) + 1</code></pre>
</details>
</dd>
<dt id="phi.math.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>value: phi.math._tensors.Tensor, widths: dict, mode: extrapolation_.Extrapolation) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor along the specified dimensions, determining the added values using the given extrapolation_.</p>
<p>This is equivalent to calling <code>mode.pad(value, widths)</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Extrapolation object</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
<dt><strong><code>mode</code></strong></dt>
<dd>'extrapolation_.Extrapolation': </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>padded Tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(value: Tensor, widths: dict, mode: &#39;extrapolation_.Extrapolation&#39;) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor along the specified dimensions, determining the added values using the given extrapolation_.
    
    This is equivalent to calling `mode.pad(value, widths)`.

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)
      mode: Extrapolation object
      value: Tensor: 
      widths: dict: 
      mode: &#39;extrapolation_.Extrapolation&#39;: 

    Returns:
      padded Tensor

    &#34;&#34;&#34;
    return mode.pad(value, widths)</code></pre>
</details>
</dd>
<dt id="phi.math.precision"><code class="name flex">
<span>def <span class="ident">precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision for the local context.</p>
<p>Usage: <code>with precision(p):</code></p>
<p>This overrides the global setting, see <code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>16 for half, 32 for single, 64 for double</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision for the local context.

    Usage: `with precision(p):`

    This overrides the global setting, see `set_global_precision()`.

    Args:
        floating_point_bits: 16 for half, 32 for single, 64 for double
    &#34;&#34;&#34;
    _PRECISION.append(floating_point_bits)
    try:
        yield None
    finally:
        _PRECISION.pop(-1)</code></pre>
</details>
</dd>
<dt id="phi.math.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>obj: phi.math._tensors.Tensor = None, name: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.</p>
<p>Unlike NumPy's array printing, the dimensions are sorted.
Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
Typically, this means x right, y up.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd>tensor-like</dd>
<dt><strong><code>name</code></strong></dt>
<dd>name of the tensor</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_(obj: Tensor or TensorLike or Number or tuple or list or None = None, name: str = &#34;&#34;):
    &#34;&#34;&#34;
    Print a tensor with no more than two spatial dimensions, slicing it along all batch and channel dimensions.
    
    Unlike NumPy&#39;s array printing, the dimensions are sorted.
    Elements along the alphabetically first dimension is printed to the right, the second dimension upward.
    Typically, this means x right, y up.

    Args:
        obj: tensor-like
        name: name of the tensor

    Returns:

    &#34;&#34;&#34;
    def variables(obj) -&gt; dict:
        if hasattr(obj, &#39;__variable_attrs__&#39;) or hasattr(obj, &#39;__value_attrs__&#39;):
            return {f&#34;.{a}&#34;: getattr(obj, a) for a in variable_attributes(obj)}
        elif isinstance(obj, (tuple, list)):
            return {f&#34;[{i}]&#34;: item for i, item in enumerate(obj)}
        elif isinstance(obj, dict):
            return obj
        else:
            raise ValueError(f&#34;Not TensorLike: {type(obj)}&#34;)

    if obj is None:
        print()
    elif isinstance(obj, Tensor):
        _print_tensor(obj, name)
    elif isinstance(obj, TensorLike):
        for n, val in variables(obj).items():
            print_(val, name + n)
    else:
        value = wrap(obj)
        _print_tensor(value, name)</code></pre>
</details>
</dd>
<dt id="phi.math.print_gradient"><code class="name flex">
<span>def <span class="ident">print_gradient</span></span>(<span>value: phi.math._tensors.Tensor, name='', detailed=False) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Prints the gradient vector of <code>value</code> when computed.
The gradient at <code>value</code> is the vector-Jacobian product of all operations between the output of this function and the loss value.</p>
<p>The gradient is not printed in jit mode, see <code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code>.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python">def f(x):
    x = math.print_gradient(x, 'dx')
    return math.l1_loss(x)

math.functional_gradient(f)(math.ones(x=6))
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> for which the gradient may be computed later.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>(Optional) Name to print along with the gradient values</dd>
<dt><strong><code>detailed</code></strong></dt>
<dd>If <code>False</code>, prints a short summary of the gradient tensor.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>identity(value)</code> which when differentiated, prints the gradient vector.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_gradient(value: Tensor, name=&#34;&#34;, detailed=False) -&gt; Tensor:
    &#34;&#34;&#34;
    Prints the gradient vector of `value` when computed.
    The gradient at `value` is the vector-Jacobian product of all operations between the output of this function and the loss value.

    The gradient is not printed in jit mode, see `jit_compile()`.

    Example:
        ```python
        def f(x):
            x = math.print_gradient(x, &#39;dx&#39;)
            return math.l1_loss(x)

        math.functional_gradient(f)(math.ones(x=6))
        ```

    Args:
        value: `Tensor` for which the gradient may be computed later.
        name: (Optional) Name to print along with the gradient values
        detailed: If `False`, prints a short summary of the gradient tensor.

    Returns:
        `identity(value)` which when differentiated, prints the gradient vector.
    &#34;&#34;&#34;
    def print_grad(_x, _y, dx):
        if all_available(_x, dx):
            if detailed:
                print_(dx, name=name)
            else:
                print(f&#34;{name}:  \t{dx}&#34;)
        return dx,
    identity = custom_gradient(lambda x: x, print_grad)
    return identity(value)</code></pre>
</details>
</dd>
<dt id="phi.math.prod"><code class="name flex">
<span>def <span class="ident">prod</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prod(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.prod(native, dim),
                   collapsed_function=lambda inner, red_shape: inner ** red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.random_normal"><code class="name flex">
<span>def <span class="ident">random_normal</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_normal(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_normal(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_normal, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.random_uniform"><code class="name flex">
<span>def <span class="ident">random_uniform</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform(shape=EMPTY_SHAPE, dtype=None, **dimensions):

    def uniform_random_uniform(shape, dtype):
        native = choose_backend(*shape.sizes, prefer_default=True).random_uniform(shape.sizes)
        native = native if dtype is None else native.astype(dtype)
        return NativeTensor(native, shape)

    return _initialize(uniform_random_uniform, shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.range"><code class="name flex">
<span>def <span class="ident">range</span></span>(<span>start_or_stop: int, stop: int = None, step=1, dim='range')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def arange(start_or_stop: int, stop: int or None = None, step=1, dim=&#39;range&#39;):
    if stop is None:
        start, stop = 0, start_or_stop
    else:
        start = start_or_stop
    native = choose_backend(start, stop, prefer_default=True).range(start, stop, step, DType(int, 32))
    return NativeTensor(native, shape_(**{dim: stop - start}))</code></pre>
</details>
</dd>
<dt id="phi.math.range_tensor"><code class="name flex">
<span>def <span class="ident">range_tensor</span></span>(<span>shape: phi.math._shape.Shape, **dims)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def range_tensor(shape: Shape, **dims):
    shape &amp;= shape_(**dims)
    data = arange(0, shape.volume)
    result = split_dimension(data, &#39;range&#39;, shape)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.real"><code class="name flex">
<span>def <span class="ident">real</span></span>(<span>complex) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def real(complex) -&gt; Tensor:
    return _backend_op1(complex, Backend.real)</code></pre>
</details>
</dd>
<dt id="phi.math.record_gradients"><code class="name flex">
<span>def <span class="ident">record_gradients</span></span>(<span>*x: phi.math._tensors.Tensor, persistent=False)</span>
</code></dt>
<dd>
<div class="desc"><p><em>Deprecated. Use <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code> instead.</em></p>
<p>Context expression to record gradients for operations within that directly or indirectly depend on <code>x</code>.</p>
<p>The function <code><a title="phi.math.gradients" href="#phi.math.gradients">gradients()</a></code> may be called within the context to evaluate the gradients of a Tensor derived from <code>x</code> w.r.t. <code>x</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*x</code></strong></dt>
<dd>Parameters for which gradients of the form dL/dx may be computed</dd>
<dt><strong><code>persistent</code></strong></dt>
<dd>if <code>False</code>, <code><a title="phi.math.gradients" href="#phi.math.gradients">gradients()</a></code> may only be called once within the context</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@contextmanager
def record_gradients(*x: Tensor, persistent=False):
    &#34;&#34;&#34;
    *Deprecated. Use `functional_gradient()` instead.*

    Context expression to record gradients for operations within that directly or indirectly depend on `x`.

    The function `gradients()` may be called within the context to evaluate the gradients of a Tensor derived from `x` w.r.t. `x`.

    Args:
        *x: Parameters for which gradients of the form dL/dx may be computed
        persistent: if `False`, `gradients()` may only be called once within the context
    &#34;&#34;&#34;
    warnings.warn(&#34;math.record_gradients() is deprecated. Use functional_gradient() instead.&#34;, DeprecationWarning)
    for x_ in x:
        x_._expand()
    natives = sum([x_._natives() for x_ in x], ())
    backend = choose_backend(*natives)
    ctx = backend.record_gradients(natives, persistent=persistent)
    _PARAM_STACK.append(x)
    ctx.__enter__()
    try:
        yield None
    finally:
        ctx.__exit__(None, None, None)
        _PARAM_STACK.pop(0)</code></pre>
</details>
</dd>
<dt id="phi.math.reshaped_native"><code class="name flex">
<span>def <span class="ident">reshaped_native</span></span>(<span>value: phi.math._tensors.Tensor, groups: tuple, force_expand: Any = False, to_numpy=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native representation of <code>value</code> where dimensions are laid out according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phi.math.native" href="#phi.math.native">native()</a></code>, <code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions()</a></code>, <code><a title="phi.math.reshaped_tensor" href="#phi.math.reshaped_tensor">reshaped_tensor()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension names as <code>str</code> or groups of dimensions to be joined as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>force_expand</code></strong></dt>
<dd><code>bool</code> or sequence of dimensions.
If <code>True</code>, repeats the tensor along missing dimensions.
If <code>False</code>, puts singleton dimensions where possible.
If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>If True, converts the native tensor to a <code>numpy.ndarray</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor with dimensions matching <code>groups</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshaped_native(value: Tensor,
                    groups: tuple or list,
                    force_expand: Any = False,
                    to_numpy=False):
    &#34;&#34;&#34;
    Returns a native representation of `value` where dimensions are laid out according to `groups`.

    See Also:
        `native()`, `join_dimensions()`, `reshaped_tensor()`.

    Args:
        value: `Tensor`
        groups: Sequence of dimension names as `str` or groups of dimensions to be joined as `Shape`.
        force_expand: `bool` or sequence of dimensions.
            If `True`, repeats the tensor along missing dimensions.
            If `False`, puts singleton dimensions where possible.
            If a sequence of dimensions is provided, only forces the expansion for groups containing those dimensions.
        to_numpy: If True, converts the native tensor to a `numpy.ndarray`.

    Returns:
        Native tensor with dimensions matching `groups`.
    &#34;&#34;&#34;
    assert isinstance(value, Tensor), f&#34;value must be a Tensor but got {type(value)}&#34;
    order = []
    for i, group in enumerate(groups):
        if isinstance(group, Shape):
            present = value.shape.only(group)
            if force_expand is True or present.volume &gt; 1 or (force_expand is not False and group.only(force_expand).volume &gt; 1):
                value = _expand_dims(value, group)
            value = join_dimensions(value, group, f&#34;group{i}&#34;)
            order.append(f&#34;group{i}&#34;)
        else:
            assert isinstance(group, str), f&#34;Groups must be either str or Shape but got {group}&#34;
            order.append(group)
    return value.numpy(order) if to_numpy else value.native(order)</code></pre>
</details>
</dd>
<dt id="phi.math.reshaped_tensor"><code class="name flex">
<span>def <span class="ident">reshaped_tensor</span></span>(<span>value: Any, groups: tuple, check_sizes=False, convert=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> from a native tensor or tensor-like whereby the dimensions of <code>value</code> are split according to <code>groups</code>.</p>
<p>See Also:
<code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.reshaped_native" href="#phi.math.reshaped_native">reshaped_native()</a></code>, <code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>Native tensor or tensor-like.</dd>
<dt><strong><code>groups</code></strong></dt>
<dd>Sequence of dimension names as <code>str</code> or groups of dimensions to be joined as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
<dt><strong><code>check_sizes</code></strong></dt>
<dd>If True, group sizes must match the sizes of <code>value</code> exactly. Otherwise, allows singleton dimensions.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with all dimensions from <code>groups</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reshaped_tensor(value: Any,
                    groups: tuple or list,
                    check_sizes=False,
                    convert=True):
    &#34;&#34;&#34;
    Creates a `Tensor` from a native tensor or tensor-like whereby the dimensions of `value` are split according to `groups`.

    See Also:
        `phi.math.tensor()`, `reshaped_native()`, `split_dimension()`.

    Args:
        value: Native tensor or tensor-like.
        groups: Sequence of dimension names as `str` or groups of dimensions to be joined as `Shape`.
        check_sizes: If True, group sizes must match the sizes of `value` exactly. Otherwise, allows singleton dimensions.
        convert: If True, converts the data to the native format of the current default backend.
            If False, wraps the data in a `Tensor` but keeps the given data reference if possible.

    Returns:
        `Tensor` with all dimensions from `groups`
    &#34;&#34;&#34;
    names = [group if isinstance(group, str) else f&#39;group{i}&#39; for i, group in enumerate(groups)]
    value = tensor(value, names, convert=convert)
    for i, group in enumerate(groups):
        if isinstance(group, Shape):
            if value.shape.get_size(f&#39;group{i}&#39;) == group.volume:
                value = split_dimension(value, f&#39;group{i}&#39;, group)
            else:
                if check_sizes:
                    raise AssertionError()
                value = value.dimension(f&#39;group{i}&#39;)[0]  # remove group dim
                value = _expand_dims(value, group)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round_(x) -&gt; Tensor:
    return _backend_op1(x, Backend.round)</code></pre>
</details>
</dd>
<dt id="phi.math.sample_subgrid"><code class="name flex">
<span>def <span class="ident">sample_subgrid</span></span>(<span>grid: phi.math._tensors.Tensor, start: phi.math._tensors.Tensor, size: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a sub-grid from <code>grid</code> with equal distance between sampling points.
The values at the new sample points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> to be resampled. Values are assumed to be sampled at cell centers.</dd>
<dt><strong><code>start</code></strong></dt>
<dd>Origin point of sub-grid within <code>grid</code>, measured in number of cells.
Must have a single dimension called <code>vector</code>.
Example: <code>start=(1, 0.5)</code> would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
The order of dims must be equal to <code>size</code> and <code>grid.shape.spatial</code>.</dd>
<dt><strong><code>size</code></strong></dt>
<dd>Resolution of the sub-grid. Must not be larger than the resolution of <code>grid</code>.
The order of dims must be equal to <code>start</code> and <code>grid.shape.spatial</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Sub-grid as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_subgrid(grid: Tensor, start: Tensor, size: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    Samples a sub-grid from `grid` with equal distance between sampling points.
    The values at the new sample points are determined via linear interpolation.

    Args:
        grid: `Tensor` to be resampled. Values are assumed to be sampled at cell centers.
        start: Origin point of sub-grid within `grid`, measured in number of cells.
            Must have a single dimension called `vector`.
            Example: `start=(1, 0.5)` would slice off the first grid point in dim 1 and take the mean of neighbouring points in dim 2.
            The order of dims must be equal to `size` and `grid.shape.spatial`.
        size: Resolution of the sub-grid. Must not be larger than the resolution of `grid`.
            The order of dims must be equal to `start` and `grid.shape.spatial`.

    Returns:
      Sub-grid as `Tensor`
    &#34;&#34;&#34;
    assert start.shape.names == (&#39;vector&#39;,)
    assert grid.shape.spatial.names == size.names
    assert math.all_available(start), &#34;Cannot perform sample_subgrid() during tracing, &#39;start&#39; must be known.&#34;
    discard = {}
    for dim, d_start, d_size in zip(grid.shape.spatial.names, start, size):
        discard[dim] = slice(int(d_start), int(d_start) + d_size + (1 if d_start != 0 else 0))
    grid = grid[discard]
    upper_weight = start % 1
    lower_weight = 1 - upper_weight
    for i, dim in enumerate(grid.shape.spatial.names):
        if upper_weight[i].native() not in (0, 1):
            lower, upper = shift(grid, (0, 1), [dim], padding=None, stack_dim=None)
            grid = upper * upper_weight[i] + lower * lower_weight[i]
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.scatter"><code class="name flex">
<span>def <span class="ident">scatter</span></span>(<span>base_grid: phi.math._tensors.Tensor, indices: phi.math._tensors.Tensor, values: phi.math._tensors.Tensor, scatter_dims: str, mode: str = 'update', outside_handling: str = 'discard', indices_gradient=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Scatters <code>values</code> into <code>base_grid</code> at <code>indices</code>.
Depending on <code>mode</code>, this method has one of the following effects:</p>
<ul>
<li><code>mode='update'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by <code>values</code>. The result is undefined if <code>indices</code> contains duplicates.</li>
<li><code>mode='add'</code>: Adds <code>values</code> to <code>base_grid</code> at <code>indices</code>. The values corresponding to duplicate indices are accumulated.</li>
<li><code>mode='mean'</code>: Replaces the values of <code>base_grid</code> at <code>indices</code> by the mean of all <code>values</code> with the same index.</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base_grid</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> into which <code>values</code> are scattered.</dd>
<dt><strong><code>indices</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of n-dimensional indices at which to place <code>values</code>.
Must have a single channel dimension with size matching the number of spatial dimensions of <code>base_grid</code>.
This dimension is optional if the spatial rank is 1.
Must also contain all <code>scatter_dims</code>.</dd>
<dt><strong><code>values</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of values to scatter at <code>indices</code>.</dd>
<dt><strong><code>scatter_dims</code></strong></dt>
<dd>Dimensions of <code>values</code> and/or <code>indices</code> to reduce during scattering.
These dimensions are not treated as batch dimensions.</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Scatter mode as <code>str</code>. One of ('add', 'mean', 'update')</dd>
<dt><strong><code>outside_handling</code></strong></dt>
<dd>
<p>Defines how indices lying outside the bounds of <code>base_grid</code> are handled.</p>
<ul>
<li><code>'discard'</code>: outside indices are ignored.</li>
<li><code>'clamp'</code>: outside indices are projected onto the closest point inside the grid.</li>
<li><code>'undefined'</code>: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.</li>
</ul>
</dd>
<dt><strong><code>indices_gradient</code></strong></dt>
<dd>Whether to allow the gradient of this operation to be backpropagated through <code>indices</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>base_grid</code> with updated values at <code>indices</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scatter(base_grid: Tensor or Shape,
            indices: Tensor,
            values: Tensor,
            scatter_dims: str or tuple or list or &#39;Shape&#39;,
            mode: str = &#39;update&#39;,
            outside_handling: str = &#39;discard&#39;,
            indices_gradient=False):
    &#34;&#34;&#34;
    Scatters `values` into `base_grid` at `indices`.
    Depending on `mode`, this method has one of the following effects:

    * `mode=&#39;update&#39;`: Replaces the values of `base_grid` at `indices` by `values`. The result is undefined if `indices` contains duplicates.
    * `mode=&#39;add&#39;`: Adds `values` to `base_grid` at `indices`. The values corresponding to duplicate indices are accumulated.
    * `mode=&#39;mean&#39;`: Replaces the values of `base_grid` at `indices` by the mean of all `values` with the same index.

    Args:
        base_grid: `Tensor` into which `values` are scattered.
        indices: `Tensor` of n-dimensional indices at which to place `values`.
            Must have a single channel dimension with size matching the number of spatial dimensions of `base_grid`.
            This dimension is optional if the spatial rank is 1.
            Must also contain all `scatter_dims`.
        values: `Tensor` of values to scatter at `indices`.
        scatter_dims: Dimensions of `values` and/or `indices` to reduce during scattering.
            These dimensions are not treated as batch dimensions.
        mode: Scatter mode as `str`. One of (&#39;add&#39;, &#39;mean&#39;, &#39;update&#39;)
        outside_handling: Defines how indices lying outside the bounds of `base_grid` are handled.

            * `&#39;discard&#39;`: outside indices are ignored.
            * `&#39;clamp&#39;`: outside indices are projected onto the closest point inside the grid.
            * `&#39;undefined&#39;`: All points are expected to lie inside the grid. Otherwise an error may be thrown or an undefined tensor may be returned.
        indices_gradient: Whether to allow the gradient of this operation to be backpropagated through `indices`.

    Returns:
        Copy of `base_grid` with updated values at `indices`.
    &#34;&#34;&#34;
    assert mode in (&#39;update&#39;, &#39;add&#39;, &#39;mean&#39;)
    assert outside_handling in (&#39;discard&#39;, &#39;clamp&#39;, &#39;undefined&#39;)
    assert isinstance(indices_gradient, bool)
    grid_shape = base_grid if isinstance(base_grid, Shape) else base_grid.shape
    assert indices.shape.channel.names == (&#39;vector&#39;,) or (grid_shape.spatial_rank == 1 and indices.shape.channel_rank == 0)

    batches = (values.shape.non_channel &amp; indices.shape.non_channel).without(scatter_dims)
    lists = indices.shape.only(scatter_dims)
    channels = (grid_shape.channel &amp; values.shape.channel).without(scatter_dims)
    # --- Reshape base_grid to (batch, *base_grid.shape, vector) ---
    shaped_base_grid = join_dimensions(base_grid, batches, &#39;batch_&#39;, pos=0)
    shaped_base_grid = join_dimensions(shaped_base_grid, channels, &#39;vector_&#39;, pos=-1)
    # --- Reshape indices to (batch, list, vector) ---
    shaped_indices = join_dimensions(indices, batches, &#39;batch_&#39;, pos=0)
    shaped_indices = join_dimensions(shaped_indices, lists, &#39;list_&#39;, pos=1)
    # --- Reshape values to (batch, list, vector) and expand it to all elements / indices ---
    values = _expand_dims(values, channels)
    values = ones(indices.shape.batch) * values
    shaped_values = join_dimensions(values, batches, &#39;batch_&#39;, pos=0)
    shaped_values = join_dimensions(shaped_values, lists, &#39;list_&#39;, pos=1)
    shaped_values = join_dimensions(shaped_values, channels, &#39;vector_&#39;, pos=-1)

    # --- Set up grid ---
    if isinstance(base_grid, Shape):
        with choose_backend_t(indices, values):
            base_grid = zeros(base_grid &amp; batches &amp; values.shape.channel)
        if mode != &#39;add&#39;:
            base_grid += math.nan
    # --- Handle outside indices ---
    if outside_handling == &#39;clamp&#39;:
        shaped_indices = clip(shaped_indices, 0, tensor(grid_shape.spatial, &#39;vector&#39;) - 1)
    elif outside_handling == &#39;discard&#39;:
        indices_inside = min_((round_(shaped_indices) &gt;= tensor(0.)) &amp;
                              (round_(shaped_indices) &lt; tensor(grid_shape.spatial, &#39;vector&#39;)), &#39;vector&#39;)
        shaped_indices = shaped_indices.list_[indices_inside]
        shaped_values = shaped_values.list_[indices_inside]
        if shaped_indices.shape.is_non_uniform:
            raise NotImplementedError()

    def scatter_forward(shaped_base_grid_, shaped_indices_, shaped_values_):
        shaped_indices_ = to_int32(round_(shaped_indices_))
        native_grid = reshaped_native(base_grid, (batches, *base_grid.shape.spatial.names, channels), force_expand=True)
        native_values = shaped_values_.native(&#39;batch_, list_, vector_&#39;)
        native_indices = shaped_indices_.native(&#39;batch_, list_, vector&#39;)
        backend = choose_backend(native_indices, native_values, native_grid)
        if mode in (&#39;add&#39;, &#39;update&#39;):
            native_result = backend.scatter(native_grid, native_indices, native_values, mode=mode)
        else:  # mean
            zero_grid = backend.zeros_like(native_grid)
            summed = backend.scatter(zero_grid, native_indices, native_values, mode=&#39;add&#39;)
            count = backend.scatter(zero_grid, native_indices, backend.ones_like(native_values), mode=&#39;add&#39;)
            native_result = summed / backend.maximum(count, 1)
            native_result = backend.where(count == 0, native_grid, native_result)
        return tensor(native_result, shaped_base_grid_.shape)

    def scatter_backward(shaped_base_grid_, shaped_indices_, shaped_values_, output, d_output):
        from ._nd import spatial_gradient
        values_grad = gather(d_output, shaped_indices_)
        spatial_gradient_indices = gather(spatial_gradient(d_output), shaped_indices_)
        indices_grad = mean(spatial_gradient_indices * shaped_values_, &#39;vector_&#39;)
        return None, indices_grad, values_grad

    scatter_function = scatter_forward
    if indices_gradient:
        from phi.math import custom_gradient
        scatter_function = custom_gradient(scatter_forward, scatter_backward)

    result = scatter_function(shaped_base_grid, shaped_indices, shaped_values)
    return reshaped_tensor(result, (batches, *base_grid.shape.spatial.names, channels), check_sizes=True)</code></pre>
</details>
</dd>
<dt id="phi.math.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>seed: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the current seed of all backends and the built-in <code>random</code> package.</p>
<p>Calling this function with a fixed value at the start of an application yields reproducible results
as long as the same backend is used.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>seed</code></strong></dt>
<dd>Seed to use.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seed(seed: int):
    &#34;&#34;&#34;
    Sets the current seed of all backends and the built-in `random` package.

    Calling this function with a fixed value at the start of an application yields reproducible results
    as long as the same backend is used.

    Args:
        seed: Seed to use.
    &#34;&#34;&#34;
    for backend in BACKENDS:
        backend.seed(seed)
    import random
    random.seed(0)</code></pre>
</details>
</dd>
<dt id="phi.math.set_global_precision"><code class="name flex">
<span>def <span class="ident">set_global_precision</span></span>(<span>floating_point_bits: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.</p>
<p>If <code>floating_point_bits</code> is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
Operations may also convert floating point values to this precision, even if the input had a different precision.</p>
<p>If <code>floating_point_bits</code> is None, new tensors will default to float32 unless specified otherwise.
The output of math operations has the same precision as its inputs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>floating_point_bits</code></strong></dt>
<dd>one of (16, 32, 64, None)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_global_precision(floating_point_bits: int):
    &#34;&#34;&#34;
    Sets the floating point precision of DYNAMIC_BACKEND which affects all registered backends.

    If `floating_point_bits` is an integer, all floating point tensors created henceforth will be of the corresponding data type, float16, float32 or float64.
    Operations may also convert floating point values to this precision, even if the input had a different precision.

    If `floating_point_bits` is None, new tensors will default to float32 unless specified otherwise.
    The output of math operations has the same precision as its inputs.

    Args:
      floating_point_bits: one of (16, 32, 64, None)
    &#34;&#34;&#34;
    _PRECISION[0] = floating_point_bits</code></pre>
</details>
</dd>
<dt id="phi.math.shape"><code class="name flex">
<span>def <span class="ident">shape</span></span>(<span>**dims: int) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape from the dimension names and their respective sizes.</p>
<p>Dimension types are inferred from the names according to the following rules:</p>
<ul>
<li>single letter -&gt; spatial dimension</li>
<li>starts with 'vector' -&gt; channel dimension</li>
<li>else -&gt; batch dimension</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>names -&gt; size</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shape(**dims: int) -&gt; Shape:  # TODO add (base: Shape or dict, )
    &#34;&#34;&#34;
    Creates a Shape from the dimension names and their respective sizes.
    
    Dimension types are inferred from the names according to the following rules:
    
    * single letter -&gt; spatial dimension
    * starts with &#39;vector&#39; -&gt; channel dimension
    * else -&gt; batch dimension

    Args:
        dims: names -&gt; size

    Returns:
        `Shape`
    &#34;&#34;&#34;
    types = []
    for name, size in dims.items():
        types.append(dim_type(name))
    return Shape(dims.values(), dims.keys(), types)</code></pre>
</details>
</dd>
<dt id="phi.math.shift"><code class="name flex">
<span>def <span class="ident">shift</span></span>(<span>x: phi.math._tensors.Tensor, offsets: tuple, dims: tuple = None, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, stack_dim: str = 'shift') ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>shift Tensor by a fixed offset and abiding by extrapolation</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>Input data</dd>
<dt><strong><code>offsets</code></strong></dt>
<dd>Shift size</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions along which to shift, defaults to None</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>padding to be performed at the boundary, defaults to extrapolation.BOUNDARY</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>dimensions to be stacked, defaults to 'shift'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>offset_tensor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shift(x: Tensor,
          offsets: tuple,
          dims: tuple or None = None,
          padding: Extrapolation or None = extrapolation.BOUNDARY,
          stack_dim: str or None = &#39;shift&#39;) -&gt; list:
    &#34;&#34;&#34;
    shift Tensor by a fixed offset and abiding by extrapolation

    Args:
        x: Input data
        offsets: Shift size
        dims: Dimensions along which to shift, defaults to None
        padding: padding to be performed at the boundary, defaults to extrapolation.BOUNDARY
        stack_dim: dimensions to be stacked, defaults to &#39;shift&#39;

    Returns:
        list: offset_tensor

    &#34;&#34;&#34;
    if stack_dim is None:
        assert len(dims) == 1
    x = wrap(x)
    dims = dims if dims is not None else x.shape.spatial.names
    pad_lower = max(0, -min(offsets))
    pad_upper = max(0, max(offsets))
    if padding:
        x = math.pad(x, {axis: (pad_lower, pad_upper) for axis in dims}, mode=padding)
    offset_tensors = []
    for offset in offsets:
        components = []
        for dimension in dims:
            if padding:
                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(pad_lower, -pad_upper or None) for dim in dims}
            else:
                slices = {dim: slice(pad_lower + offset, (-pad_upper + offset) or None) if dim == dimension else slice(None, None) for dim in dims}
            components.append(x[slices])
        offset_tensors.append(channel_stack(components, stack_dim) if stack_dim is not None else components[0])
    return offset_tensors</code></pre>
</details>
</dd>
<dt id="phi.math.sign"><code class="name flex">
<span>def <span class="ident">sign</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sign(x) -&gt; Tensor:
    return _backend_op1(x, Backend.sign)</code></pre>
</details>
</dd>
<dt id="phi.math.sin"><code class="name flex">
<span>def <span class="ident">sin</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sin(x) -&gt; Tensor:
    return _backend_op1(x, Backend.sin)</code></pre>
</details>
</dd>
<dt id="phi.math.solve_linear"><code class="name flex">
<span>def <span class="ident">solve_linear</span></span>(<span>f: Callable[[~X], ~Y], y: ~Y, solve: phi.math._functional.Solve[~X, ~Y]) ‑> ~X</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the system of linear equations <em>f(x) = y</em> and returns <em>x</em>.
For maximum performance, compile <code>f</code> using <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> beforehand.
This will use a matrix representation of <code>f</code> to solve the linear system.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>The gradient of this operation will perform another linear solve with the parameters specified by <code><a title="phi.math.Solve.gradient_solve" href="#phi.math.Solve.gradient_solve">Solve.gradient_solve</a></code>.</p>
<p>See Also:
<code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code>, <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Linear function with single <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> positional argument and return value.</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>f(x)</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>solution of the linear system of equations <code>f(x) = y</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve_linear(f: Callable[[X], Y], y: Y, solve: Solve[X, Y]) -&gt; X:
    &#34;&#34;&#34;
    Solves the system of linear equations *f(x) = y* and returns *x*.
    For maximum performance, compile `f` using `jit_compile_linear()` beforehand.
    This will use a matrix representation of `f` to solve the linear system.

    To obtain additional information about the performed solve, use a `SolveTape`.

    The gradient of this operation will perform another linear solve with the parameters specified by `Solve.gradient_solve`.

    See Also:
        `solve_nonlinear()`, `jit_compile_linear()`.

    Args:
        f: Linear function with single `Tensor` or `TensorLike` positional argument and return value.
        y: Desired output of `f(x)` as `Tensor` or `TensorLike`.
        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.

    Returns:
        x: solution of the linear system of equations `f(x) = y` as `Tensor` or `TensorLike`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the solve failed prematurely.
    &#34;&#34;&#34;
    y_nest, y_tensors = disassemble_nested(y)
    x0_nest, x0_tensors = disassemble_nested(solve.x0)
    assert len(x0_tensors) == len(y_tensors) == 1, &#34;Only single-tensor linear solves are currently supported&#34;
    backend = choose_backend_t(*y_tensors, *x0_tensors)

    if not all_available(*y_tensors, *x0_tensors):  # jit mode
        f = jit_compile_linear(f) if backend.supports(Backend.sparse_tensor) else jit_compile(f)

    if isinstance(f, LinearFunction) and backend.supports(Backend.sparse_tensor):
        matrix = f.sparse_coordinate_matrix(solve.x0)
        return _matrix_solve(y, solve, matrix, backend=backend)  # custom_gradient
    else:
        return _function_solve(y, solve, f=f, backend=backend)  # custom_gradient</code></pre>
</details>
</dd>
<dt id="phi.math.solve_nonlinear"><code class="name flex">
<span>def <span class="ident">solve_nonlinear</span></span>(<span>f: Callable, y, solve: phi.math._functional.Solve) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Solves the non-linear equation <em>f(x) = y</em> by minimizing the norm of the residual.</p>
<p>This method is limited to backends that support <code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code>, currently PyTorch, TensorFlow and Jax.</p>
<p>To obtain additional information about the performed solve, use a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code>.</p>
<p>See Also:
<code><a title="phi.math.minimize" href="#phi.math.minimize">minimize()</a></code>, <code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>f</code></strong></dt>
<dd>Function whose output is optimized to match <code>y</code>.
All positional arguments of <code>f</code> are optimized and must be <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.
The output of <code>f</code> must match <code>y</code>.</dd>
<dt><strong><code>y</code></strong></dt>
<dd>Desired output of <code>f(x)</code> as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.</dd>
<dt><strong><code>solve</code></strong></dt>
<dd><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> object specifying optimization method, parameters and initial guess for <code>x</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>x</code></dt>
<dd>Solution fulfilling <code>f(x) = y</code> within specified tolerance as <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></dt>
<dd>If the desired accuracy was not be reached within the maximum number of iterations.</dd>
<dt><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></dt>
<dd>If the solve failed prematurely.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve_nonlinear(f: Callable, y, solve: Solve) -&gt; Tensor:
    &#34;&#34;&#34;
    Solves the non-linear equation *f(x) = y* by minimizing the norm of the residual.

    This method is limited to backends that support `functional_gradient()`, currently PyTorch, TensorFlow and Jax.

    To obtain additional information about the performed solve, use a `SolveTape`.

    See Also:
        `minimize()`, `solve_linear()`.

    Args:
        f: Function whose output is optimized to match `y`.
            All positional arguments of `f` are optimized and must be `Tensor` or `TensorLike`.
            The output of `f` must match `y`.
        y: Desired output of `f(x)` as `Tensor` or `TensorLike`.
        solve: `Solve` object specifying optimization method, parameters and initial guess for `x`.

    Returns:
        x: Solution fulfilling `f(x) = y` within specified tolerance as `Tensor` or `TensorLike`.

    Raises:
        NotConverged: If the desired accuracy was not be reached within the maximum number of iterations.
        Diverged: If the solve failed prematurely.
    &#34;&#34;&#34;
    from ._nd import l2_loss

    def min_func(x):
        diff = f(x) - y
        l2 = l2_loss(diff)
        return l2

    rel_tol_to_abs = solve.relative_tolerance * l2_loss(y, batch_norm=True)
    solve.absolute_tolerance = rel_tol_to_abs
    solve.relative_tolerance = 0
    return minimize(min_func, solve)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_gradient"><code class="name flex">
<span>def <span class="ident">spatial_gradient</span></span>(<span>grid: phi.math._tensors.Tensor, dx: float = 1, difference: str = 'central', padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None, stack_dim: str = 'gradient')</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the spatial_gradient of a scalar channel from finite differences.
The spatial_gradient vectors are in reverse order, lowest dimension first.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>grid values</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>optional) sequence of dimension names</dd>
<dt><strong><code>dx</code></strong></dt>
<dd>physical distance between grid points (default 1)</dd>
<dt><strong><code>difference</code></strong></dt>
<dd>type of difference, one of ('forward', 'backward', 'central') (default 'forward')</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>tensor padding mode</dd>
<dt><strong><code>stack_dim</code></strong></dt>
<dd>name of the new vector dimension listing the spatial_gradient w.r.t. the various axes</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of shape (batch_size, spatial_dimensions&hellip;, spatial rank)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_gradient(grid: Tensor,
                     dx: float or int = 1,
                     difference: str = &#39;central&#39;,
                     padding: Extrapolation or None = extrapolation.BOUNDARY,
                     dims: tuple or None = None,
                     stack_dim: str = &#39;gradient&#39;):
    &#34;&#34;&#34;
    Calculates the spatial_gradient of a scalar channel from finite differences.
    The spatial_gradient vectors are in reverse order, lowest dimension first.

    Args:
      grid: grid values
      dims: optional) sequence of dimension names
      dx: physical distance between grid points (default 1)
      difference: type of difference, one of (&#39;forward&#39;, &#39;backward&#39;, &#39;central&#39;) (default &#39;forward&#39;)
      padding: tensor padding mode
      stack_dim: name of the new vector dimension listing the spatial_gradient w.r.t. the various axes

    Returns:
      tensor of shape (batch_size, spatial_dimensions..., spatial rank)

    &#34;&#34;&#34;
    grid = wrap(grid)
    if difference.lower() == &#39;central&#39;:
        left, right = shift(grid, (-1, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / (dx * 2)
    elif difference.lower() == &#39;forward&#39;:
        left, right = shift(grid, (0, 1), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    elif difference.lower() == &#39;backward&#39;:
        left, right = shift(grid, (-1, 0), dims, padding, stack_dim=stack_dim)
        return (right - left) / dx
    else:
        raise ValueError(&#39;Invalid difference type: {}. Can be CENTRAL or FORWARD&#39;.format(difference))</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_shape"><code class="name flex">
<span>def <span class="ident">spatial_shape</span></span>(<span>sizes: phi.math._shape.Shape, names: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Shape with the following properties:</p>
<ul>
<li>All dimensions are of type 'spatial'</li>
<li>The shape's <code>names</code> match <code>names</code>, if provided</li>
</ul>
<p>Depending on the type of <code>sizes</code>, returns</p>
<ul>
<li>Shape -&gt; (reordered) spatial sub-shape</li>
<li>dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes</li>
<li>tuple/list of sizes -&gt; matches names to sizes and keeps order</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>list of integers or dict or Shape</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Order of dimensions. Optional if isinstance(sizes, (dict, Shape))</dd>
<dt><strong><code>sizes</code></strong></dt>
<dd>Shape or dict or tuple or list: </dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_shape(sizes: Shape or dict or tuple or list, names: tuple or list = None) -&gt; Shape:
    &#34;&#34;&#34;
    Creates a Shape with the following properties:
    
    * All dimensions are of type &#39;spatial&#39;
    * The shape&#39;s `names` match `names`, if provided
    
    Depending on the type of `sizes`, returns
    
    * Shape -&gt; (reordered) spatial sub-shape
    * dict[dim: str -&gt; size] -&gt; (reordered) shape with given names and sizes
    * tuple/list of sizes -&gt; matches names to sizes and keeps order

    Args:
      sizes: list of integers or dict or Shape
      names: Order of dimensions. Optional if isinstance(sizes, (dict, Shape))
      sizes: Shape or dict or tuple or list: 
      names: tuple or list:  (Default value = None)

    Returns:
      Shape containing only spatial dimensions

    &#34;&#34;&#34;
    return _pure_shape(sizes, names, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_stack"><code class="name flex">
<span>def <span class="ident">spatial_stack</span></span>(<span>values, dim: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_stack(values, dim: str):
    return _stack(values, dim, SPATIAL_DIM)</code></pre>
</details>
</dd>
<dt id="phi.math.spatial_sum"><code class="name flex">
<span>def <span class="ident">spatial_sum</span></span>(<span>value: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_sum(value: Tensor):
    return math.sum_(value, dim=value.shape.spatial.names)</code></pre>
</details>
</dd>
<dt id="phi.math.split_dimension"><code class="name flex">
<span>def <span class="ident">split_dimension</span></span>(<span>value: phi.math._tensors.Tensor, dim: str, split_dims: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Decompresses a tensor dimension by unstacking the elements along it.
The compressed dimension <code>dim</code> is assumed to contain elements laid out according to the order or <code>split_dims</code>.</p>
<p>See Also:
<code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> for which one dimension should be split.</dd>
<dt><strong><code>dim</code></strong></dt>
<dd>Compressed dimension to be decompressed.</dd>
<dt><strong><code>split_dims</code></strong></dt>
<dd>Ordered new dimensions to replace <code>dim</code> as <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> with decompressed shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split_dimension(value: Tensor, dim: str, split_dims: Shape):
    &#34;&#34;&#34;
    Decompresses a tensor dimension by unstacking the elements along it.
    The compressed dimension `dim` is assumed to contain elements laid out according to the order or `split_dims`.

    See Also:
        `join_dimensions()`

    Args:
        value: `Tensor` for which one dimension should be split.
        dim: Compressed dimension to be decompressed.
        split_dims: Ordered new dimensions to replace `dim` as `Shape`.

    Returns:
        `Tensor` with decompressed shape
    &#34;&#34;&#34;
    if split_dims.rank == 0:
        return value.dimension(dim)[0]  # remove dim
    if split_dims.rank == 1:
        new_shape = value.shape.without(dim).expand(split_dims.sizes[0], split_dims.name, split_dims.types[0], pos=value.shape.index(dim))
        return value._with_shape_replaced(new_shape)
    else:
        native = value.native()
        new_shape = value.shape.without(dim)
        i = value.shape.index(dim)
        for size, name, dim_type in split_dims.dimensions:
            new_shape = new_shape.expand(size, name, dim_type, pos=i)
            i += 1
        native_reshaped = choose_backend(native).reshape(native, new_shape.sizes)
        return NativeTensor(native_reshaped, new_shape)</code></pre>
</details>
</dd>
<dt id="phi.math.sqrt"><code class="name flex">
<span>def <span class="ident">sqrt</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt(x) -&gt; Tensor:
    return _backend_op1(x, Backend.sqrt)</code></pre>
</details>
</dd>
<dt id="phi.math.std"><code class="name flex">
<span>def <span class="ident">std</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def std(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.std(native, dim),
                   collapsed_function=lambda inner, red_shape: inner,
                   unaffected_function=lambda value: value * 0)</code></pre>
</details>
</dd>
<dt id="phi.math.stop_gradient"><code class="name flex">
<span>def <span class="ident">stop_gradient</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"><p>Disables gradients for the given tensor.
This may switch off the gradients for <code>value</code> itself or create a copy of <code>value</code> with disabled gradients.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> for which gradients should be disabled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Copy of <code>x</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stop_gradient(x):
    &#34;&#34;&#34;
    Disables gradients for the given tensor.
    This may switch off the gradients for `value` itself or create a copy of `value` with disabled gradients.

    Args:
        x: `Tensor` or `TensorLike` for which gradients should be disabled.

    Returns:
        Copy of `x`.
    &#34;&#34;&#34;
    if isinstance(x, Tensor):
        return x._op1(lambda native: choose_backend(native).stop_gradient(native))
    elif isinstance(x, TensorLike):
        nest, values = disassemble_nested(x)
        new_values = [stop_gradient(v) for v in values]
        return assemble_nested(nest, new_values)
    else:
        return wrap(choose_backend(x).stop_gradient(x))</code></pre>
</details>
</dd>
<dt id="phi.math.sum"><code class="name flex">
<span>def <span class="ident">sum</span></span>(<span>value: phi.math._tensors.Tensor, dim: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sum_(value: Tensor or list or tuple,
         dim: str or int or tuple or list or None or Shape = None) -&gt; Tensor:
    return _reduce(value, dim,
                   native_function=lambda backend, native, dim: backend.sum(native, dim),
                   collapsed_function=lambda inner, red_shape: inner * red_shape.volume)</code></pre>
</details>
</dd>
<dt id="phi.math.tan"><code class="name flex">
<span>def <span class="ident">tan</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tan(x) -&gt; Tensor:
    return _backend_op1(x, Backend.tan)</code></pre>
</details>
</dd>
<dt id="phi.math.tensor"><code class="name flex">
<span>def <span class="ident">tensor</span></span>(<span>data: phi.math._tensors.Tensor, names: str = None, convert: bool = True) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Create a Tensor from the specified <code>data</code>.
If <code>convert=True</code>, converts <code>data</code> to the preferred format of the default backend.</p>
<p><code>data</code> must be one of the following:</p>
<ul>
<li>Number: returns a dimensionless Tensor.</li>
<li>Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.</li>
<li><code>tuple</code> or <code>list</code> of numbers: backs the Tensor with native tensor.</li>
<li><code>tuple</code> or <code>list</code> of non-numbers: creates tensors for the items and stacks them.</li>
<li>Tensor: renames dimensions and dimension types if <code>names</code> is specified. Converts all internal native values of the tensor if <code>convert=True</code>.</li>
<li>Shape: creates a 1D tensor listing the dimension sizes.</li>
</ul>
<p>While specifying <code>names</code> is optional in some cases, it is recommended to always specify them.</p>
<p>Dimension types are always inferred from the dimension names if specified.</p>
<p>See Also:
<code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> which uses <code>convert=False</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>native tensor, scalar, sequence, Shape or Tensor</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Dimension names. Dimension types are inferred from the names.</dd>
<dt><strong><code>convert</code></strong></dt>
<dd>If True, converts the data to the native format of the current default backend.
If False, wraps the data in a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> but keeps the given data reference if possible.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>if dimension names are not provided and cannot automatically be inferred</dd>
<dt><code>ValueError</code></dt>
<dd>if <code>data</code> is not tensor-like</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor containing same values as data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensor(data: Tensor or Shape or tuple or list or numbers.Number,
           names: str or tuple or list = None,
           convert: bool = True) -&gt; Tensor:  # TODO assume convert_unsupported, add convert_external=False for constants
    &#34;&#34;&#34;
    Create a Tensor from the specified `data`.
    If `convert=True`, converts `data` to the preferred format of the default backend.

    `data` must be one of the following:
    
    * Number: returns a dimensionless Tensor.
    * Native tensor such as NumPy array, TensorFlow tensor or PyTorch tensor.
    * `tuple` or `list` of numbers: backs the Tensor with native tensor.
    * `tuple` or `list` of non-numbers: creates tensors for the items and stacks them.
    * Tensor: renames dimensions and dimension types if `names` is specified. Converts all internal native values of the tensor if `convert=True`.
    * Shape: creates a 1D tensor listing the dimension sizes.
    
    While specifying `names` is optional in some cases, it is recommended to always specify them.
    
    Dimension types are always inferred from the dimension names if specified.

    See Also:
        `phi.math.wrap()` which uses `convert=False`.

    Args:
      data: native tensor, scalar, sequence, Shape or Tensor
      names: Dimension names. Dimension types are inferred from the names.
      convert: If True, converts the data to the native format of the current default backend.
        If False, wraps the data in a `Tensor` but keeps the given data reference if possible.

    Raises:
      AssertionError: if dimension names are not provided and cannot automatically be inferred
      ValueError: if `data` is not tensor-like

    Returns:
      Tensor containing same values as data
    &#34;&#34;&#34;
    if isinstance(data, Tensor):
        if convert:
            backend = choose_backend(*data._natives())
            if backend != default_backend():
                data = data._op1(lambda n: convert_(n, use_dlpack=False))
        if names is None:
            return data
        else:
            names = parse_dim_names(names, data.rank)
            names = [n if n is not None else o for n, o in zip(names, data.shape.names)]
            types = [dim_type(n) if n is not None else o for n, o in zip(names, data.shape.types)]
            new_shape = Shape(data.shape.sizes, names, types)
            return data._with_shape_replaced(new_shape)
    elif isinstance(data, Shape):
        assert names is not None
        data = data.sizes
    elif isinstance(data, (numbers.Number, bool, str)):
        assert not names, f&#34;Trying to create a zero-dimensional Tensor from value &#39;{data}&#39; but names={names}&#34;
        if convert:
            data = default_backend().as_tensor(data, convert_external=True)
        return NativeTensor(data, EMPTY_SHAPE)
    if isinstance(data, (tuple, list)):
        array = np.array(data)
        if array.dtype != object:
            data = array
        else:
            elements = tensors(*data, names=None if names is None else names[1:], convert=convert)
            common_shape = combine_safe(*[e.shape for e in elements])
            rank = 1 + common_shape.rank
            stack_dim = &#39;vector&#39; if names is None else parse_dim_names(names, rank)[0]
            assert all(stack_dim not in t.shape for t in elements), f&#34;Cannot stack tensors with dimension &#39;{stack_dim}&#39; because a tensor already has that dimension.&#34;
            elements = [CollapsedTensor(e, common_shape) if e.shape.rank &lt; common_shape.rank else e for e in elements]
            from ._ops import cast_same
            elements = cast_same(*elements)
            return TensorStack(elements, dim_name=stack_dim, dim_type=dim_type(stack_dim))
    backend = choose_backend(data, raise_error=False)
    if backend:
        if names is None:
            assert backend.ndims(data) &lt;= 1, &#34;Specify dimension names for tensors with more than 1 dimension&#34;
            names = [&#39;vector&#39;] * backend.ndims(data)  # [] or [&#39;vector&#39;]
            types = [CHANNEL_DIM] * backend.ndims(data)
        else:
            names = parse_dim_names(names, len(data.shape))
            assert None not in names, f&#34;All names must be specified but got {names}&#34;
            types = [dim_type(n) for n in names]
        shape = Shape(data.shape, names, types)
        if convert:
            data = convert_(data, use_dlpack=False)
        return NativeTensor(data, shape)
    raise ValueError(f&#34;{type(data)} is not supported. Only (Tensor, tuple, list, np.ndarray, native tensors) are allowed.\nCurrent backends: {BACKENDS}&#34;)</code></pre>
</details>
</dd>
<dt id="phi.math.tensors"><code class="name flex">
<span>def <span class="ident">tensors</span></span>(<span>*objects: phi.math._tensors.Tensor, names: str = None, convert: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calls <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code> on multiple arguments independently.</p>
<h2 id="example">Example</h2>
<p>scalar_tensor, vector_tensor = tensors(0, (1, 2, 3))</p>
<h2 id="returns">Returns</h2>
<p>Sequence of same length as <code>objects</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tensors(*objects: Tensor or Shape or tuple or list or numbers.Number,
            names: str or tuple or list = None,
            convert: bool = True):
    &#34;&#34;&#34;
    Calls `tensor()` on multiple arguments independently.

    Example:

        scalar_tensor, vector_tensor = tensors(0, (1, 2, 3))

    Returns:
        Sequence of same length as `objects`.
    &#34;&#34;&#34;
    return [tensor(obj, names, convert) for obj in objects]</code></pre>
</details>
</dd>
<dt id="phi.math.to_complex"><code class="name flex">
<span>def <span class="ident">to_complex</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_complex(x) -&gt; Tensor:
    return _backend_op1(x, Backend.to_complex)</code></pre>
</details>
</dd>
<dt id="phi.math.to_float"><code class="name flex">
<span>def <span class="ident">to_float</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Converts the given tensor to floating point format with the currently specified precision.</p>
<p>The precision can be set globally using <code>math.set_global_precision()</code> and locally using <code>with math.precision()</code>.</p>
<p>See the <code><a title="phi.math" href="#phi.math">phi.math</a></code> module documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html">https://tum-pbs.github.io/PhiFlow/Math.html</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>x</code></strong></dt>
<dd>values to convert</dd>
<dt><strong><code>x</code></strong></dt>
<dd>Tensor: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tensor of same shape as <code>x</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_float(x) -&gt; Tensor:
    &#34;&#34;&#34;
    Converts the given tensor to floating point format with the currently specified precision.
    
    The precision can be set globally using `math.set_global_precision()` and locally using `with math.precision()`.
    
    See the `phi.math` module documentation at https://tum-pbs.github.io/PhiFlow/Math.html

    Args:
      x: values to convert
      x: Tensor: 

    Returns:
      Tensor of same shape as `x`

    &#34;&#34;&#34;
    return _backend_op1(x, Backend.to_float)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int32"><code class="name flex">
<span>def <span class="ident">to_int32</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int32(x) -&gt; Tensor:
    return _backend_op1(x, Backend.to_int32)</code></pre>
</details>
</dd>
<dt id="phi.math.to_int64"><code class="name flex">
<span>def <span class="ident">to_int64</span></span>(<span>x) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_int64(x) -&gt; Tensor:
    return _backend_op1(x, Backend.to_int64)</code></pre>
</details>
</dd>
<dt id="phi.math.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>value, axes)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transpose(value, axes):
    if isinstance(value, Tensor):
        return CollapsedTensor(value, value.shape[axes])
    else:
        return choose_backend(value).transpose(value, axes)</code></pre>
</details>
</dd>
<dt id="phi.math.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>value: phi.math._tensors.Tensor, dim: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Alias for <code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">Tensor.unstack()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(value: Tensor, dim: str):
    &#34;&#34;&#34; Alias for `Tensor.unstack()` &#34;&#34;&#34;
    return value.unstack(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.upsample2x"><code class="name flex">
<span>def <span class="ident">upsample2x</span></span>(<span>grid: phi.math._tensors.Tensor, padding: <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a> = boundary, dims: tuple = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Resamples a regular grid to double the number of spatial sample points per dimension.
The grid values at the new points are determined via linear interpolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>grid</code></strong></dt>
<dd>half-size grid</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>grid extrapolation</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>dims along which up-sampling is applied. If None, up-sample along all spatial dims.</dd>
<dt><strong><code>grid</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Extrapolation:
(Default value = extrapolation.BOUNDARY)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>double-size grid</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upsample2x(grid: Tensor,
               padding: Extrapolation = extrapolation.BOUNDARY,
               dims: tuple or None = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Resamples a regular grid to double the number of spatial sample points per dimension.
    The grid values at the new points are determined via linear interpolation.

    Args:
      grid: half-size grid
      padding: grid extrapolation
      dims: dims along which up-sampling is applied. If None, up-sample along all spatial dims.
      grid: Tensor: 
      padding: Extrapolation:  (Default value = extrapolation.BOUNDARY)
      dims: tuple or None:  (Default value = None)

    Returns:
      double-size grid

    &#34;&#34;&#34;
    for i, dim in enumerate(grid.shape.spatial.only(dims).names):
        left, center, right = shift(grid, (-1, 0, 1), (dim,), padding, None)
        interp_left = 0.25 * left + 0.75 * center
        interp_right = 0.75 * center + 0.25 * right
        stacked = math.spatial_stack([interp_left, interp_right], &#39;_interleave&#39;)
        grid = math.join_dimensions(stacked, (dim, &#39;_interleave&#39;), dim)
    return grid</code></pre>
</details>
</dd>
<dt id="phi.math.vec_abs"><code class="name flex">
<span>def <span class="ident">vec_abs</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_abs(vec: Tensor):
    return math.sqrt(math.sum_(vec ** 2, dim=vec.shape.channel.names))</code></pre>
</details>
</dd>
<dt id="phi.math.vec_squared"><code class="name flex">
<span>def <span class="ident">vec_squared</span></span>(<span>vec: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vec_squared(vec: Tensor):
    return math.sum_(vec ** 2, dim=vec.shape.channel.names)</code></pre>
</details>
</dd>
<dt id="phi.math.where"><code class="name flex">
<span>def <span class="ident">where</span></span>(<span>condition: phi.math._tensors.Tensor, value_true: phi.math._tensors.Tensor, value_false: phi.math._tensors.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a tensor by choosing either values from <code>value_true</code> or <code>value_false</code> depending on <code>condition</code>.
If <code>condition</code> is not of type boolean, non-zero values are interpreted as True.</p>
<p>This function requires non-None values for <code>value_true</code> and <code>value_false</code>.
To get the indices of True / non-zero values, use :func:<code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>condition</code></strong></dt>
<dd>determines where to choose values from value_true or from value_false</dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>values to pick where condition != 0 / True</dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>values to pick where condition == 0 / False</dd>
<dt><strong><code>condition</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_true</code></strong></dt>
<dd>Tensor or float or int: </dd>
<dt><strong><code>value_false</code></strong></dt>
<dd>Tensor or float or int: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor containing dimensions of all inputs</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def where(condition: Tensor or float or int, value_true: Tensor or float or int, value_false: Tensor or float or int):
    &#34;&#34;&#34;
    Builds a tensor by choosing either values from `value_true` or `value_false` depending on `condition`.
    If `condition` is not of type boolean, non-zero values are interpreted as True.
    
    This function requires non-None values for `value_true` and `value_false`.
    To get the indices of True / non-zero values, use :func:`nonzero`.

    Args:
      condition: determines where to choose values from value_true or from value_false
      value_true: values to pick where condition != 0 / True
      value_false: values to pick where condition == 0 / False
      condition: Tensor or float or int: 
      value_true: Tensor or float or int: 
      value_false: Tensor or float or int: 

    Returns:
      tensor containing dimensions of all inputs

    &#34;&#34;&#34;
    condition, value_true, value_false = tensors(condition, value_true, value_false)
    shape, (c, vt, vf) = broadcastable_native_tensors(condition, value_true, value_false)
    result = choose_backend(c, vt, vf).where(c, vt, vf)
    return NativeTensor(result, shape)</code></pre>
</details>
</dd>
<dt id="phi.math.wrap"><code class="name flex">
<span>def <span class="ident">wrap</span></span>(<span>data: phi.math._tensors.Tensor, names: str = None) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Short for <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code> with <code>convert=False</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap(data: Tensor or Shape or tuple or list or numbers.Number,
         names: str or tuple or list = None) -&gt; Tensor:
    &#34;&#34;&#34; Short for `phi.math.tensor()` with `convert=False`. &#34;&#34;&#34;
    return tensor(data, names=names, convert=False)  # TODO inline, simplify</code></pre>
</details>
</dd>
<dt id="phi.math.zeros"><code class="name flex">
<span>def <span class="ident">zeros</span></span>(<span>shape=(), dtype=None, **dimensions)</span>
</code></dt>
<dd>
<div class="desc"><p>Define a tensor with specified shape with value 0 / False everywhere.</p>
<p>This method may not immediately allocate the memory to store the values.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>shape</code></strong></dt>
<dd>base tensor shape (Default value = EMPTY_SHAPE)</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>data type (Default value = None)</dd>
<dt><strong><code>dimensions</code></strong></dt>
<dd>additional dimensions, types are determined from names</dd>
<dt><strong><code>**dimensions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor of specified shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros(shape=EMPTY_SHAPE, dtype=None, **dimensions):
    &#34;&#34;&#34;
    Define a tensor with specified shape with value 0 / False everywhere.
    
    This method may not immediately allocate the memory to store the values.

    Args:
      shape: base tensor shape (Default value = EMPTY_SHAPE)
      dtype: data type (Default value = None)
      dimensions: additional dimensions, types are determined from names
      **dimensions: 

    Returns:
      tensor of specified shape

    &#34;&#34;&#34;
    return _initialize(lambda shape, dtype: CollapsedTensor(NativeTensor(default_backend().zeros((), dtype=dtype), EMPTY_SHAPE), shape), shape, dtype, **dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.zeros_like"><code class="name flex">
<span>def <span class="ident">zeros_like</span></span>(<span>obj)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_like(obj):
    nest, values = disassemble_nested(obj)
    values0 = [zeros(t.shape, dtype=t.dtype) for t in values]
    return assemble_nested(nest, values0)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phi.math.ConvergenceException"><code class="flex name class">
<span>class <span class="ident">ConvergenceException</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for exceptions raised when a solve does not converge.</p>
<p>See Also:
<code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code>, <code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvergenceException(RuntimeError):
    &#34;&#34;&#34;
    Base class for exceptions raised when a solve does not converge.

    See Also:
        `Diverged`, `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        RuntimeError.__init__(self, result.msg)
        self.result: SolveInfo = result
        &#34;&#34;&#34; `SolveInfo` holding information about the solve. &#34;&#34;&#34;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._functional.Diverged</li>
<li>phi.math._functional.NotConverged</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.ConvergenceException.result"><code class="name">var <span class="ident">result</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> holding information about the solve.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.DType"><code class="flex name class">
<span>class <span class="ident">DType</span></span>
<span>(</span><span>kind: type, bits: int = 8)</span>
</code></dt>
<dd>
<div class="desc"><p>Data type for tensors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>kind</code></strong></dt>
<dd>Python type, one of <code>(bool, int, float, complex, str)</code></dd>
<dt><strong><code>bits</code></strong></dt>
<dd>number of bits, typically a multiple of 8.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DType:

    def __init__(self, kind: type, bits: int = 8):
        &#34;&#34;&#34;
        Data type for tensors.

        Args:
          kind: Python type, one of `(bool, int, float, complex, str)`
          bits: number of bits, typically a multiple of 8.
        &#34;&#34;&#34;
        assert kind in (bool, int, float, complex, str)
        if kind is bool:
            assert bits == 8
        else:
            assert isinstance(bits, int)
        self.kind = kind
        &#34;&#34;&#34; Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex) &#34;&#34;&#34;
        self.bits = bits
        &#34;&#34;&#34; Number of bits used to store a single value of this type. See `DType.itemsize`. &#34;&#34;&#34;

    @property
    def precision(self):
        &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
        if self.kind == float:
            return self.bits
        if self.kind == complex:
            return self.bits // 2
        else:
            return None

    @property
    def itemsize(self):
        &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
        assert self.bits % 8 == 0
        return self.bits // 8

    def __eq__(self, other):
        return isinstance(other, DType) and self.kind == other.kind and self.bits == other.bits

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self.kind) + hash(self.bits)

    def __repr__(self):
        return f&#34;{self.kind.__name__}{self.bits}&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.DType.bits"><code class="name">var <span class="ident">bits</span></code></dt>
<dd>
<div class="desc"><p>Number of bits used to store a single value of this type. See <code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">DType.itemsize</a></code>.</p></div>
</dd>
<dt id="phi.math.DType.itemsize"><code class="name">var <span class="ident">itemsize</span></code></dt>
<dd>
<div class="desc"><p>Number of bytes used to storea single value of this type. See <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def itemsize(self):
    &#34;&#34;&#34; Number of bytes used to storea single value of this type. See `DType.bits`. &#34;&#34;&#34;
    assert self.bits % 8 == 0
    return self.bits // 8</code></pre>
</details>
</dd>
<dt id="phi.math.DType.kind"><code class="name">var <span class="ident">kind</span></code></dt>
<dd>
<div class="desc"><p>Python class corresponding to the type of data, ignoring precision. One of (bool, int, float, complex)</p></div>
</dd>
<dt id="phi.math.DType.precision"><code class="name">var <span class="ident">precision</span></code></dt>
<dd>
<div class="desc"><p>Floating point precision. Only defined if <code>kind in (float, complex)</code>. For complex values, returns half of <code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">DType.bits</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def precision(self):
    &#34;&#34;&#34; Floating point precision. Only defined if `kind in (float, complex)`. For complex values, returns half of `DType.bits`. &#34;&#34;&#34;
    if self.kind == float:
        return self.bits
    if self.kind == complex:
        return self.bits // 2
    else:
        return None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Diverged"><code class="flex name class">
<span>class <span class="ident">Diverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised if the optimization was stopped prematurely and cannot continue.
This may indicate that no solution exists.</p>
<p>The values of the last estimate <code>x</code> may or may not be finite.</p>
<p>This exception inherits from <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Diverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised if the optimization was stopped prematurely and cannot continue.
    This may indicate that no solution exists.

    The values of the last estimate `x` may or may not be finite.

    This exception inherits from `ConvergenceException`.

    See Also:
        `NotConverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math._functional.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phi.math.Extrapolation"><code class="flex name class">
<span>class <span class="ident">Extrapolation</span></span>
</code></dt>
<dd>
<div class="desc"><p>Extrapolations are used to determine values of grids or other structures outside the sampled bounds.</p>
<p>They play a vital role in padding and sampling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pad_rank</code></strong></dt>
<dd>low-ranking extrapolations are handled first during mixed-extrapolation padding.</dd>
</dl>
<p>The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.</p>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Extrapolation:

    def __init__(self, pad_rank):
        &#34;&#34;&#34;
        Extrapolations are used to determine values of grids or other structures outside the sampled bounds.

        They play a vital role in padding and sampling.

        Args:
          pad_rank: low-ranking extrapolations are handled first during mixed-extrapolation padding.
        The typical order is periodic=1, boundary=2, symmetric=3, reflect=4, constant=5.

        Returns:

        &#34;&#34;&#34;
        self.pad_rank = pad_rank

    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
        
        Use `from_dict()` to restore the Extrapolation object.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def spatial_gradient(self) -&gt; &#39;Extrapolation&#39;:
        &#34;&#34;&#34;Returns the extrapolation for the spatial spatial_gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
        raise NotImplementedError()

    def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
        &#34;&#34;&#34;
        Pads a tensor using values from self.pad_values()

        Args:
          value: tensor to be padded
          widths: name: str -&gt; (lower: int, upper: int)}
          value: Tensor: 
          widths: dict: 

        Returns:

        &#34;&#34;&#34;
        for dim in widths:
            values = []
            if widths[dim][False] &gt; 0:
                values.append(self.pad_values(value, widths[dim][False], dim, False))
            values.append(value)
            if widths[dim][True] &gt; 0:
                values.append(self.pad_values(value, widths[dim][True], dim, True))
            value = math.concat(values, dim)
        return value

    def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
        &#34;&#34;&#34;
        Determines the values with which the given tensor would be padded at the specified using this extrapolation.

        Args:
          value: tensor to be padded
          width: number of cells to pad perpendicular to the face. Must be larger than zero.
          dimension: axis in which to pad
          upper_edge: True for upper edge, False for lower edge
          value: Tensor: 
          width: int: 
          dimension: str: 
          upper_edge: bool: 

        Returns:
          tensor that can be concatenated to value for padding

        &#34;&#34;&#34;
        raise NotImplementedError()

    def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
        &#34;&#34;&#34;
        If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
        
        Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
        Coordinates are then snapped to the valid index range.
        This is the default implementation.

        Args:
          coordinates: integer coordinates in index space
          shape: tensor shape
          coordinates: Tensor: 
          shape: Shape: 

        Returns:
          transformed coordinates

        &#34;&#34;&#34;
        return math.clip(coordinates, 0, math.wrap(shape.spatial - 1, &#39;vector&#39;))

    @property
    def is_copy_pad(self):
        &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
        return False

    @property
    def native_grid_sample_mode(self) -&gt; Union[str, None]:
        return None

    def __getitem__(self, item):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="phi.math.extrapolation.ConstantExtrapolation" href="extrapolation.html#phi.math.extrapolation.ConstantExtrapolation">ConstantExtrapolation</a></li>
<li>phi.math.extrapolation._CopyExtrapolation</li>
<li>phi.math.extrapolation._MixedExtrapolation</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Extrapolation.is_copy_pad"><code class="name">var <span class="ident">is_copy_pad</span></code></dt>
<dd>
<div class="desc"><p>:return: True if all pad values are copies of existing values in the tensor to be padded</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_copy_pad(self):
    &#34;&#34;&#34;:return: True if all pad values are copies of existing values in the tensor to be padded&#34;&#34;&#34;
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.native_grid_sample_mode"><code class="name">var <span class="ident">native_grid_sample_mode</span> : Union[str, NoneType]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def native_grid_sample_mode(self) -&gt; Union[str, None]:
    return None</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Extrapolation.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>self, value: phi.math._tensors.Tensor, widths: dict) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Pads a tensor using values from self.pad_values()</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>widths</code></strong></dt>
<dd>name: str -&gt; (lower: int, upper: int)}</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>widths</code></strong></dt>
<dd>dict: </dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(self, value: Tensor, widths: dict) -&gt; Tensor:
    &#34;&#34;&#34;
    Pads a tensor using values from self.pad_values()

    Args:
      value: tensor to be padded
      widths: name: str -&gt; (lower: int, upper: int)}
      value: Tensor: 
      widths: dict: 

    Returns:

    &#34;&#34;&#34;
    for dim in widths:
        values = []
        if widths[dim][False] &gt; 0:
            values.append(self.pad_values(value, widths[dim][False], dim, False))
        values.append(value)
        if widths[dim][True] &gt; 0:
            values.append(self.pad_values(value, widths[dim][True], dim, True))
        value = math.concat(values, dim)
    return value</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.pad_values"><code class="name flex">
<span>def <span class="ident">pad_values</span></span>(<span>self, value: phi.math._tensors.Tensor, width: int, dimension: str, upper_edge: bool) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Determines the values with which the given tensor would be padded at the specified using this extrapolation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>value</code></strong></dt>
<dd>tensor to be padded</dd>
<dt><strong><code>width</code></strong></dt>
<dd>number of cells to pad perpendicular to the face. Must be larger than zero.</dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>axis in which to pad</dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>True for upper edge, False for lower edge</dd>
<dt><strong><code>value</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>width</code></strong></dt>
<dd>int: </dd>
<dt><strong><code>dimension</code></strong></dt>
<dd>str: </dd>
<dt><strong><code>upper_edge</code></strong></dt>
<dd>bool: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tensor that can be concatenated to value for padding</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_values(self, value: Tensor, width: int, dimension: str, upper_edge: bool) -&gt; Tensor:
    &#34;&#34;&#34;
    Determines the values with which the given tensor would be padded at the specified using this extrapolation.

    Args:
      value: tensor to be padded
      width: number of cells to pad perpendicular to the face. Must be larger than zero.
      dimension: axis in which to pad
      upper_edge: True for upper edge, False for lower edge
      value: Tensor: 
      width: int: 
      dimension: str: 
      upper_edge: bool: 

    Returns:
      tensor that can be concatenated to value for padding

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.spatial_gradient"><code class="name flex">
<span>def <span class="ident">spatial_gradient</span></span>(<span>self) ‑> <a title="phi.math.extrapolation.Extrapolation" href="extrapolation.html#phi.math.extrapolation.Extrapolation">Extrapolation</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the extrapolation for the spatial spatial_gradient of a tensor/field with this extrapolation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def spatial_gradient(self) -&gt; &#39;Extrapolation&#39;:
    &#34;&#34;&#34;Returns the extrapolation for the spatial spatial_gradient of a tensor/field with this extrapolation.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Serialize this extrapolation to a dictionary that is serializable (JSON-writable).</p>
<p>Use <code>from_dict()</code> to restore the Extrapolation object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Serialize this extrapolation to a dictionary that is serializable (JSON-writable).
    
    Use `from_dict()` to restore the Extrapolation object.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Extrapolation.transform_coordinates"><code class="name flex">
<span>def <span class="ident">transform_coordinates</span></span>(<span>self, coordinates: phi.math._tensors.Tensor, shape: phi.math._shape.Shape) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.</p>
<p>Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
Coordinates are then snapped to the valid index range.
This is the default implementation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>coordinates</code></strong></dt>
<dd>integer coordinates in index space</dd>
<dt><strong><code>shape</code></strong></dt>
<dd>tensor shape</dd>
<dt><strong><code>coordinates</code></strong></dt>
<dd>Tensor: </dd>
<dt><strong><code>shape</code></strong></dt>
<dd>Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>transformed coordinates</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_coordinates(self, coordinates: Tensor, shape: Shape) -&gt; Tensor:
    &#34;&#34;&#34;
    If is_copy_pad, transforms outsider coordinates to point to the index from which the value should be copied.
    
    Otherwise, the grid tensor is assumed to hold the correct boundary values for this extrapolation at the edge.
    Coordinates are then snapped to the valid index range.
    This is the default implementation.

    Args:
      coordinates: integer coordinates in index space
      shape: tensor shape
      coordinates: Tensor: 
      shape: Shape: 

    Returns:
      transformed coordinates

    &#34;&#34;&#34;
    return math.clip(coordinates, 0, math.wrap(shape.spatial - 1, &#39;vector&#39;))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.LinearFunction"><code class="flex name class">
<span>class <span class="ident">LinearFunction</span></span>
</code></dt>
<dd>
<div class="desc"><p>Just-in-time compiled linear function of <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> arguments and return values.</p>
<p>Use <code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code> to create a linear function representation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearFunction(Generic[X, Y], Callable[[X], Y]):
    &#34;&#34;&#34;
    Just-in-time compiled linear function of `Tensor` arguments and return values.

    Use `jit_compile_linear()` to create a linear function representation.
    &#34;&#34;&#34;

    def __init__(self, f):
        self.f = f
        self.tracers: Dict[SignatureKey, ShiftLinTracer] = {}
        self.nl_jit = JitFunction(f)  # for backends that do not support sparse matrices

    def _trace(self, in_key: SignatureKey) -&gt; &#39;ShiftLinTracer&#39;:
        assert len(in_key.shapes) == 1, &#34;Linear functions only support one argument.&#34;
        with in_key.backend:
            x = math.ones(in_key.shapes[0])
            tracer = ShiftLinTracer(x, {EMPTY_SHAPE: math.ones()}, x.shape)
        f_input = assemble_nested(in_key.nest, [tracer])
        assert isinstance(f_input, tuple)
        result = self.f(*f_input)
        _, result_tensors = disassemble_nested(result)
        assert len(result_tensors) == 1, f&#34;Linear function must return a single Tensor or tensor-like but got {result}&#34;
        result_tensor = result_tensors[0]
        assert isinstance(result_tensor, ShiftLinTracer), f&#34;Tracing linear function &#39;{self.f.__name__}&#39; failed. Make sure only linear operations are used.&#34;
        return result_tensor

    def __call__(self, *args: X, **kwargs) -&gt; Y:
        nest, tensors = disassemble_nested(args)
        assert tensors, &#34;Linear function requires at least one argument&#34;
        if any(isinstance(t, ShiftLinTracer) for t in tensors):
            # TODO: if t is identity, use cached ShiftLinTracer, otherwise multiply two ShiftLinTracers
            return self.f(*args, **kwargs)
        backend = math.choose_backend_t(*tensors)
        if not backend.supports(Backend.sparse_tensor):
            warnings.warn(f&#34;Sparse matrices are not supported by {backend}. Falling back to regular jit compilation.&#34;)
            return self.nl_jit(*args, **kwargs)
        natives, shapes = disassemble_tensors(tensors)
        key = SignatureKey(None, nest, shapes, kwargs, backend)
        if key not in self.tracers:
            self.tracers[key] = self._trace(key)
        return self.tracers[key].apply(tensors[0])

    def sparse_coordinate_matrix(self, *args: Tensor, **kwargs):
        key, _ = key_from_args(*args, **kwargs)
        assert key.backend.supports(Backend.sparse_tensor)
        if key not in self.tracers:
            self.tracers[key] = self._trace(key)
        return self.tracers[key].get_sparse_coordinate_matrix()

    def stencil_inspector(self, *args, **kwargs):
        key, _ = key_from_args(*args, **kwargs)
        tracer = self._trace(key)

        def print_stencil(**indices):
            pos = shape(**indices)
            print(f&#34;{self.f.__name__}: {pos} = {&#39; + &#39;.join(f&#39;{val[indices]} * {vector_add(pos, offset)}&#39; for offset, val in tracer.val.items() if (val[indices] != 0).all)}&#34;)

        return print_stencil</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>collections.abc.Callable</li>
<li>typing.Generic</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="phi.math.LinearFunction.sparse_coordinate_matrix"><code class="name flex">
<span>def <span class="ident">sparse_coordinate_matrix</span></span>(<span>self, *args: phi.math._tensors.Tensor, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sparse_coordinate_matrix(self, *args: Tensor, **kwargs):
    key, _ = key_from_args(*args, **kwargs)
    assert key.backend.supports(Backend.sparse_tensor)
    if key not in self.tracers:
        self.tracers[key] = self._trace(key)
    return self.tracers[key].get_sparse_coordinate_matrix()</code></pre>
</details>
</dd>
<dt id="phi.math.LinearFunction.stencil_inspector"><code class="name flex">
<span>def <span class="ident">stencil_inspector</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stencil_inspector(self, *args, **kwargs):
    key, _ = key_from_args(*args, **kwargs)
    tracer = self._trace(key)

    def print_stencil(**indices):
        pos = shape(**indices)
        print(f&#34;{self.f.__name__}: {pos} = {&#39; + &#39;.join(f&#39;{val[indices]} * {vector_add(pos, offset)}&#39; for offset, val in tracer.val.items() if (val[indices] != 0).all)}&#34;)

    return print_stencil</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.NotConverged"><code class="flex name class">
<span>class <span class="ident">NotConverged</span></span>
</code></dt>
<dd>
<div class="desc"><p>Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.</p>
<p>This exception inherits from <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code>.</p>
<p>See Also:
<code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NotConverged(ConvergenceException):
    &#34;&#34;&#34;
    Raised during optimization if the desired accuracy was not reached within the maximum number of iterations.

    This exception inherits from `ConvergenceException`.

    See Also:
        `Diverged`.
    &#34;&#34;&#34;

    def __init__(self, result: SolveInfo):
        ConvergenceException.__init__(self, result)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>phi.math._functional.ConvergenceException</li>
<li>builtins.RuntimeError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
</dd>
<dt id="phi.math.Shape"><code class="flex name class">
<span>class <span class="ident">Shape</span></span>
</code></dt>
<dd>
<div class="desc"><p>Shapes enumerate dimensions, each consisting of a name, size and type.</p>
<p>To construct a Shape manually, use <code><a title="phi.math.shape" href="#phi.math.shape">shape()</a></code> instead.
This constructor is meant for internal use only.</p>
<p>Construct a Shape from sizes, names and types sequences.
All arguments must have same length.</p>
<p>To create a Shape with inferred dimension types, use :func:<code>shape(**dims)</code> instead.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sizes</code></strong></dt>
<dd>Ordered dimension sizes</dd>
<dt><strong><code>names</code></strong></dt>
<dd>Ordered dimension names, either strings (spatial, batch) or integers (channel)</dd>
<dt><strong><code>types</code></strong></dt>
<dd>Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Shape:
    &#34;&#34;&#34;Shapes enumerate dimensions, each consisting of a name, size and type.&#34;&#34;&#34;

    def __init__(self, sizes: tuple or list, names: tuple or list, types: tuple or list):
        &#34;&#34;&#34;
        To construct a Shape manually, use `shape()` instead.
        This constructor is meant for internal use only.

        Construct a Shape from sizes, names and types sequences.
        All arguments must have same length.

        To create a Shape with inferred dimension types, use :func:`shape(**dims)` instead.

        Args:
            sizes: Ordered dimension sizes
            names: Ordered dimension names, either strings (spatial, batch) or integers (channel)
            types: Ordered types, all values should be one of (CHANNEL_DIM, SPATIAL_DIM, BATCH_DIM)
        &#34;&#34;&#34;
        assert len(sizes) == len(names) == len(types), f&#34;sizes={sizes} ({len(sizes)}), names={names} ({len(names)}), types={types} ({len(types)})&#34;
        if len(sizes) &gt; 0:
            from ._tensors import Tensor
            self.sizes = tuple(s if isinstance(s, Tensor) or s is None else int(s) for s in sizes)
        else:
            self.sizes = ()
        &#34;&#34;&#34; Ordered dimension sizes as `tuple`  &#34;&#34;&#34;
        self.names = tuple(names)
        &#34;&#34;&#34; Ordered dimension names as `tuple` of `str` &#34;&#34;&#34;
        assert all(isinstance(n, str) for n in names), f&#34;All names must be of type string but got {names}&#34;
        self.types = tuple(types)  # undocumented, may be private

    @property
    def named_sizes(self):
        &#34;&#34;&#34;
        For iterating over names and sizes

            for name, size in shape.named_sizes:

        Returns:
            iterable
        &#34;&#34;&#34;
        return zip(self.names, self.sizes)

    @property
    def dimensions(self):
        &#34;&#34;&#34;
        For iterating over sizes, names and types.
        Meant for internal use.

        See `Shape.named_sizes()`.
        &#34;&#34;&#34;
        return zip(self.sizes, self.names, self.types)

    def __len__(self):
        return len(self.sizes)

    def __contains__(self, item):
        return item in self.names

    def index(self, name: str or list or tuple or &#39;Shape&#39; or None):
        &#34;&#34;&#34;
        Finds the index of the dimension(s) within this Shape.

        Args:
          name: dimension name or sequence thereof, including Shape object
          name: str or list or tuple or Shape: 

        Returns:
          single index or sequence of indices

        &#34;&#34;&#34;
        if name is None:
            return None
        if isinstance(name, (list, tuple)):
            return tuple(self.index(n) for n in name)
        if isinstance(name, Shape):
            return tuple(self.index(n) for n in name.names)
        for idx, dim_name in enumerate(self.names):
            if dim_name == name:
                return idx
        raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))

    def indices(self, names: tuple or list or &#39;Shape&#39;):
        if isinstance(names, (list, tuple)):
            return tuple(self.index(n) for n in names)
        if isinstance(names, Shape):
            return tuple(self.index(n) for n in names.names)
        else:
            raise ValueError(names)

    def get_size(self, dim: str or tuple or list):
        &#34;&#34;&#34;
        Args:
            dim: dimension name or sequence of dimension names

        Returns:
            size associated with `dim`
        &#34;&#34;&#34;
        if isinstance(dim, str):
            return self.sizes[self.names.index(dim)]
        elif isinstance(dim, (tuple, list)):
            return tuple(self.get_size(n) for n in dim)
        else:
            raise ValueError(dim)

    def __getattr__(self, name):
        if name in self.names:
            return self.get_size(name)
        raise AttributeError(&#34;Shape has no attribute &#39;%s&#39;&#34; % (name,))

    def get_type(self, name: str or tuple or list or &#39;Shape&#39;):
        if isinstance(name, str):
            return self.types[self.names.index(name)]
        elif isinstance(name, (tuple, list)):
            return tuple(self.get_type(n) for n in name)
        elif isinstance(name, Shape):
            return tuple(self.get_type(n) for n in name.names)
        else:
            raise ValueError(name)

    def __getitem__(self, selection):
        if isinstance(selection, int):
            return self.sizes[selection]
        elif isinstance(selection, slice):
            return Shape(self.sizes[selection], self.names[selection], self.types[selection])
        return Shape([self.sizes[i] for i in selection], [self.names[i] for i in selection], [self.types[i] for i in selection])

    @property
    def batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]

    @property
    def non_batch(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]

    @property
    def spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_spatial(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def collection(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the collection dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]

    @property
    def non_collection(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-collection dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]

    @property
    def channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]

    @property
    def non_channel(self) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

        See also:
            `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

        Returns:
            New `Shape` object
        &#34;&#34;&#34;
        return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]

    def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
        &#34;&#34;&#34;
        Slices this `Shape` along a dimension.
        The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

        Non-uniform tensor shapes may be unstacked along other dimensions as well, see
        https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

        Args:
            dim: dimension to unstack

        Returns:
            slices of this shape
        &#34;&#34;&#34;
        if dim == &#39;dims&#39;:
            return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
        if dim not in self:
            return tuple([self])
        else:
            from ._tensors import Tensor
            inner = self.without(dim)
            sizes = []
            dim_size = self.get_size(dim)
            for size in inner.sizes:
                if isinstance(size, Tensor) and dim in size.shape:
                    sizes.append(size.unstack(dim))
                    dim_size = size.shape.get_size(dim)
                else:
                    sizes.append(size)
            assert isinstance(dim_size, int)
            shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
            return shapes

    @property
    def name(self) -&gt; str:
        &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
        assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
        return self.names[0]

    def mask(self, names: tuple or list or set):
        &#34;&#34;&#34;
        Returns a binary sequence corresponding to the names of this Shape.
        A value of 1 means that a dimension of this Shape is contained in `names`.

        Args:
          names: collection of dimension
          names: tuple or list or set: 

        Returns:
          binary sequence

        &#34;&#34;&#34;
        if isinstance(names, str):
            names = [names]
        mask = [1 if name in names else 0 for name in self.names]
        return tuple(mask)

    def __repr__(self):
        strings = [&#39;%s=%s&#39; % (name, size) for size, name, _ in self.dimensions]
        return &#39;(&#39; + &#39;, &#39;.join(strings) + &#39;)&#39;

    def __eq__(self, other):
        if not isinstance(other, Shape):
            return False
        if self.names != other.names or self.types != other.types:
            return False
        for size1, size2 in zip(self.sizes, other.sizes):
            equal = size1 == size2
            assert isinstance(equal, (bool, math.Tensor))
            if isinstance(equal, math.Tensor):
                equal = equal.all
            if not equal:
                return False
        return True

    def __ne__(self, other):
        return not self == other

    def __bool__(self):
        return self.rank &gt; 0

    def normal_order(self):
        sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
        names = self.batch.names + self.spatial.names + self.channel.names
        types = self.batch.types + self.spatial.types + self.channel.types
        return Shape(sizes, names, types)

    def reorder(self, names: tuple or list):
        assert len(names) == self.rank
        order = [self.index(n) for n in names]
        return self[order]

    def order_group(self, names: tuple or list or &#39;Shape&#39;):
        &#34;&#34;&#34; Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. &#34;&#34;&#34;
        if isinstance(names, Shape):
            names = names.names
        result = []
        for dim in self.names:
            if dim not in result:
                if dim in names:
                    result.extend(names)
                else:
                    result.append(dim)
        return result

    def sorted(self, names: tuple or list or &#39;Shape&#39;) -&gt; Tuple[str]:
        &#34;&#34;&#34; Sorts `names` in the order in which they appear in this Shape. Names not part of this shape keep their position after the previous dimension. &#34;&#34;&#34;
        names: Tuple[str] = names.names if isinstance(names, Shape) else names
        positions = {}
        pos = 0
        for name in names:
            if name in self.names:
                pos = self.index(name)
            positions[name] = pos
        return tuple(sorted(names, key=lambda n: positions[n]))

    def alphabetically(self):
        return self.reorder(sorted(self.names))

    @property
    def reversed(self) -&gt; &#39;Shape&#39;:
        return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)))

    def combined(self, other: &#39;Shape&#39;, combine_spatial=False) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a Shape object that both `self` and `other` can be broadcast to.
        If `self` and `other` are incompatible, raises a ValueError.

        Args:
          other: Shape
          other: Shape: 
          combine_spatial:  (Default value = False)

        Returns:
          combined shape
          :raise: ValueError if shapes don&#39;t match

        &#34;&#34;&#34;
        return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])

    def __and__(self, other):
        return combine_safe(self, other, check_exact=[SPATIAL_DIM])

    def expand(self, size, name: str, dim_type: str, pos=None) -&gt; &#39;Shape&#39;:
        if pos is None:
            same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
            if len(same_type_dims) &gt; 0:
                pos = self.index(same_type_dims.names[0])
            else:
                pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
        elif pos &lt; 0:
            pos += self.rank + 1
        sizes = list(self.sizes)
        names = list(self.names)
        types = list(self.types)
        sizes.insert(pos, size)
        names.insert(pos, name)
        types.insert(pos, dim_type)
        return Shape(sizes, names, types)

    def extend(self, other: &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34; Appends the dimensions to the end of this `Shape`. The dimensions of `other` must not be included in this shape. &#34;&#34;&#34;
        for name in other.names:
            assert name not in self.names
        return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)

    def without(self, dims: str or tuple or list or &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Builds a new shape from this one that is missing all given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is `Shape.only()`.

        Args:
          dims: Single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

        Returns:
          Shape without specified dimensions
        &#34;&#34;&#34;
        if isinstance(dims, str):
            return self[[i for i in range(self.rank) if self.names[i] != dims]]
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] not in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
        # elif dims is None:  # subtract all
        #     return EMPTY_SHAPE
        else:
            raise ValueError(dims)

    reduce = without

    def only(self, dims: str or tuple or list or &#39;Shape&#39;):
        &#34;&#34;&#34;
        Builds a new shape from this one that only contains the given dimensions.
        Dimensions in `dims` that are not part of this Shape are ignored.
        
        The complementary operation is :func:`Shape.without`.

        Args:
          dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
          dims: str or tuple or list or Shape: 

        Returns:
          Shape containing only specified dimensions

        &#34;&#34;&#34;
        if isinstance(dims, str):
            dims = parse_dim_order(dims)
        if isinstance(dims, (tuple, list)):
            return self[[i for i in range(self.rank) if self.names[i] in dims]]
        elif isinstance(dims, Shape):
            return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
        elif dims is None:  # keep all
            return self
        else:
            raise ValueError(dims)

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions.
        Equal to `len(shape)`.

        See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
        &#34;&#34;&#34;
        return len(self.sizes)

    @property
    def batch_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == BATCH_DIM:
                r += 1
        return r

    @property
    def spatial_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == SPATIAL_DIM:
                r += 1
        return r

    @property
    def channel_rank(self) -&gt; int:
        &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
        r = 0
        for ty in self.types:
            if ty == CHANNEL_DIM:
                r += 1
        return r

    def to_batch(self, dims: tuple or list or None = None) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns a shape like this Shape but with `dims` being of type `batch`.
        
        Leaves this Shape object untouched.

        Args:
          dims: sequence of dimension names to convert or None to convert all dimensions
          dims: tuple or list or None:  (Default value = None)

        Returns:
          new Shape object

        &#34;&#34;&#34;
        if dims is None:
            return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
        else:
            return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])

    @property
    def well_defined(self):
        &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
        return None not in self.sizes

    @property
    def shape(self, list_dim=&#39;dims&#39;) -&gt; &#39;Shape&#39;:
        &#34;&#34;&#34;
        Returns the shape of this `Shape`.
        The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

        Sizes of type `Tensor` can cause the result to have additional dimensions.

        Args:
            list_dim: name of dimension listing the dimensions of this shape

        Returns:
            second order shape
        &#34;&#34;&#34;
        from phi.math import Tensor
        shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
        for size in self.sizes:
            if isinstance(size, Tensor):
                shape = shape &amp; size.shape
        return shape

    @property
    def is_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is uniform if it all sizes have a single integer value.

        See Also:
            `Shape.is_non_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        return not self.is_non_uniform

    @property
    def is_non_uniform(self) -&gt; bool:
        &#34;&#34;&#34;
        A shape is non-uniform if the size of any dimension varies along another dimension.

        See Also:
            `Shape.is_uniform`, `Shape.shape`.
        &#34;&#34;&#34;
        from phi.math import Tensor
        for size in self.sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                return True
        return False

    def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;):
        if isinstance(sizes, Shape):
            sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
            return Shape(sizes, self.names, self.types)
        else:
            assert len(sizes) == len(self.sizes), f&#34;Cannot create shape from {self} with sizes {sizes}&#34;
            return Shape(sizes, self.names, self.types)

    def with_size(self, name, size):
        new_sizes = list(self.sizes)
        new_sizes[self.index(name)] = size
        return self.with_sizes(new_sizes)

    def with_names(self, names: str or tuple or list):
        if isinstance(names, str):
            names = parse_dim_names(names, self.rank)
            names = [n if n is not None else o for n, o in zip(names, self.names)]
        return Shape(self.sizes, names, self.types)

    def with_types(self, types: &#39;Shape&#39;):
        return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])

    def perm(self, names):
        assert len(set(names)) == len(names), f&#34;No duplicates allowed but got {names}&#34;
        assert len(names) &gt;= len(self.names), f&#34;Cannot find permutation for {self} because names {set(self.names) - set(names)} are missing&#34;
        assert len(names) &lt;= len(self.names), f&#34;Cannot find permutation for {self} because too many names were passed: {names}&#34;
        perm = [self.names.index(name) for name in names]
        return perm

    @property
    def volume(self) -&gt; int or None:
        &#34;&#34;&#34;
        Returns the total number of values contained in a tensor of this shape.
        This is the product of all dimension sizes.

        Returns:
            volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
        &#34;&#34;&#34;
        from phi.math import Tensor
        for dim, size in self.named_sizes:
            if isinstance(size, Tensor) and size.rank &gt; 0:
                non_uniform_dim = size.shape.names[0]
                shapes = self.unstack(non_uniform_dim)
                return sum(s.volume for s in shapes)
        result = 1
        for size in self.sizes:
            if size is None:
                return None
            result *= size
        return int(result)

    @property
    def is_empty(self) -&gt; bool:
        &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
        return len(self.sizes) == 0

    def order(self, sequence, default=None) -&gt; tuple or list:
        &#34;&#34;&#34;
        If sequence is a dict with dimension names as keys, orders its values according to this shape.
        
        Otherwise, the sequence is returned unchanged.

        Args:
          sequence: Sequence or dict to be ordered
          default: default value used for dimensions not contained in sequence

        Returns:
          ordered sequence of values
        &#34;&#34;&#34;
        if isinstance(sequence, dict):
            result = [sequence.get(name, default) for name in self.names]
            return result
        elif isinstance(sequence, (tuple, list)):
            assert len(sequence) == self.rank
            return sequence
        else:  # just a constant
            return sequence

    def after_pad(self, widths: dict):
        sizes = list(self.sizes)
        for dim, (lo, up) in widths.items():
            sizes[self.index(dim)] += lo + up
        return Shape(sizes, self.names, self.types)

    def after_gather(self, selection: dict):
        result = self
        for name, selection in selection.items():
            if name not in self.names:
                continue
            if isinstance(selection, int):
                if result.is_uniform:
                    result = result.without(name)
                else:
                    from phi.math import Tensor
                    gathered_sizes = [(s[{name: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
                    result = result.with_sizes(gathered_sizes).without(name)
            elif isinstance(selection, slice):
                start = selection.start or 0
                stop = selection.stop or self.get_size(name)
                step = selection.step or 1
                if stop &lt; 0:
                    stop += self.get_size(name)
                    assert stop &gt;= 0
                new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
                if new_size.rank == 0:
                    new_size = int(new_size)  # NumPy array not allowed because not hashable
                result = result.with_size(name, new_size)
            else:
                raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
        return result

    def meshgrid(self):
        &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
        indices = [0] * self.rank
        while True:
            yield {name: index for name, index in zip(self.names, indices)}
            for i in range(self.rank-1, -1, -1):
                indices[i] = (indices[i] + 1) % self.sizes[i]
                if indices[i] != 0:
                    break
            else:
                return

    product = meshgrid

    def __add__(self, other):
        return self._op2(other, lambda s, o: s + o)

    def __radd__(self, other):
        return self._op2(other, lambda s, o: o + s)

    def __sub__(self, other):
        return self._op2(other, lambda s, o: s - o)

    def __rsub__(self, other):
        return self._op2(other, lambda s, o: o - s)

    def __mul__(self, other):
        return self._op2(other, lambda s, o: s * o)

    def __rmul__(self, other):
        return self._op2(other, lambda s, o: o * s)

    def _op2(self, other, fun):
        if isinstance(other, int):
            return Shape([fun(s, other) for s in self.sizes], self.names, self.types)
        elif isinstance(other, Shape):
            assert self.names == other.names, f&#34;{self.names, other.names}&#34;
            return Shape([fun(s, o) for s, o in zip(self.sizes, other.sizes)], self.names, self.types)
        else:
            return NotImplemented

    def __hash__(self):
        return hash(self.names)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Shape.batch"><code class="name">var <span class="ident">batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.batch_rank"><code class="name">var <span class="ident">batch_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of batch dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def batch_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of batch dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == BATCH_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel"><code class="name">var <span class="ident">channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.channel_rank"><code class="name">var <span class="ident">channel_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of channel dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def channel_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of channel dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == CHANNEL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.collection"><code class="name">var <span class="ident">collection</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the collection dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def collection(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the collection dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.dimensions"><code class="name">var <span class="ident">dimensions</span></code></dt>
<dd>
<div class="desc"><p>For iterating over sizes, names and types.
Meant for internal use.</p>
<p>See <code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">Shape.named_sizes</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dimensions(self):
    &#34;&#34;&#34;
    For iterating over sizes, names and types.
    Meant for internal use.

    See `Shape.named_sizes()`.
    &#34;&#34;&#34;
    return zip(self.sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_empty"><code class="name">var <span class="ident">is_empty</span> : bool</code></dt>
<dd>
<div class="desc"><p>True if this shape has no dimensions. Equivalent to <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> <code>== 0</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_empty(self) -&gt; bool:
    &#34;&#34;&#34; True if this shape has no dimensions. Equivalent to `Shape.rank` `== 0`. &#34;&#34;&#34;
    return len(self.sizes) == 0</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_non_uniform"><code class="name">var <span class="ident">is_non_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is non-uniform if the size of any dimension varies along another dimension.</p>
<p>See Also:
<code><a title="phi.math.Shape.is_uniform" href="#phi.math.Shape.is_uniform">Shape.is_uniform</a></code>, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_non_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is non-uniform if the size of any dimension varies along another dimension.

    See Also:
        `Shape.is_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    from phi.math import Tensor
    for size in self.sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            return True
    return False</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.is_uniform"><code class="name">var <span class="ident">is_uniform</span> : bool</code></dt>
<dd>
<div class="desc"><p>A shape is uniform if it all sizes have a single integer value.</p>
<p>See Also:
<code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">Shape.is_non_uniform</a></code>, <code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">Shape.shape</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_uniform(self) -&gt; bool:
    &#34;&#34;&#34;
    A shape is uniform if it all sizes have a single integer value.

    See Also:
        `Shape.is_non_uniform`, `Shape.shape`.
    &#34;&#34;&#34;
    return not self.is_non_uniform</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.name"><code class="name">var <span class="ident">name</span> : str</code></dt>
<dd>
<div class="desc"><p>Only for shapes with a single dimension. Returns the name of the dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self) -&gt; str:
    &#34;&#34;&#34; Only for shapes with a single dimension. Returns the name of the dimension. &#34;&#34;&#34;
    assert self.rank == 1, &#39;Shape.name is only defined for shapes of rank 1.&#39;
    return self.names[0]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.named_sizes"><code class="name">var <span class="ident">named_sizes</span></code></dt>
<dd>
<div class="desc"><p>For iterating over names and sizes</p>
<pre><code>for name, size in shape.named_sizes:
</code></pre>
<h2 id="returns">Returns</h2>
<p>iterable</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def named_sizes(self):
    &#34;&#34;&#34;
    For iterating over names and sizes

        for name, size in shape.named_sizes:

    Returns:
        iterable
    &#34;&#34;&#34;
    return zip(self.names, self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"><p>Ordered dimension names as <code>tuple</code> of <code>str</code></p></div>
</dd>
<dt id="phi.math.Shape.non_batch"><code class="name">var <span class="ident">non_batch</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-batch dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_batch(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-batch dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != BATCH_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_channel"><code class="name">var <span class="ident">non_channel</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-channel dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_channel(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-channel dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != CHANNEL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_collection"><code class="name">var <span class="ident">non_collection</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-collection dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_collection(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-collection dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.non_spatial"><code class="name">var <span class="ident">non_spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the non-spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def non_spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the non-spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t != SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the number of dimensions.
Equal to <code>len(<a title="phi.math.shape" href="#phi.math.shape">shape()</a>)</code>.</p>
<p>See <code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">Shape.is_empty</a></code>, <code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">Shape.batch_rank</a></code>, <code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">Shape.spatial_rank</a></code>, <code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">Shape.channel_rank</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions.
    Equal to `len(shape)`.

    See `Shape.is_empty`, `Shape.batch_rank`, `Shape.spatial_rank`, `Shape.channel_rank`.
    &#34;&#34;&#34;
    return len(self.sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reversed"><code class="name">var <span class="ident">reversed</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reversed(self) -&gt; &#39;Shape&#39;:
    return Shape(tuple(reversed(self.sizes)), tuple(reversed(self.names)), tuple(reversed(self.types)))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Returns the shape of this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
The returned shape will always contain the dimension <code>list_dim</code> with a size equal to the <code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">Shape.rank</a></code> of this shape.</p>
<p>Sizes of type <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> can cause the result to have additional dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>list_dim</code></strong></dt>
<dd>name of dimension listing the dimensions of this shape</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>second order shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self, list_dim=&#39;dims&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns the shape of this `Shape`.
    The returned shape will always contain the dimension `list_dim` with a size equal to the `Shape.rank` of this shape.

    Sizes of type `Tensor` can cause the result to have additional dimensions.

    Args:
        list_dim: name of dimension listing the dimensions of this shape

    Returns:
        second order shape
    &#34;&#34;&#34;
    from phi.math import Tensor
    shape = Shape([self.rank], [list_dim], [CHANNEL_DIM])
    for size in self.sizes:
        if isinstance(size, Tensor):
            shape = shape &amp; size.shape
    return shape</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial"><code class="name">var <span class="ident">spatial</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>Filters this shape, returning only the spatial dimensions as a new <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object.</p>
<p>See also:
<code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">Shape.batch</a></code>, <code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">Shape.spatial</a></code>, <code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">Shape.collection</a></code>, <code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">Shape.channel</a></code>, <code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">Shape.non_batch</a></code>, <code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">Shape.non_spatial</a></code>, <code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">Shape.non_collection</a></code>, <code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">Shape.non_channel</a></code>.</p>
<h2 id="returns">Returns</h2>
<p>New <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial(self) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Filters this shape, returning only the spatial dimensions as a new `Shape` object.

    See also:
        `Shape.batch`, `Shape.spatial`, `Shape.collection`, `Shape.channel`, `Shape.non_batch`, `Shape.non_spatial`, `Shape.non_collection`, `Shape.non_channel`.

    Returns:
        New `Shape` object
    &#34;&#34;&#34;
    return self[[i for i, t in enumerate(self.types) if t == SPATIAL_DIM]]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.spatial_rank"><code class="name">var <span class="ident">spatial_rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Number of spatial dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spatial_rank(self) -&gt; int:
    &#34;&#34;&#34; Number of spatial dimensions &#34;&#34;&#34;
    r = 0
    for ty in self.types:
        if ty == SPATIAL_DIM:
            r += 1
    return r</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.volume"><code class="name">var <span class="ident">volume</span> : int</code></dt>
<dd>
<div class="desc"><p>Returns the total number of values contained in a tensor of this shape.
This is the product of all dimension sizes.</p>
<h2 id="returns">Returns</h2>
<p>volume as <code>int</code> or <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code>None</code> if the shape is not <code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">Shape.well_defined</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def volume(self) -&gt; int or None:
    &#34;&#34;&#34;
    Returns the total number of values contained in a tensor of this shape.
    This is the product of all dimension sizes.

    Returns:
        volume as `int` or `Tensor` or `None` if the shape is not `Shape.well_defined`
    &#34;&#34;&#34;
    from phi.math import Tensor
    for dim, size in self.named_sizes:
        if isinstance(size, Tensor) and size.rank &gt; 0:
            non_uniform_dim = size.shape.names[0]
            shapes = self.unstack(non_uniform_dim)
            return sum(s.volume for s in shapes)
    result = 1
    for size in self.sizes:
        if size is None:
            return None
        result *= size
    return int(result)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.well_defined"><code class="name">var <span class="ident">well_defined</span></code></dt>
<dd>
<div class="desc"><p>Returns True if no dimension is <code>None</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def well_defined(self):
    &#34;&#34;&#34; Returns True if no dimension is `None`. &#34;&#34;&#34;
    return None not in self.sizes</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Shape.after_gather"><code class="name flex">
<span>def <span class="ident">after_gather</span></span>(<span>self, selection: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_gather(self, selection: dict):
    result = self
    for name, selection in selection.items():
        if name not in self.names:
            continue
        if isinstance(selection, int):
            if result.is_uniform:
                result = result.without(name)
            else:
                from phi.math import Tensor
                gathered_sizes = [(s[{name: selection}] if isinstance(s, Tensor) else s) for s in result.sizes]
                result = result.with_sizes(gathered_sizes).without(name)
        elif isinstance(selection, slice):
            start = selection.start or 0
            stop = selection.stop or self.get_size(name)
            step = selection.step or 1
            if stop &lt; 0:
                stop += self.get_size(name)
                assert stop &gt;= 0
            new_size = math.to_int64(math.ceil(math.wrap((stop - start) / step)))
            if new_size.rank == 0:
                new_size = int(new_size)  # NumPy array not allowed because not hashable
            result = result.with_size(name, new_size)
        else:
            raise NotImplementedError(f&#34;{type(selection)} not supported. Only (int, slice) allowed.&#34;)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.after_pad"><code class="name flex">
<span>def <span class="ident">after_pad</span></span>(<span>self, widths: dict)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def after_pad(self, widths: dict):
    sizes = list(self.sizes)
    for dim, (lo, up) in widths.items():
        sizes[self.index(dim)] += lo + up
    return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.alphabetically"><code class="name flex">
<span>def <span class="ident">alphabetically</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alphabetically(self):
    return self.reorder(sorted(self.names))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.combined"><code class="name flex">
<span>def <span class="ident">combined</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>, combine_spatial=False) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a Shape object that both <code>self</code> and <code>other</code> can be broadcast to.
If <code>self</code> and <code>other</code> are incompatible, raises a ValueError.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>other</code></strong></dt>
<dd>Shape</dd>
<dt><strong><code>other</code></strong></dt>
<dd>Shape: </dd>
<dt><strong><code>combine_spatial</code></strong></dt>
<dd>(Default value = False)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>combined shape
:raise: ValueError if shapes don't match</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combined(self, other: &#39;Shape&#39;, combine_spatial=False) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns a Shape object that both `self` and `other` can be broadcast to.
    If `self` and `other` are incompatible, raises a ValueError.

    Args:
      other: Shape
      other: Shape: 
      combine_spatial:  (Default value = False)

    Returns:
      combined shape
      :raise: ValueError if shapes don&#39;t match

    &#34;&#34;&#34;
    return combine_safe(self, other, check_exact=[] if combine_spatial else [SPATIAL_DIM])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>self, size, name: str, dim_type: str, pos=None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(self, size, name: str, dim_type: str, pos=None) -&gt; &#39;Shape&#39;:
    if pos is None:
        same_type_dims = self[[i for i, t in enumerate(self.types) if t == dim_type]]
        if len(same_type_dims) &gt; 0:
            pos = self.index(same_type_dims.names[0])
        else:
            pos = {BATCH_DIM: 0, SPATIAL_DIM: self.batch.rank, CHANNEL_DIM: self.rank + 1}[dim_type]
    elif pos &lt; 0:
        pos += self.rank + 1
    sizes = list(self.sizes)
    names = list(self.names)
    types = list(self.types)
    sizes.insert(pos, size)
    names.insert(pos, name)
    types.insert(pos, dim_type)
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.extend"><code class="name flex">
<span>def <span class="ident">extend</span></span>(<span>self, other: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Appends the dimensions to the end of this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>. The dimensions of <code>other</code> must not be included in this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extend(self, other: &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34; Appends the dimensions to the end of this `Shape`. The dimensions of `other` must not be included in this shape. &#34;&#34;&#34;
    for name in other.names:
        assert name not in self.names
    return Shape(self.sizes + other.sizes, self.names + other.names, self.types + other.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_size"><code class="name flex">
<span>def <span class="ident">get_size</span></span>(<span>self, dim: str)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension name or sequence of dimension names</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>size associated with <code>dim</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_size(self, dim: str or tuple or list):
    &#34;&#34;&#34;
    Args:
        dim: dimension name or sequence of dimension names

    Returns:
        size associated with `dim`
    &#34;&#34;&#34;
    if isinstance(dim, str):
        return self.sizes[self.names.index(dim)]
    elif isinstance(dim, (tuple, list)):
        return tuple(self.get_size(n) for n in dim)
    else:
        raise ValueError(dim)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_type(self, name: str or tuple or list or &#39;Shape&#39;):
    if isinstance(name, str):
        return self.types[self.names.index(name)]
    elif isinstance(name, (tuple, list)):
        return tuple(self.get_type(n) for n in name)
    elif isinstance(name, Shape):
        return tuple(self.get_type(n) for n in name.names)
    else:
        raise ValueError(name)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.index"><code class="name flex">
<span>def <span class="ident">index</span></span>(<span>self, name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds the index of the dimension(s) within this Shape.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name or sequence thereof, including Shape object</dd>
<dt><strong><code>name</code></strong></dt>
<dd>str or list or tuple or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>single index or sequence of indices</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def index(self, name: str or list or tuple or &#39;Shape&#39; or None):
    &#34;&#34;&#34;
    Finds the index of the dimension(s) within this Shape.

    Args:
      name: dimension name or sequence thereof, including Shape object
      name: str or list or tuple or Shape: 

    Returns:
      single index or sequence of indices

    &#34;&#34;&#34;
    if name is None:
        return None
    if isinstance(name, (list, tuple)):
        return tuple(self.index(n) for n in name)
    if isinstance(name, Shape):
        return tuple(self.index(n) for n in name.names)
    for idx, dim_name in enumerate(self.names):
        if dim_name == name:
            return idx
    raise ValueError(&#34;Shape %s does not contain dimension with name &#39;%s&#39;&#34; % (self, name))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.indices"><code class="name flex">
<span>def <span class="ident">indices</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def indices(self, names: tuple or list or &#39;Shape&#39;):
    if isinstance(names, (list, tuple)):
        return tuple(self.index(n) for n in names)
    if isinstance(names, Shape):
        return tuple(self.index(n) for n in names.names)
    else:
        raise ValueError(names)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.mask"><code class="name flex">
<span>def <span class="ident">mask</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a binary sequence corresponding to the names of this Shape.
A value of 1 means that a dimension of this Shape is contained in <code>names</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>names</code></strong></dt>
<dd>collection of dimension</dd>
<dt><strong><code>names</code></strong></dt>
<dd>tuple or list or set: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>binary sequence</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mask(self, names: tuple or list or set):
    &#34;&#34;&#34;
    Returns a binary sequence corresponding to the names of this Shape.
    A value of 1 means that a dimension of this Shape is contained in `names`.

    Args:
      names: collection of dimension
      names: tuple or list or set: 

    Returns:
      binary sequence

    &#34;&#34;&#34;
    if isinstance(names, str):
        names = [names]
    mask = [1 if name in names else 0 for name in self.names]
    return tuple(mask)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.meshgrid"><code class="name flex">
<span>def <span class="ident">meshgrid</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.normal_order"><code class="name flex">
<span>def <span class="ident">normal_order</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_order(self):
    sizes = self.batch.sizes + self.spatial.sizes + self.channel.sizes
    names = self.batch.names + self.spatial.names + self.channel.names
    types = self.batch.types + self.spatial.types + self.channel.types
    return Shape(sizes, names, types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.only"><code class="name flex">
<span>def <span class="ident">only</span></span>(<span>self, dims: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that only contains the given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is :func:<code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">Shape.without()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>str or tuple or list or Shape: </dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape containing only specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def only(self, dims: str or tuple or list or &#39;Shape&#39;):
    &#34;&#34;&#34;
    Builds a new shape from this one that only contains the given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is :func:`Shape.without`.

    Args:
      dims: single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: str or tuple or list or Shape: 

    Returns:
      Shape containing only specified dimensions

    &#34;&#34;&#34;
    if isinstance(dims, str):
        dims = parse_dim_order(dims)
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] in dims.names]]
    elif dims is None:  # keep all
        return self
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order"><code class="name flex">
<span>def <span class="ident">order</span></span>(<span>self, sequence, default=None) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>If sequence is a dict with dimension names as keys, orders its values according to this shape.</p>
<p>Otherwise, the sequence is returned unchanged.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sequence</code></strong></dt>
<dd>Sequence or dict to be ordered</dd>
<dt><strong><code>default</code></strong></dt>
<dd>default value used for dimensions not contained in sequence</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ordered sequence of values</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order(self, sequence, default=None) -&gt; tuple or list:
    &#34;&#34;&#34;
    If sequence is a dict with dimension names as keys, orders its values according to this shape.
    
    Otherwise, the sequence is returned unchanged.

    Args:
      sequence: Sequence or dict to be ordered
      default: default value used for dimensions not contained in sequence

    Returns:
      ordered sequence of values
    &#34;&#34;&#34;
    if isinstance(sequence, dict):
        result = [sequence.get(name, default) for name in self.names]
        return result
    elif isinstance(sequence, (tuple, list)):
        assert len(sequence) == self.rank
        return sequence
    else:  # just a constant
        return sequence</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.order_group"><code class="name flex">
<span>def <span class="ident">order_group</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"><p>Reorders the dimensions of this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> so that <code>names</code> are clustered together and occur in the specified order.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def order_group(self, names: tuple or list or &#39;Shape&#39;):
    &#34;&#34;&#34; Reorders the dimensions of this `Shape` so that `names` are clustered together and occur in the specified order. &#34;&#34;&#34;
    if isinstance(names, Shape):
        names = names.names
    result = []
    for dim in self.names:
        if dim not in result:
            if dim in names:
                result.extend(names)
            else:
                result.append(dim)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.perm"><code class="name flex">
<span>def <span class="ident">perm</span></span>(<span>self, names)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def perm(self, names):
    assert len(set(names)) == len(names), f&#34;No duplicates allowed but got {names}&#34;
    assert len(names) &gt;= len(self.names), f&#34;Cannot find permutation for {self} because names {set(self.names) - set(names)} are missing&#34;
    assert len(names) &lt;= len(self.names), f&#34;Cannot find permutation for {self} because too many names were passed: {names}&#34;
    perm = [self.names.index(name) for name in names]
    return perm</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.product"><code class="name flex">
<span>def <span class="ident">product</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a sequence containing all multi-indices within a tensor of this shape.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def meshgrid(self):
    &#34;&#34;&#34;Builds a sequence containing all multi-indices within a tensor of this shape.&#34;&#34;&#34;
    indices = [0] * self.rank
    while True:
        yield {name: index for name, index in zip(self.names, indices)}
        for i in range(self.rank-1, -1, -1):
            indices[i] = (indices[i] + 1) % self.sizes[i]
            if indices[i] != 0:
                break
        else:
            return</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reduce"><code class="name flex">
<span>def <span class="ident">reduce</span></span>(<span>self, dims: str) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is <code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to exclude as <code>str</code> or <code>tuple</code> or <code>list</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>. Dimensions that are not included in this shape are ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is `Shape.only()`.

    Args:
      dims: Single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

    Returns:
      Shape without specified dimensions
    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    # elif dims is None:  # subtract all
    #     return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.reorder"><code class="name flex">
<span>def <span class="ident">reorder</span></span>(<span>self, names: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reorder(self, names: tuple or list):
    assert len(names) == self.rank
    order = [self.index(n) for n in names]
    return self[order]</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.sorted"><code class="name flex">
<span>def <span class="ident">sorted</span></span>(<span>self, names: tuple) ‑> Tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Sorts <code>names</code> in the order in which they appear in this Shape. Names not part of this shape keep their position after the previous dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sorted(self, names: tuple or list or &#39;Shape&#39;) -&gt; Tuple[str]:
    &#34;&#34;&#34; Sorts `names` in the order in which they appear in this Shape. Names not part of this shape keep their position after the previous dimension. &#34;&#34;&#34;
    names: Tuple[str] = names.names if isinstance(names, Shape) else names
    positions = {}
    pos = 0
    for name in names:
        if name in self.names:
            pos = self.index(name)
        positions[name] = pos
    return tuple(sorted(names, key=lambda n: positions[n]))</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.to_batch"><code class="name flex">
<span>def <span class="ident">to_batch</span></span>(<span>self, dims: tuple = None) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shape like this Shape but with <code>dims</code> being of type <code>batch</code>.</p>
<p>Leaves this Shape object untouched.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>sequence of dimension names to convert or None to convert all dimensions</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>tuple or list or None:
(Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>new Shape object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_batch(self, dims: tuple or list or None = None) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Returns a shape like this Shape but with `dims` being of type `batch`.
    
    Leaves this Shape object untouched.

    Args:
      dims: sequence of dimension names to convert or None to convert all dimensions
      dims: tuple or list or None:  (Default value = None)

    Returns:
      new Shape object

    &#34;&#34;&#34;
    if dims is None:
        return Shape(self.sizes, self.names, [BATCH_DIM] * self.rank)
    else:
        return Shape(self.sizes, self.names, [BATCH_DIM if dim in dims else self.types[i] for i, dim in enumerate(self.names)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dim='dims') ‑> Tuple[phi.math._shape.Shape]</span>
</code></dt>
<dd>
<div class="desc"><p>Slices this <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> along a dimension.
The dimension listing the sizes of the shape is referred to as <code>'dims'</code>.</p>
<p>Non-uniform tensor shapes may be unstacked along other dimensions as well, see
<a href="https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors">https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dim</code></strong></dt>
<dd>dimension to unstack</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>slices of this shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dim=&#39;dims&#39;) -&gt; Tuple[&#39;Shape&#39;]:
    &#34;&#34;&#34;
    Slices this `Shape` along a dimension.
    The dimension listing the sizes of the shape is referred to as `&#39;dims&#39;`.

    Non-uniform tensor shapes may be unstacked along other dimensions as well, see
    https://tum-pbs.github.io/PhiFlow/Math.html#non-uniform-tensors

    Args:
        dim: dimension to unstack

    Returns:
        slices of this shape
    &#34;&#34;&#34;
    if dim == &#39;dims&#39;:
        return tuple(Shape([self.sizes[i]], [self.names[i]], [self.types[i]]) for i in range(self.rank))
    if dim not in self:
        return tuple([self])
    else:
        from ._tensors import Tensor
        inner = self.without(dim)
        sizes = []
        dim_size = self.get_size(dim)
        for size in inner.sizes:
            if isinstance(size, Tensor) and dim in size.shape:
                sizes.append(size.unstack(dim))
                dim_size = size.shape.get_size(dim)
            else:
                sizes.append(size)
        assert isinstance(dim_size, int)
        shapes = tuple(Shape([int(size[i]) if isinstance(size, tuple) else size for size in sizes], inner.names, inner.types) for i in range(dim_size))
        return shapes</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_names"><code class="name flex">
<span>def <span class="ident">with_names</span></span>(<span>self, names: str)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_names(self, names: str or tuple or list):
    if isinstance(names, str):
        names = parse_dim_names(names, self.rank)
        names = [n if n is not None else o for n, o in zip(names, self.names)]
    return Shape(self.sizes, names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_size"><code class="name flex">
<span>def <span class="ident">with_size</span></span>(<span>self, name, size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_size(self, name, size):
    new_sizes = list(self.sizes)
    new_sizes[self.index(name)] = size
    return self.with_sizes(new_sizes)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_sizes"><code class="name flex">
<span>def <span class="ident">with_sizes</span></span>(<span>self, sizes: tuple)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_sizes(self, sizes: tuple or list or &#39;Shape&#39;):
    if isinstance(sizes, Shape):
        sizes = [sizes.get_size(dim) if dim in sizes else self.sizes[i] for i, dim in enumerate(self.names)]
        return Shape(sizes, self.names, self.types)
    else:
        assert len(sizes) == len(self.sizes), f&#34;Cannot create shape from {self} with sizes {sizes}&#34;
        return Shape(sizes, self.names, self.types)</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.with_types"><code class="name flex">
<span>def <span class="ident">with_types</span></span>(<span>self, types: <a title="phi.math.Shape" href="#phi.math.Shape">Shape</a>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def with_types(self, types: &#39;Shape&#39;):
    return Shape(self.sizes, self.names, [types.get_type(name) if name in types else self_type for name, self_type in zip(self.names, self.types)])</code></pre>
</details>
</dd>
<dt id="phi.math.Shape.without"><code class="name flex">
<span>def <span class="ident">without</span></span>(<span>self, dims: str) ‑> phi.math._shape.Shape</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new shape from this one that is missing all given dimensions.
Dimensions in <code>dims</code> that are not part of this Shape are ignored.</p>
<p>The complementary operation is <code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">Shape.only()</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dims</code></strong></dt>
<dd>Single dimension (str) or collection of dimensions (tuple, list, Shape)</dd>
<dt><strong><code>dims</code></strong></dt>
<dd>Dimensions to exclude as <code>str</code> or <code>tuple</code> or <code>list</code> or <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>. Dimensions that are not included in this shape are ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Shape without specified dimensions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def without(self, dims: str or tuple or list or &#39;Shape&#39;) -&gt; &#39;Shape&#39;:
    &#34;&#34;&#34;
    Builds a new shape from this one that is missing all given dimensions.
    Dimensions in `dims` that are not part of this Shape are ignored.
    
    The complementary operation is `Shape.only()`.

    Args:
      dims: Single dimension (str) or collection of dimensions (tuple, list, Shape)
      dims: Dimensions to exclude as `str` or `tuple` or `list` or `Shape`. Dimensions that are not included in this shape are ignored.

    Returns:
      Shape without specified dimensions
    &#34;&#34;&#34;
    if isinstance(dims, str):
        return self[[i for i in range(self.rank) if self.names[i] != dims]]
    if isinstance(dims, (tuple, list)):
        return self[[i for i in range(self.rank) if self.names[i] not in dims]]
    elif isinstance(dims, Shape):
        return self[[i for i in range(self.rank) if self.names[i] not in dims.names]]
    # elif dims is None:  # subtract all
    #     return EMPTY_SHAPE
    else:
        raise ValueError(dims)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.Solve"><code class="flex name class">
<span>class <span class="ident">Solve</span></span>
<span>(</span><span>method: str, relative_tolerance: float, absolute_tolerance: float, max_iterations: int = 1000, x0: ~X = None, suppress: tuple = (), gradient_solve: <a title="phi.math.Solve" href="#phi.math.Solve">Solve</a>[Y, X] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Specifies parameters and stopping criteria for solving a minimization problem or system of equations.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solve(Generic[X, Y]):  # TODO move to phi.math._functional, put Tensors there
    &#34;&#34;&#34;
    Specifies parameters and stopping criteria for solving a minimization problem or system of equations.
    &#34;&#34;&#34;

    def __init__(self,
                 method: str,
                 relative_tolerance: float or Tensor,
                 absolute_tolerance: float or Tensor,
                 max_iterations: int or Tensor = 1000,
                 x0: X or Any = None,
                 suppress: tuple or list = (),
                 gradient_solve: &#39;Solve[Y, X]&#39; or None = None):
        assert isinstance(method, str)
        self.method: str = method
        &#34;&#34;&#34; Optimization method to use. Available solvers depend on the solve function that is used to perform the solve. &#34;&#34;&#34;
        self.relative_tolerance: Tensor = wrap(relative_tolerance)
        &#34;&#34;&#34; Relative tolerance for linear solves only. This must be `0` for minimization problems.
        For systems of equations *f(x)=y*, the final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.absolute_tolerance: Tensor = wrap(absolute_tolerance)
        &#34;&#34;&#34; Absolut tolerance for optimization problems and linear solves.
        For systems of equations *f(x)=y*, the final tolerance is `max(relative_tolerance * norm(y), absolute_tolerance)`. &#34;&#34;&#34;
        self.max_iterations: Tensor = wrap(max_iterations)
        &#34;&#34;&#34; Maximum number of iterations to perform before raising a `NotConverged` error is raised. &#34;&#34;&#34;
        self.x0 = x0
        &#34;&#34;&#34; Initial guess for the method, of same type and dimensionality as the solve result.
         This property must be set to a value compatible with the solution `x` before running a method. &#34;&#34;&#34;
        assert all(issubclass(err, ConvergenceException) for err in suppress)
        self.suppress: tuple = tuple(suppress)
        &#34;&#34;&#34; Error types to suppress; `tuple` of `ConvergenceException` types. For these errors, the solve function will instead return the partial result without raising the error. &#34;&#34;&#34;
        self._gradient_solve: Solve[Y, X] = gradient_solve
        self.id = str(uuid.uuid4())

    @property
    def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
        &#34;&#34;&#34;
        Parameters to use for the gradient pass when an implicit gradient is computed.
        If `None`, a duplicate of this `Solve` is created for the gradient solve.

        In any case, the gradient solve information will be stored in `gradient_solve.result`.
        &#34;&#34;&#34;
        if self._gradient_solve is None:
            self._gradient_solve = Solve(self.method, self.relative_tolerance, self.absolute_tolerance, self.max_iterations, None, self.suppress)
        return self._gradient_solve

    def __repr__(self):
        return f&#34;{self.method} with tolerance {self.relative_tolerance} (rel), {self.absolute_tolerance} (abs), max_iterations={self.max_iterations}&#34;

    def __eq__(self, other):
        if not isinstance(other, Solve):
            return False
        if self.method != other.method \
                or self.absolute_tolerance != other.absolute_tolerance \
                or self.relative_tolerance != other.relative_tolerance \
                or self.max_iterations != other.max_iterations \
                or self.suppress != other.suppress:
            return False
        if self.x0 is None:
            return other.x0 is None
        else:
            raise AssertionError(&#34;Cannot compare Solves with x0 set&#34;)

    def __variable_attrs__(self) -&gt; Tuple[str]:
        return &#39;x0&#39;,</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Solve.absolute_tolerance"><code class="name">var <span class="ident">absolute_tolerance</span></code></dt>
<dd>
<div class="desc"><p>Absolut tolerance for optimization problems and linear solves.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.gradient_solve"><code class="name">var <span class="ident">gradient_solve</span> : phi.math._functional.Solve[~Y, ~X]</code></dt>
<dd>
<div class="desc"><p>Parameters to use for the gradient pass when an implicit gradient is computed.
If <code>None</code>, a duplicate of this <code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code> is created for the gradient solve.</p>
<p>In any case, the gradient solve information will be stored in <code>gradient_solve.result</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def gradient_solve(self) -&gt; &#39;Solve[Y, X]&#39;:
    &#34;&#34;&#34;
    Parameters to use for the gradient pass when an implicit gradient is computed.
    If `None`, a duplicate of this `Solve` is created for the gradient solve.

    In any case, the gradient solve information will be stored in `gradient_solve.result`.
    &#34;&#34;&#34;
    if self._gradient_solve is None:
        self._gradient_solve = Solve(self.method, self.relative_tolerance, self.absolute_tolerance, self.max_iterations, None, self.suppress)
    return self._gradient_solve</code></pre>
</details>
</dd>
<dt id="phi.math.Solve.max_iterations"><code class="name">var <span class="ident">max_iterations</span></code></dt>
<dd>
<div class="desc"><p>Maximum number of iterations to perform before raising a <code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code> error is raised.</p></div>
</dd>
<dt id="phi.math.Solve.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p>Optimization method to use. Available solvers depend on the solve function that is used to perform the solve.</p></div>
</dd>
<dt id="phi.math.Solve.relative_tolerance"><code class="name">var <span class="ident">relative_tolerance</span></code></dt>
<dd>
<div class="desc"><p>Relative tolerance for linear solves only. This must be <code>0</code> for minimization problems.
For systems of equations <em>f(x)=y</em>, the final tolerance is <code>max(relative_tolerance * norm(y), absolute_tolerance)</code>.</p></div>
</dd>
<dt id="phi.math.Solve.suppress"><code class="name">var <span class="ident">suppress</span></code></dt>
<dd>
<div class="desc"><p>Error types to suppress; <code>tuple</code> of <code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code> types. For these errors, the solve function will instead return the partial result without raising the error.</p></div>
</dd>
<dt id="phi.math.Solve.x0"><code class="name">var <span class="ident">x0</span></code></dt>
<dd>
<div class="desc"><p>Initial guess for the method, of same type and dimensionality as the solve result.
This property must be set to a value compatible with the solution <code>x</code> before running a method.</p></div>
</dd>
</dl>
</dd>
<dt id="phi.math.SolveInfo"><code class="flex name class">
<span>class <span class="ident">SolveInfo</span></span>
</code></dt>
<dd>
<div class="desc"><p>Stores information about the solution or trajectory of a solve.</p>
<p>When representing the full optimization trajectory, all tracked quantities will have an additional <code>trajectory</code> batch dimension.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveInfo(Generic[X, Y]):
    &#34;&#34;&#34;
    Stores information about the solution or trajectory of a solve.

    When representing the full optimization trajectory, all tracked quantities will have an additional `trajectory` batch dimension.
    &#34;&#34;&#34;

    def __init__(self,
                 solve: Solve,
                 x: X,
                 residual: Y or None,
                 iterations: Tensor or None,
                 function_evaluations: Tensor or None,
                 converged: Tensor,
                 diverged: Tensor,
                 method: str,
                 msg: str = None):
        # tuple.__new__(SolveInfo, (x, residual, iterations, function_evaluations, converged, diverged))
        self.solve: Solve[X, Y] = solve
        &#34;&#34;&#34; `Solve`, Parameters specified for the solve. &#34;&#34;&#34;
        self.x: X = x
        &#34;&#34;&#34; `Tensor` or `TensorLike`, solution estimate. &#34;&#34;&#34;
        self.residual: Y = residual
        &#34;&#34;&#34; `Tensor` or `TensorLike`, residual vector for systems of equations or function value for minimization problems. &#34;&#34;&#34;
        self.iterations: Tensor = iterations
        &#34;&#34;&#34; `Tensor`, number of performed iterations to reach this state. &#34;&#34;&#34;
        self.function_evaluations: Tensor = function_evaluations
        &#34;&#34;&#34; `Tensor`, how often the function (or its gradient function) was called. &#34;&#34;&#34;
        self.converged: Tensor = converged
        &#34;&#34;&#34; `Tensor`, whether the residual is within the specified tolerance. &#34;&#34;&#34;
        self.diverged: Tensor = diverged
        &#34;&#34;&#34; `Tensor`, whether the solve has diverged at this point. &#34;&#34;&#34;
        self.method = method
        &#34;&#34;&#34; `str`, which method and implementation that was used. &#34;&#34;&#34;
        if not msg:
            if self.diverged.any:
                msg = f&#34;Solve diverged within {iterations if iterations is not None else &#39;?&#39;} iterations using {method}.&#34;
            elif not self.converged.trajectory[-1].all:
                msg = f&#34;Solve did not converge to rel={solve.relative_tolerance}, abs={solve.absolute_tolerance} within {solve.max_iterations} iterations using {method}.&#34;
            else:
                msg = f&#34;Converged within {iterations if iterations is not None else &#39;?&#39;} iterations.&#34;
        self.msg = msg
        &#34;&#34;&#34; `str`, termination message &#34;&#34;&#34;

    def __repr__(self):
        return self.msg

    def snapshot(self, index):
        return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index], self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg)

    def convergence_check(self, only_warn: bool):
        if self.diverged.any:
            if Diverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg)
                else:
                    raise Diverged(self)
        if not self.converged.trajectory[-1].all:
            if NotConverged not in self.solve.suppress:
                if only_warn:
                    warnings.warn(self.msg)
                else:
                    raise NotConverged(self)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>typing.Generic</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.SolveInfo.converged"><code class="name">var <span class="ident">converged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, whether the residual is within the specified tolerance.</p></div>
</dd>
<dt id="phi.math.SolveInfo.diverged"><code class="name">var <span class="ident">diverged</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, whether the solve has diverged at this point.</p></div>
</dd>
<dt id="phi.math.SolveInfo.function_evaluations"><code class="name">var <span class="ident">function_evaluations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, how often the function (or its gradient function) was called.</p></div>
</dd>
<dt id="phi.math.SolveInfo.iterations"><code class="name">var <span class="ident">iterations</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>, number of performed iterations to reach this state.</p></div>
</dd>
<dt id="phi.math.SolveInfo.method"><code class="name">var <span class="ident">method</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, which method and implementation that was used.</p></div>
</dd>
<dt id="phi.math.SolveInfo.msg"><code class="name">var <span class="ident">msg</span></code></dt>
<dd>
<div class="desc"><p><code>str</code>, termination message</p></div>
</dd>
<dt id="phi.math.SolveInfo.residual"><code class="name">var <span class="ident">residual</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>, residual vector for systems of equations or function value for minimization problems.</p></div>
</dd>
<dt id="phi.math.SolveInfo.solve"><code class="name">var <span class="ident">solve</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code>, Parameters specified for the solve.</p></div>
</dd>
<dt id="phi.math.SolveInfo.x"><code class="name">var <span class="ident">x</span></code></dt>
<dd>
<div class="desc"><p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code>, solution estimate.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.SolveInfo.convergence_check"><code class="name flex">
<span>def <span class="ident">convergence_check</span></span>(<span>self, only_warn: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convergence_check(self, only_warn: bool):
    if self.diverged.any:
        if Diverged not in self.solve.suppress:
            if only_warn:
                warnings.warn(self.msg)
            else:
                raise Diverged(self)
    if not self.converged.trajectory[-1].all:
        if NotConverged not in self.solve.suppress:
            if only_warn:
                warnings.warn(self.msg)
            else:
                raise NotConverged(self)</code></pre>
</details>
</dd>
<dt id="phi.math.SolveInfo.snapshot"><code class="name flex">
<span>def <span class="ident">snapshot</span></span>(<span>self, index)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def snapshot(self, index):
    return SolveInfo(self.solve, self.x.trajectory[index], self.residual.trajectory[index], self.iterations.trajectory[index], self.function_evaluations.trajectory[index], self.converged.trajectory[index], self.diverged.trajectory[index], self.method, self.msg)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.SolveTape"><code class="flex name class">
<span>class <span class="ident">SolveTape</span></span>
<span>(</span><span>record_trajectories=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Used to record additional information about solves invoked via <code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear()</a></code>, <code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear()</a></code> or <code><a title="phi.math.minimize" href="#phi.math.minimize">minimize()</a></code>.
While a <code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code> is active, certain performance optimizations and algorithm implementations may be disabled.</p>
<p>To access a <code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> of a recorded solve, use</p>
<pre><code class="language-python">solve = Solve(method, ...)
with SolveTape() as solves:
    x = math.solve_linear(f, y, solve)
result: SolveInfo = solves[solve]  # get by Solve
result: SolveInfo = solves[0]  # get by index
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>record_trajectories</code></strong></dt>
<dd>When enabled, the entries of <code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code> will contain an additional batch dimension named <code>trajectory</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolveTape:

    def __init__(self, record_trajectories=False):
        &#34;&#34;&#34;
        Used to record additional information about solves invoked via `solve_linear()`, `solve_nonlinear()` or `minimize()`.
        While a `SolveTape` is active, certain performance optimizations and algorithm implementations may be disabled.

        To access a `SolveInfo` of a recorded solve, use
        ```python
        solve = Solve(method, ...)
        with SolveTape() as solves:
            x = math.solve_linear(f, y, solve)
        result: SolveInfo = solves[solve]  # get by Solve
        result: SolveInfo = solves[0]  # get by index
        ```

        Args:
            record_trajectories: When enabled, the entries of `SolveInfo` will contain an additional batch dimension named `trajectory`.
        &#34;&#34;&#34;
        self.record_trajectories = record_trajectories
        self.solves: List[SolveInfo] = []
        self.solve_ids: List[str] = []

    def __enter__(self):
        _SOLVE_TAPES.append(self)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        _SOLVE_TAPES.remove(self)

    def _add(self, solve: Solve, trj: bool, result: SolveInfo):
        if any(s.solve.id == solve.id for s in self.solves):
            warnings.warn(&#34;SolveTape contains two results for the same solve settings. SolveTape[solve] will return the first solve result.&#34;)
        if self.record_trajectories:
            assert trj, &#34;Solve did not record a trajectory.&#34;
            self.solves.append(result)
        elif trj:
            self.solves.append(result.snapshot(-1))
        else:
            self.solves.append(result)
        self.solve_ids.append(solve.id)

    def __getitem__(self, item) -&gt; SolveInfo:
        if isinstance(item, int):
            return self.solves[item]
        else:
            assert isinstance(item, Solve)
            solves = [s for s in self.solves if s.solve.id == item.id]
            if len(solves) == 0:
                raise KeyError(f&#34;No solve recorded with key &#39;{item}&#39;.&#34;)
            assert len(solves) == 1
            return solves[0]

    def __iter__(self):
        return iter(self.solves)

    def __len__(self):
        return len(self.solves)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class to represent structured data of one data type.</p>
<p>Unlike with <code>numpy.ndarray</code>, the dimensions of Tensors have names and types.
Additionally, tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.</p>
<p>To check whether a value is a tensor, use <code>isinstance(value, <a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a>)</code>.</p>
<p>To construct a Tensor, use <code><a title="phi.math.tensor" href="#phi.math.tensor">tensor()</a></code>, <code><a title="phi.math.wrap" href="#phi.math.wrap">wrap()</a></code> or one of the basic tensor creation functions,
see <a href="https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation">https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation</a> .</p>
<p>Tensors are not editable.
When backed by an editable native tensor, e.g. a <code>numpy.ndarray</code>, do not edit the underlying data structure.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tensor:
    &#34;&#34;&#34;
    Abstract base class to represent structured data of one data type.

    Unlike with `numpy.ndarray`, the dimensions of Tensors have names and types.
    Additionally, tensors can have non-uniform shapes, meaning that the size of dimensions can vary along other dimensions.

    To check whether a value is a tensor, use `isinstance(value, Tensor)`.

    To construct a Tensor, use `phi.math.tensor()`, `phi.math.wrap()` or one of the basic tensor creation functions,
    see https://tum-pbs.github.io/PhiFlow/Math.html#tensor-creation .

    Tensors are not editable.
    When backed by an editable native tensor, e.g. a `numpy.ndarray`, do not edit the underlying data structure.
    &#34;&#34;&#34;

    def native(self, order: str or tuple or list = None):
        &#34;&#34;&#34;
        Returns a native tensor object with the dimensions ordered according to `order`.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        Args:
            order: (Optional) list of dimension names. If not given, the current dimension order is kept.

        Returns:
            Native tensor representation

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        raise NotImplementedError()

    def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
        &#34;&#34;&#34;
        Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
        
        *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
        To get a differentiable tensor, use `Tensor.native()` instead.
        
        Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
        If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

        If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.

        See Also:
            `phi.math.numpy()`

        Args:
            order: (Optional) list of dimension names. If not given, the current dimension order is kept.

        Returns:
            NumPy representation

        Raises:
            ValueError if the tensor cannot be transposed to match target_shape
        &#34;&#34;&#34;
        native = self.native(order=order)
        return choose_backend(native).numpy(native)

    @property
    def dtype(self) -&gt; DType:
        &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def shape(self) -&gt; Shape:
        &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def default_backend(self):
        from ._ops import choose_backend_t
        return choose_backend_t(self)

    def _with_shape_replaced(self, new_shape):
        raise NotImplementedError()

    def _with_natives_replaced(self, natives: list):
        &#34;&#34;&#34; Replaces all n _natives() of this Tensor with the first n elements of the list and removes them from the list. &#34;&#34;&#34;
        raise NotImplementedError()

    @property
    def rank(self) -&gt; int:
        &#34;&#34;&#34; Equal to `tensor.shape.rank`. &#34;&#34;&#34;
        return self.shape.rank

    @property
    def _is_special(self) -&gt; bool:
        &#34;&#34;&#34;
        Special tensors store additional internal information.
        They should not be converted to native() in intermediate operations.
        
        Tracking tensors are special tensors.
        
        TensorStack prevents performing the actual stack operation if one of its component tensors is special.

        Args:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def __len__(self):
        return self.shape.volume if self.rank == 1 else NotImplemented

    def __bool__(self):
        from ._ops import all_
        if not self.default_backend.supports(Backend.jit_compile):  # NumPy
            return bool(self.native()) if self.rank == 0 else bool(all_(self).native())
        else:
            # __bool__ does not work with TensorFlow tracing.
            # TensorFlow needs to see a tf.Tensor in loop conditions but won&#39;t allow bool() invocations.
            # However, this function must always return a Python bool.
            raise AssertionError(&#34;To evaluate the boolean value of a Tensor, use &#39;Tensor.all&#39;.&#34;)

    @property
    def all(self):
        from ._ops import all_available, all_, cast
        if all_available(self):
            if self.rank == 0:
                return bool(self.native())
            else:
                return bool(all_(self).native())
        else:
            if self.rank == 0:
                return cast(self, DType(bool)).native()
            else:
                return all_(self).native()

    @property
    def any(self):
        from ._ops import all_available, any_, cast
        if all_available(self):
            if self.rank == 0:
                return bool(self.native())
            else:
                return bool(any_(self).native())
        else:
            if self.rank == 0:
                return cast(self, DType(bool)).native()
            else:
                return any_(self).native()

    @property
    def mean(self):
        from ._ops import mean
        return mean(self).native()

    @property
    def sum(self):
        from ._ops import sum_
        return sum_(self).native()

    def __int__(self):
        return int(self.native()) if self.shape.volume == 1 else NotImplemented

    def __float__(self):
        return float(self.native()) if self.shape.volume == 1 else NotImplemented

    def __complex__(self):
        return complex(self.native()) if self.shape.volume == 1 else NotImplemented

    def __index__(self):
        assert self.shape.volume == 1, f&#34;Only scalar tensors can be converted to index but has shape {self.shape}&#34;
        assert self.dtype.kind == int, f&#34;Only int tensors can be converted to index but dtype is {self.dtype}&#34;
        return int(self.native())

    def _summary_str(self) -&gt; str:
        try:
            from ._ops import all_available, min_, max_, sum_
            if all_available(self):
                if self.rank == 0:
                    return str(self.numpy())
                elif self.shape.volume is not None and self.shape.volume &lt;= 6:
                    content = list(np.reshape(self.numpy(), [-1]))
                    content = &#39;, &#39;.join([repr(number) for number in content])
                    if self.shape.rank == 1 and (self.dtype.kind in (bool, int) or self.dtype.precision == get_precision()):
                        if self.shape.name == &#39;vector&#39;:
                            return f&#34;({content})&#34;
                        return f&#34;({content}) along {self.shape.name}&#34;
                    return f&#34;{self.shape} {self.dtype}  {content}&#34;
                else:
                    if self.dtype.kind in (float, int):
                        min_val, max_val = min_(self), max_(self)
                        return f&#34;{self.shape} {self.dtype}  {min_val} &lt; ... &lt; {max_val}&#34;
                    elif self.dtype.kind == complex:
                        max_val = max_(abs(self))
                        return f&#34;{self.shape} {self.dtype} |...| &lt; {max_val}&#34;
                    elif self.dtype.kind == bool:
                        return f&#34;{self.shape} {sum_(self)} / {self.shape.volume} True&#34;
                    else:
                        return f&#34;{self.shape} {self.dtype}&#34;
            else:
                if self.rank == 0:
                    return f&#34;{self.default_backend} scalar {self.dtype}&#34;
                else:
                    return f&#34;{self.default_backend} {self.shape} {self.dtype}&#34;
        except BaseException as err:
            return f&#34;{self.shape}, failed to fetch values: {err}&#34;

    def __repr__(self):
        return self._summary_str()

    def __format__(self, format_spec):
        from ._ops import all_available
        if not all_available(self):
            return self._summary_str()
        if self.shape.volume &gt; 1:
            return self._summary_str()
        val = self.numpy()
        return format(val, format_spec)

    def __getitem__(self, item):
        if isinstance(item, Tensor):
            from ._ops import gather
            return gather(self, item)
        if isinstance(item, (int, slice)):
            assert self.rank == 1
            item = {self.shape.names[0]: item}
        if isinstance(item, (tuple, list)):
            if item[0] == Ellipsis:
                assert len(item) - 1 == self.shape.channel.rank
                item = {name: selection for name, selection in zip(self.shape.channel.names, item[1:])}
            elif len(item) == self.shape.channel.rank:
                item = {name: selection for name, selection in zip(self.shape.channel.names, item)}
            elif len(item) == self.shape.rank:  # legacy indexing
                warnings.warn(&#34;Slicing with sequence should only be used for channel dimensions.&#34;)
                item = {name: selection for name, selection in zip(self.shape.names, item)}
        assert isinstance(item, dict)  # dict mapping name -&gt; slice/int
        return self._getitem(item)

    def _getitem(self, selection: dict) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Slice the tensor along specified dimensions.

        Args:
          selection: dim_name: str -&gt; int or slice
          selection: dict: 

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Reverses the order of elements along one or multiple dimensions.

        Args:
            *dims: dimensions to flip

        Returns:
            `Tensor` of the same `Shape`
        &#34;&#34;&#34;
        raise NotImplementedError()

    # def __setitem__(self, key, value):
    #     &#34;&#34;&#34;
    #     All tensors are editable.
    #
    #     :param key: list/tuple of slices / indices
    #     :param value:
    #     :return:
    #     &#34;&#34;&#34;
    #     raise NotImplementedError()

    def unstack(self, dimension: str):
        &#34;&#34;&#34;
        Splits this tensor along the specified dimension.
        The returned tensors have the same dimensions as this tensor save the unstacked dimension.

        Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

        See Also:
            `TensorDim.unstack()`

        Args:
          dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

        Returns:
          tuple of tensors

        &#34;&#34;&#34;
        raise NotImplementedError()

    def dimension(self, name) -&gt; &#39;TensorDim&#39;:
        &#34;&#34;&#34;
        Returns a reference to a specific dimension of this tensor.
        This is equivalent to the syntax `tensor.&lt;name&gt;`.

        The dimension need not be part of the `Tensor.shape` in which case its size is 1.

        Args:
            name: dimension name

        Returns:
            `TensorDim` corresponding to a dimension of this tensor
        &#34;&#34;&#34;
        return TensorDim(self, name)

    def __getattr__(self, name):
        if name.startswith(&#39;_&#39;):
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        if name == &#39;is_tensor_like&#39;:  # TensorFlow replaces abs() while tracing and checks for this attribute
            raise AttributeError(f&#34;&#39;{type(self)}&#39; object has no attribute &#39;{name}&#39;&#34;)
        assert name not in (&#39;shape&#39;, &#39;_shape&#39;, &#39;tensor&#39;), name
        return TensorDim(self, name)

    def __add__(self, other):
        return self._op2(other, lambda x, y: x + y, lambda x, y: choose_backend(x, y).add(x, y))

    def __radd__(self, other):
        return self._op2(other, lambda x, y: y + x, lambda x, y: choose_backend(x, y).add(y, x))

    def __sub__(self, other):
        return self._op2(other, lambda x, y: x - y, lambda x, y: choose_backend(x, y).sub(x, y))

    def __rsub__(self, other):
        return self._op2(other, lambda x, y: y - x, lambda x, y: choose_backend(x, y).sub(y, x))

    def __and__(self, other):
        return self._op2(other, lambda x, y: x &amp; y, lambda x, y: choose_backend(x, y).and_(x, y))

    def __or__(self, other):
        return self._op2(other, lambda x, y: x | y, lambda x, y: choose_backend(x, y).or_(x, y))

    def __xor__(self, other):
        return self._op2(other, lambda x, y: x ^ y, lambda x, y: choose_backend(x, y).xor(x, y))

    def __mul__(self, other):
        return self._op2(other, lambda x, y: x * y, lambda x, y: choose_backend(x, y).mul(x, y))

    def __rmul__(self, other):
        return self._op2(other, lambda x, y: y * x, lambda x, y: choose_backend(x, y).mul(y, x))

    def __truediv__(self, other):
        return self._op2(other, lambda x, y: x / y, lambda x, y: choose_backend(x, y).div(x, y))

    def __rtruediv__(self, other):
        return self._op2(other, lambda x, y: y / x, lambda x, y: choose_backend(x, y).div(y, x))

    def __divmod__(self, other):
        return self._op2(other, lambda x, y: divmod(x, y), lambda x, y: divmod(x, y))

    def __rdivmod__(self, other):
        return self._op2(other, lambda x, y: divmod(y, x), lambda x, y: divmod(y, x))

    def __floordiv__(self, other):
        return self._op2(other, lambda x, y: x // y, lambda x, y: choose_backend(x, y).floordiv(x, y))

    def __rfloordiv__(self, other):
        return self._op2(other, lambda x, y: y // x, lambda x, y: choose_backend(x, y).floordiv(y, x))

    def __pow__(self, power, modulo=None):
        assert modulo is None
        return self._op2(power, lambda x, y: x ** y, lambda x, y: choose_backend(x, y).pow(x, y))

    def __rpow__(self, other):
        return self._op2(other, lambda x, y: y ** x, lambda x, y: choose_backend(x, y).pow(y, x))

    def __mod__(self, other):
        return self._op2(other, lambda x, y: x % y, lambda x, y: choose_backend(x, y).mod(x, y))

    def __rmod__(self, other):
        return self._op2(other, lambda x, y: y % x, lambda x, y: choose_backend(x, y).mod(y, x))

    def __eq__(self, other):
        return self._op2(other, lambda x, y: x == y, lambda x, y: choose_backend(x, y).equal(x, y))

    def __ne__(self, other):
        return self._op2(other, lambda x, y: x != y, lambda x, y: choose_backend(x, y).not_equal(x, y))

    def __lt__(self, other):
        return self._op2(other, lambda x, y: x &lt; y, lambda x, y: choose_backend(x, y).greater_than(y, x))

    def __le__(self, other):
        return self._op2(other, lambda x, y: x &lt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(y, x))

    def __gt__(self, other):
        return self._op2(other, lambda x, y: x &gt; y, lambda x, y: choose_backend(x, y).greater_than(x, y))

    def __ge__(self, other):
        return self._op2(other, lambda x, y: x &gt;= y, lambda x, y: choose_backend(x, y).greater_or_equal(x, y))

    def __abs__(self):
        return self._op1(lambda t: choose_backend(t).abs(t))

    def __round__(self, n=None):
        return self._op1(lambda t: choose_backend(t).round(t))

    def __copy__(self):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=True))

    def __deepcopy__(self, memodict={}):
        return self._op1(lambda t: choose_backend(t).copy(t, only_mutable=False))

    def __neg__(self):
        return self._op1(lambda t: -t)

    def __invert__(self):
        return self._op1(lambda t: ~t)

    def __reversed__(self):
        assert self.shape.channel.rank == 1
        return self[::-1]

    def __iter__(self):
        assert self.rank == 1, f&#34;Can only iterate over 1D tensors but got {self.shape}&#34;
        return iter(self.native())

    def _tensor(self, other):
        return compatible_tensor(other, compat_shape=self.shape, compat_natives=self._natives(), convert=False)

    def _op1(self, native_function):
        &#34;&#34;&#34;
        Transform the values of this tensor given a function that can be applied to any native tensor.

        Args:
          native_function:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _op2(self, other: &#39;Tensor&#39;, operator: Callable, native_function: Callable) -&gt; &#39;Tensor&#39;:
        &#34;&#34;&#34;
        Apply a broadcast operation on two tensors.

        Args:
          other: second argument
          operator: function (Tensor, Tensor) -&gt; Tensor, used to propagate the operation to children tensors to have Python choose the callee
          native_function: function (native tensor, native tensor) -&gt; native tensor
          other: &#39;Tensor&#39;: 
          operator: Callable:
          native_function: Callable:

        Returns:

        &#34;&#34;&#34;
        raise NotImplementedError()

    def _natives(self) -&gt; tuple:
        raise NotImplementedError(self.__class__)

    def _expand(self):
        &#34;&#34;&#34; Expands all compressed tensors to their defined size as if they were being used in `Tensor.native()`. &#34;&#34;&#34;
        raise NotImplementedError(self.__class__)

    def _tensor_reduce(self,
                       dims: Tuple[str],
                       native_function: Callable,
                       collapsed_function: Callable = lambda inner_reduced, collapsed_dims_to_reduce: inner_reduced,
                       unaffected_function: Callable = lambda value: value):
        raise NotImplementedError(self.__class__)

    def _simplify(self):
        return self</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li>phi.math._functional.ShiftLinTracer</li>
<li>phi.math._tensors.CollapsedTensor</li>
<li>phi.math._tensors.NativeTensor</li>
<li>phi.math._tensors.TensorStack</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.Tensor.all"><code class="name">var <span class="ident">all</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def all(self):
    from ._ops import all_available, all_, cast
    if all_available(self):
        if self.rank == 0:
            return bool(self.native())
        else:
            return bool(all_(self).native())
    else:
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return all_(self).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.any"><code class="name">var <span class="ident">any</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def any(self):
    from ._ops import all_available, any_, cast
    if all_available(self):
        if self.rank == 0:
            return bool(self.native())
        else:
            return bool(any_(self).native())
    else:
        if self.rank == 0:
            return cast(self, DType(bool)).native()
        else:
            return any_(self).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.default_backend"><code class="name">var <span class="ident">default_backend</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def default_backend(self):
    from ._ops import choose_backend_t
    return choose_backend_t(self)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.dtype"><code class="name">var <span class="ident">dtype</span> : phi.math.backend._dtype.DType</code></dt>
<dd>
<div class="desc"><p>Data type of the elements of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dtype(self) -&gt; DType:
    &#34;&#34;&#34; Data type of the elements of this `Tensor`. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.mean"><code class="name">var <span class="ident">mean</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean(self):
    from ._ops import mean
    return mean(self).native()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.rank"><code class="name">var <span class="ident">rank</span> : int</code></dt>
<dd>
<div class="desc"><p>Equal to <code>tensor.shape.rank</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rank(self) -&gt; int:
    &#34;&#34;&#34; Equal to `tensor.shape.rank`. &#34;&#34;&#34;
    return self.shape.rank</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.shape"><code class="name">var <span class="ident">shape</span> : phi.math._shape.Shape</code></dt>
<dd>
<div class="desc"><p>The <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> lists the dimensions with their sizes, names and types.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; Shape:
    &#34;&#34;&#34; The `Shape` lists the dimensions with their sizes, names and types. &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.sum"><code class="name">var <span class="ident">sum</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sum(self):
    from ._ops import sum_
    return sum_(self).native()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.Tensor.dimension"><code class="name flex">
<span>def <span class="ident">dimension</span></span>(<span>self, name) ‑> phi.math._tensors.TensorDim</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a reference to a specific dimension of this tensor.
This is equivalent to the syntax <code>tensor.&lt;name&gt;</code>.</p>
<p>The dimension need not be part of the <code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">Tensor.shape</a></code> in which case its size is 1.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>dimension name</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code> corresponding to a dimension of this tensor</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dimension(self, name) -&gt; &#39;TensorDim&#39;:
    &#34;&#34;&#34;
    Returns a reference to a specific dimension of this tensor.
    This is equivalent to the syntax `tensor.&lt;name&gt;`.

    The dimension need not be part of the `Tensor.shape` in which case its size is 1.

    Args:
        name: dimension name

    Returns:
        `TensorDim` corresponding to a dimension of this tensor
    &#34;&#34;&#34;
    return TensorDim(self, name)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self, *dims: str) ‑> phi.math._tensors.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Reverses the order of elements along one or multiple dimensions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*dims</code></strong></dt>
<dd>dimensions to flip</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> of the same <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self, *dims: str) -&gt; &#39;Tensor&#39;:
    &#34;&#34;&#34;
    Reverses the order of elements along one or multiple dimensions.

    Args:
        *dims: dimensions to flip

    Returns:
        `Tensor` of the same `Shape`
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.native"><code class="name flex">
<span>def <span class="ident">native</span></span>(<span>self, order: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a native tensor object with the dimensions ordered according to <code>order</code>.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) list of dimension names. If not given, the current dimension order is kept.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Native tensor representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def native(self, order: str or tuple or list = None):
    &#34;&#34;&#34;
    Returns a native tensor object with the dimensions ordered according to `order`.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    Args:
        order: (Optional) list of dimension names. If not given, the current dimension order is kept.

    Returns:
        Native tensor representation

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.numpy"><code class="name flex">
<span>def <span class="ident">numpy</span></span>(<span>self, order: str = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Converts this tensor to a <code>numpy.ndarray</code> with dimensions ordered according to <code>order</code>.</p>
<p><em>Note</em>: Using this function breaks the autograd chain. The returned tensor is not differentiable.
To get a differentiable tensor, use <code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">Tensor.native()</a></code> instead.</p>
<p>Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
If a dimension of the tensor is not listed in <code>order</code>, a <code>ValueError</code> is raised.</p>
<p>If this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> is backed by a NumPy array, a reference to this array may be returned.</p>
<p>See Also:
<code><a title="phi.math.numpy" href="#phi.math.numpy">numpy()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>order</code></strong></dt>
<dd>(Optional) list of dimension names. If not given, the current dimension order is kept.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NumPy representation</p>
<h2 id="raises">Raises</h2>
<p>ValueError if the tensor cannot be transposed to match target_shape</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def numpy(self, order: str or tuple or list = None) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Converts this tensor to a `numpy.ndarray` with dimensions ordered according to `order`.
    
    *Note*: Using this function breaks the autograd chain. The returned tensor is not differentiable.
    To get a differentiable tensor, use `Tensor.native()` instead.
    
    Transposes the underlying tensor to match the name order and adds singleton dimensions for new dimension names.
    If a dimension of the tensor is not listed in `order`, a `ValueError` is raised.

    If this `Tensor` is backed by a NumPy array, a reference to this array may be returned.

    See Also:
        `phi.math.numpy()`

    Args:
        order: (Optional) list of dimension names. If not given, the current dimension order is kept.

    Returns:
        NumPy representation

    Raises:
        ValueError if the tensor cannot be transposed to match target_shape
    &#34;&#34;&#34;
    native = self.native(order=order)
    return choose_backend(native).numpy(native)</code></pre>
</details>
</dd>
<dt id="phi.math.Tensor.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, dimension: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Splits this tensor along the specified dimension.
The returned tensors have the same dimensions as this tensor save the unstacked dimension.</p>
<p>Raises an error if the dimension is not part of the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of this <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>See Also:
<code><a title="phi.math.TensorDim.unstack" href="#phi.math.TensorDim.unstack">TensorDim.unstack()</a></code></p>
<h2 id="args">Args</h2>
<p>dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension</p>
<h2 id="returns">Returns</h2>
<p>tuple of tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, dimension: str):
    &#34;&#34;&#34;
    Splits this tensor along the specified dimension.
    The returned tensors have the same dimensions as this tensor save the unstacked dimension.

    Raises an error if the dimension is not part of the `Shape` of this `Tensor`.

    See Also:
        `TensorDim.unstack()`

    Args:
      dimension(str or int or TensorDim): name of dimension or Dimension or None for component dimension

    Returns:
      tuple of tensors

    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.TensorDim"><code class="flex name class">
<span>class <span class="ident">TensorDim</span></span>
</code></dt>
<dd>
<div class="desc"><p>Reference to a specific dimension of a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>To obtain a <code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code>, use <code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">Tensor.dimension()</a></code> or the syntax <code>tensor.&lt;dim&gt;</code>.</p>
<p>Indexing a <code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code> as <code>tdim[start:stop:step]</code> returns a sliced <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<p>See the documentation at <a href="https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking">https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking</a> .</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorDim:
    &#34;&#34;&#34;
    Reference to a specific dimension of a `Tensor`.

    To obtain a `TensorDim`, use `Tensor.dimension()` or the syntax `tensor.&lt;dim&gt;`.

    Indexing a `TensorDim` as `tdim[start:stop:step]` returns a sliced `Tensor`.

    See the documentation at https://tum-pbs.github.io/PhiFlow/Math.html#indexing-slicing-unstacking .
    &#34;&#34;&#34;

    def __init__(self, tensor: Tensor, name: str):
        self.tensor = tensor
        self.name = name

    @property
    def exists(self):
        &#34;&#34;&#34; Whether the dimension is listed in the `Shape` of the `Tensor`. &#34;&#34;&#34;
        return self.name in self.tensor.shape

    def __str__(self):
        &#34;&#34;&#34; Dimension name. &#34;&#34;&#34;
        return self.name

    def __repr__(self):
        return f&#34;Dimension &#39;{self.name}&#39; of {self.tensor.shape}&#34;

    def unstack(self, size: int or None = None, to_numpy=False, to_python=False) -&gt; tuple:
        &#34;&#34;&#34;
        See `unstack_spatial()`.

        Args:
            size: (optional)
                None: unstack along this dimension, error if dimension does not exist
                int: repeating unstack if dimension does not exist
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            sliced tensors
        &#34;&#34;&#34;
        if size is None:
            result = self.tensor.unstack(self.name)
        else:
            if self.exists:
                unstacked = self.tensor.unstack(self.name)
                assert len(unstacked) == size, f&#34;Size of dimension {self.name} does not match {size}.&#34;
                result = unstacked
            else:
                result = (self.tensor,) * size
        if to_numpy or to_python:
            result = tuple(component.numpy() for component in result)
            if to_python:
                result = tuple(component.tolist() for component in result)
        return result

    def optional_unstack(self, to_numpy=False, to_python=False):
        &#34;&#34;&#34;
        Unstacks the `Tensor` along this dimension if the dimension is listed in the `Shape`.
        Otherwise returns the original `Tensor`.

        Args:
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            `tuple` of sliced tensors or original `Tensor`
        &#34;&#34;&#34;
        if self.exists:
            return self.unstack(to_numpy=to_numpy, to_python=to_python)
        else:
            if to_numpy or to_python:
                result = self.tensor.numpy()
                if to_python:
                    return result.tolist()
                return result
            return self.tensor

    def unstack_spatial(self, components: str or tuple or list, to_numpy=False, to_python=False) -&gt; tuple:
        &#34;&#34;&#34;
        Slices the tensor along this dimension, returning only the selected components in the specified order.

        Args:
            components:
            to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
            to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

        Returns:
            selected components
        &#34;&#34;&#34;
        if isinstance(components, str):
            components = parse_dim_order(components)
        if self.exists:
            spatial = self.tensor.shape.spatial
            result = []
            if spatial.is_empty:
                spatial = [GLOBAL_AXIS_ORDER.axis_name(i, len(components)) for i in range(len(components))]
            for dim in components:
                component_index = spatial.index(dim)
                result.append(self.tensor[{self.name: component_index}])
        else:
            result = [self.tensor] * len(components)
        if to_numpy or to_python:
            result = tuple(component.numpy() for component in result)
            if to_python:
                result = tuple(component.tolist() for component in result)
        return tuple(result)

    @property
    def index(self):
        &#34;&#34;&#34; The index of this dimension in the `Shape` of the `Tensor`. &#34;&#34;&#34;
        return self.tensor.shape.index(self.name)

    def __int__(self):
        return self.index

    def __len__(self):
        assert self.name in self.tensor.shape, f&#34;Dimension {self.name} does not exist for tensor {self.tensor.shape}&#34;
        return self.tensor.shape.get_size(self.name)

    @property
    def size(self):
        &#34;&#34;&#34; Length of this tensor dimension as listed in the `Shape`, otherwise `1`. &#34;&#34;&#34;
        if self.exists:
            return self.tensor.shape.get_size(self.name)
        else:
            return 1

    def as_batch(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. &#34;&#34;&#34;
        return self._as(BATCH_DIM, name)

    def as_spatial(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. &#34;&#34;&#34;
        return self._as(SPATIAL_DIM, name)

    def as_channel(self, name: str or None = None):
        &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. &#34;&#34;&#34;
        return self._as(CHANNEL_DIM, name)

    def _as(self, dim_type: int, name: str or None):
        shape = self.tensor.shape
        new_types = list(shape.types)
        new_types[self.index] = dim_type
        new_names = shape.names
        if name is not None:
            new_names = list(new_names)
            new_names[self.index] = name
        new_shape = Shape(shape.sizes, new_names, new_types)
        return self.tensor._with_shape_replaced(new_shape)

    @property
    def _dim_type(self):
        return self.tensor.shape.get_type(self.name)

    @property
    def is_spatial(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *spatial*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == SPATIAL_DIM

    @property
    def is_batch(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *batch*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == BATCH_DIM

    @property
    def is_channel(self):
        &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *channel*. Only defined for existing dimensions. &#34;&#34;&#34;
        return self._dim_type == CHANNEL_DIM

    def __getitem__(self, item):
        if isinstance(item, str):
            item = self.tensor.shape.spatial.index(item)
        elif isinstance(item, Tensor) and item.dtype == DType(bool):
            from ._ops import boolean_mask
            return boolean_mask(self.tensor, self.name, item)
        return self.tensor[{self.name: item}]

    def flip(self):
        &#34;&#34;&#34; Flips the element order along this dimension and returns the result as a `Tensor`. &#34;&#34;&#34;
        return self.tensor.flip(self.name)

    def split(self, split_dimensions: Shape):
        &#34;&#34;&#34; See `phi.math.split_dimension()` &#34;&#34;&#34;
        from ._ops import split_dimension
        return split_dimension(self.tensor, self.name, split_dimensions)

    def __mul__(self, other):
        if isinstance(other, TensorDim):
            from ._ops import dot
            return dot(self.tensor, (self.name,), other.tensor, (other.name,))
        else:
            return NotImplemented

    def __call__(self, *args, **kwargs):
        raise TypeError(f&#34;Method Tensor.{self.name}() does not exist.&#34;)</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="phi.math.TensorDim.exists"><code class="name">var <span class="ident">exists</span></code></dt>
<dd>
<div class="desc"><p>Whether the dimension is listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def exists(self):
    &#34;&#34;&#34; Whether the dimension is listed in the `Shape` of the `Tensor`. &#34;&#34;&#34;
    return self.name in self.tensor.shape</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.index"><code class="name">var <span class="ident">index</span></code></dt>
<dd>
<div class="desc"><p>The index of this dimension in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def index(self):
    &#34;&#34;&#34; The index of this dimension in the `Shape` of the `Tensor`. &#34;&#34;&#34;
    return self.tensor.shape.index(self.name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_batch"><code class="name">var <span class="ident">is_batch</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>batch</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_batch(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *batch*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == BATCH_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_channel"><code class="name">var <span class="ident">is_channel</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>channel</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_channel(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *channel*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == CHANNEL_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.is_spatial"><code class="name">var <span class="ident">is_spatial</span></code></dt>
<dd>
<div class="desc"><p>Whether the type of this dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code> is <em>spatial</em>. Only defined for existing dimensions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_spatial(self):
    &#34;&#34;&#34; Whether the type of this dimension as listed in the `Shape` is *spatial*. Only defined for existing dimensions. &#34;&#34;&#34;
    return self._dim_type == SPATIAL_DIM</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.size"><code class="name">var <span class="ident">size</span></code></dt>
<dd>
<div class="desc"><p>Length of this tensor dimension as listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>, otherwise <code>1</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def size(self):
    &#34;&#34;&#34; Length of this tensor dimension as listed in the `Shape`, otherwise `1`. &#34;&#34;&#34;
    if self.exists:
        return self.tensor.shape.get_size(self.name)
    else:
        return 1</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="phi.math.TensorDim.as_batch"><code class="name flex">
<span>def <span class="ident">as_batch</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>batch</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_batch(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *batch*. &#34;&#34;&#34;
    return self._as(BATCH_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.as_channel"><code class="name flex">
<span>def <span class="ident">as_channel</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>channel</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_channel(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *channel*. &#34;&#34;&#34;
    return self._as(CHANNEL_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.as_spatial"><code class="name flex">
<span>def <span class="ident">as_spatial</span></span>(<span>self, name: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a shallow copy of the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> where the type of this dimension is <em>spatial</em>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def as_spatial(self, name: str or None = None):
    &#34;&#34;&#34; Returns a shallow copy of the `Tensor` where the type of this dimension is *spatial*. &#34;&#34;&#34;
    return self._as(SPATIAL_DIM, name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.flip"><code class="name flex">
<span>def <span class="ident">flip</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Flips the element order along this dimension and returns the result as a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flip(self):
    &#34;&#34;&#34; Flips the element order along this dimension and returns the result as a `Tensor`. &#34;&#34;&#34;
    return self.tensor.flip(self.name)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.optional_unstack"><code class="name flex">
<span>def <span class="ident">optional_unstack</span></span>(<span>self, to_numpy=False, to_python=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Unstacks the <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> along this dimension if the dimension is listed in the <code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code>.
Otherwise returns the original <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of sliced tensors or original <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optional_unstack(self, to_numpy=False, to_python=False):
    &#34;&#34;&#34;
    Unstacks the `Tensor` along this dimension if the dimension is listed in the `Shape`.
    Otherwise returns the original `Tensor`.

    Args:
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        `tuple` of sliced tensors or original `Tensor`
    &#34;&#34;&#34;
    if self.exists:
        return self.unstack(to_numpy=to_numpy, to_python=to_python)
    else:
        if to_numpy or to_python:
            result = self.tensor.numpy()
            if to_python:
                return result.tolist()
            return result
        return self.tensor</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.split"><code class="name flex">
<span>def <span class="ident">split</span></span>(<span>self, split_dimensions: phi.math._shape.Shape)</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension()</a></code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def split(self, split_dimensions: Shape):
    &#34;&#34;&#34; See `phi.math.split_dimension()` &#34;&#34;&#34;
    from ._ops import split_dimension
    return split_dimension(self.tensor, self.name, split_dimensions)</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.unstack"><code class="name flex">
<span>def <span class="ident">unstack</span></span>(<span>self, size: int = None, to_numpy=False, to_python=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>See <code>unstack_spatial()</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong></dt>
<dd>(optional)
None: unstack along this dimension, error if dimension does not exist
int: repeating unstack if dimension does not exist</dd>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>sliced tensors</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack(self, size: int or None = None, to_numpy=False, to_python=False) -&gt; tuple:
    &#34;&#34;&#34;
    See `unstack_spatial()`.

    Args:
        size: (optional)
            None: unstack along this dimension, error if dimension does not exist
            int: repeating unstack if dimension does not exist
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        sliced tensors
    &#34;&#34;&#34;
    if size is None:
        result = self.tensor.unstack(self.name)
    else:
        if self.exists:
            unstacked = self.tensor.unstack(self.name)
            assert len(unstacked) == size, f&#34;Size of dimension {self.name} does not match {size}.&#34;
            result = unstacked
        else:
            result = (self.tensor,) * size
    if to_numpy or to_python:
        result = tuple(component.numpy() for component in result)
        if to_python:
            result = tuple(component.tolist() for component in result)
    return result</code></pre>
</details>
</dd>
<dt id="phi.math.TensorDim.unstack_spatial"><code class="name flex">
<span>def <span class="ident">unstack_spatial</span></span>(<span>self, components: str, to_numpy=False, to_python=False) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Slices the tensor along this dimension, returning only the selected components in the specified order.</p>
<h2 id="args">Args</h2>
<dl>
<dt>components:</dt>
<dt><strong><code>to_numpy</code></strong></dt>
<dd>Whether to convert the selected data to <code>numpy.ndarray</code> objects.</dd>
<dt><strong><code>to_python</code></strong></dt>
<dd>Whether to convert the selected data to Python types, i.e. <code>int, float, complex, bool, tuple, list</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>selected components</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def unstack_spatial(self, components: str or tuple or list, to_numpy=False, to_python=False) -&gt; tuple:
    &#34;&#34;&#34;
    Slices the tensor along this dimension, returning only the selected components in the specified order.

    Args:
        components:
        to_numpy: Whether to convert the selected data to `numpy.ndarray` objects.
        to_python: Whether to convert the selected data to Python types, i.e. `int, float, complex, bool, tuple, list`.

    Returns:
        selected components
    &#34;&#34;&#34;
    if isinstance(components, str):
        components = parse_dim_order(components)
    if self.exists:
        spatial = self.tensor.shape.spatial
        result = []
        if spatial.is_empty:
            spatial = [GLOBAL_AXIS_ORDER.axis_name(i, len(components)) for i in range(len(components))]
        for dim in components:
            component_index = spatial.index(dim)
            result.append(self.tensor[{self.name: component_index}])
    else:
        result = [self.tensor] * len(components)
    if to_numpy or to_python:
        result = tuple(component.numpy() for component in result)
        if to_python:
            result = tuple(component.tolist() for component in result)
    return tuple(result)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="phi.math.TensorLike"><code class="flex name class">
<span>class <span class="ident">TensorLike</span></span>
</code></dt>
<dd>
<div class="desc"><p>Tensor-like objects can interoperate with some <code><a title="phi.math" href="#phi.math">phi.math</a></code> functions, depending on what methods they implement.
Objects are considered <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> if they implement <code><a title="phi.math.TensorLike.__variable_attrs__" href="#phi.math.TensorLike.__variable_attrs__">TensorLike.__variable_attrs__()</a></code> or <code><a title="phi.math.TensorLike.__value_attrs__" href="#phi.math.TensorLike.__value_attrs__">TensorLike.__value_attrs__()</a></code>.
This is reflected in <code>isinstance</code> checks.
Do not declare this class as a superclass.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorLike(metaclass=_TensorLikeType):
    &#34;&#34;&#34;
    Tensor-like objects can interoperate with some `phi.math` functions, depending on what methods they implement.
    Objects are considered `TensorLike` if they implement `TensorLike.__variable_attrs__()` or `TensorLike.__value_attrs__()`.
    This is reflected in `isinstance` checks.
    Do not declare this class as a superclass.
    &#34;&#34;&#34;

    def __value_attrs__(self) -&gt; Tuple[str]:
        &#34;&#34;&#34;
        Returns all `Tensor` or `TensorLike` attribute names of `self` that should be transformed by single-operand math operations,
        such as `sin()`, `exp()`.

        Returns:
            `tuple` of `str` attributes.
                Calling `getattr(self, attr)` must return a `Tensor` or `TensorLike` for all returned attributes.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def __variable_attrs__(self) -&gt; Tuple[str]:
        &#34;&#34;&#34;
        Returns all `Tensor` or `TensorLike` attribute names of `self` whose values are variable.
        Variables denote values that can change from one function call to the next or for which gradients can be recorded.
        If this method is not implemented, all attributes returned by `__value_attrs__()` are considered variable.

        The returned properties are used by the following functions:

        - `jit_compile()`
        - `jit_compile_linear()`
        - `stop_gradient()`
        - `functional_gradient()`
        - `custom_gradient()`

        Returns:
            `tuple` of `str` attributes.
                Calling `getattr(self, attr)` must return a `Tensor` or `TensorLike` for all returned attributes.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def __with_attrs__(self, **attrs):
        &#34;&#34;&#34;
        Creates a copy of this object which has the `Tensor` or `TensorLike` attributes contained in `tattrs` replaced.
        If this method is not implemented, tensor attributes are replaced using `setattr()`.

        Args:
            **attrs: `dict` mapping `str` attribute names to `Tensor` or `TensorLike`.

        Returns:
            Altered copy of `self`
        &#34;&#34;&#34;
        raise NotImplementedError()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="phi.math.TensorLike.__value_attrs__"><code class="name flex">
<span>def <span class="ident">__value_attrs__</span></span>(<span>self) ‑> Tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns all <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> attribute names of <code>self</code> that should be transformed by single-operand math operations,
such as <code><a title="phi.math.sin" href="#phi.math.sin">sin()</a></code>, <code><a title="phi.math.exp" href="#phi.math.exp">exp()</a></code>.</p>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of <code>str</code> attributes.
Calling <code>getattr(self, attr)</code> must return a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> for all returned attributes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __value_attrs__(self) -&gt; Tuple[str]:
    &#34;&#34;&#34;
    Returns all `Tensor` or `TensorLike` attribute names of `self` that should be transformed by single-operand math operations,
    such as `sin()`, `exp()`.

    Returns:
        `tuple` of `str` attributes.
            Calling `getattr(self, attr)` must return a `Tensor` or `TensorLike` for all returned attributes.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="phi.math.TensorLike.__variable_attrs__"><code class="name flex">
<span>def <span class="ident">__variable_attrs__</span></span>(<span>self) ‑> Tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns all <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> attribute names of <code>self</code> whose values are variable.
Variables denote values that can change from one function call to the next or for which gradients can be recorded.
If this method is not implemented, all attributes returned by <code>__value_attrs__()</code> are considered variable.</p>
<p>The returned properties are used by the following functions:</p>
<ul>
<li><code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile()</a></code></li>
<li><code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear()</a></code></li>
<li><code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient()</a></code></li>
<li><code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient()</a></code></li>
<li><code><a title="phi.math.custom_gradient" href="#phi.math.custom_gradient">custom_gradient()</a></code></li>
</ul>
<h2 id="returns">Returns</h2>
<p><code>tuple</code> of <code>str</code> attributes.
Calling <code>getattr(self, attr)</code> must return a <code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code> or <code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code> for all returned attributes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __variable_attrs__(self) -&gt; Tuple[str]:
    &#34;&#34;&#34;
    Returns all `Tensor` or `TensorLike` attribute names of `self` whose values are variable.
    Variables denote values that can change from one function call to the next or for which gradients can be recorded.
    If this method is not implemented, all attributes returned by `__value_attrs__()` are considered variable.

    The returned properties are used by the following functions:

    - `jit_compile()`
    - `jit_compile_linear()`
    - `stop_gradient()`
    - `functional_gradient()`
    - `custom_gradient()`

    Returns:
        `tuple` of `str` attributes.
            Calling `getattr(self, attr)` must return a `Tensor` or `TensorLike` for all returned attributes.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi" href="../index.html">phi</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phi.math.backend" href="backend/index.html">phi.math.backend</a></code></li>
<li><code><a title="phi.math.extrapolation" href="extrapolation.html">phi.math.extrapolation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="phi.math.NUMPY_BACKEND" href="#phi.math.NUMPY_BACKEND">NUMPY_BACKEND</a></code></li>
<li><code><a title="phi.math.PI" href="#phi.math.PI">PI</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="phi.math.abs" href="#phi.math.abs">abs</a></code></li>
<li><code><a title="phi.math.abs_square" href="#phi.math.abs_square">abs_square</a></code></li>
<li><code><a title="phi.math.all" href="#phi.math.all">all</a></code></li>
<li><code><a title="phi.math.all_available" href="#phi.math.all_available">all_available</a></code></li>
<li><code><a title="phi.math.any" href="#phi.math.any">any</a></code></li>
<li><code><a title="phi.math.assert_close" href="#phi.math.assert_close">assert_close</a></code></li>
<li><code><a title="phi.math.batch_shape" href="#phi.math.batch_shape">batch_shape</a></code></li>
<li><code><a title="phi.math.batch_stack" href="#phi.math.batch_stack">batch_stack</a></code></li>
<li><code><a title="phi.math.boolean_mask" href="#phi.math.boolean_mask">boolean_mask</a></code></li>
<li><code><a title="phi.math.cast" href="#phi.math.cast">cast</a></code></li>
<li><code><a title="phi.math.ceil" href="#phi.math.ceil">ceil</a></code></li>
<li><code><a title="phi.math.channel_shape" href="#phi.math.channel_shape">channel_shape</a></code></li>
<li><code><a title="phi.math.channel_stack" href="#phi.math.channel_stack">channel_stack</a></code></li>
<li><code><a title="phi.math.choose_backend" href="#phi.math.choose_backend">choose_backend</a></code></li>
<li><code><a title="phi.math.clip" href="#phi.math.clip">clip</a></code></li>
<li><code><a title="phi.math.close" href="#phi.math.close">close</a></code></li>
<li><code><a title="phi.math.closest_grid_values" href="#phi.math.closest_grid_values">closest_grid_values</a></code></li>
<li><code><a title="phi.math.concat" href="#phi.math.concat">concat</a></code></li>
<li><code><a title="phi.math.convert" href="#phi.math.convert">convert</a></code></li>
<li><code><a title="phi.math.convolve" href="#phi.math.convolve">convolve</a></code></li>
<li><code><a title="phi.math.copy" href="#phi.math.copy">copy</a></code></li>
<li><code><a title="phi.math.cos" href="#phi.math.cos">cos</a></code></li>
<li><code><a title="phi.math.cross_product" href="#phi.math.cross_product">cross_product</a></code></li>
<li><code><a title="phi.math.custom_gradient" href="#phi.math.custom_gradient">custom_gradient</a></code></li>
<li><code><a title="phi.math.divide_no_nan" href="#phi.math.divide_no_nan">divide_no_nan</a></code></li>
<li><code><a title="phi.math.dot" href="#phi.math.dot">dot</a></code></li>
<li><code><a title="phi.math.downsample2x" href="#phi.math.downsample2x">downsample2x</a></code></li>
<li><code><a title="phi.math.dtype" href="#phi.math.dtype">dtype</a></code></li>
<li><code><a title="phi.math.exp" href="#phi.math.exp">exp</a></code></li>
<li><code><a title="phi.math.expand" href="#phi.math.expand">expand</a></code></li>
<li><code><a title="phi.math.expand_batch" href="#phi.math.expand_batch">expand_batch</a></code></li>
<li><code><a title="phi.math.expand_channel" href="#phi.math.expand_channel">expand_channel</a></code></li>
<li><code><a title="phi.math.expand_spatial" href="#phi.math.expand_spatial">expand_spatial</a></code></li>
<li><code><a title="phi.math.extrapolate_valid_values" href="#phi.math.extrapolate_valid_values">extrapolate_valid_values</a></code></li>
<li><code><a title="phi.math.fft" href="#phi.math.fft">fft</a></code></li>
<li><code><a title="phi.math.fftfreq" href="#phi.math.fftfreq">fftfreq</a></code></li>
<li><code><a title="phi.math.flatten" href="#phi.math.flatten">flatten</a></code></li>
<li><code><a title="phi.math.floor" href="#phi.math.floor">floor</a></code></li>
<li><code><a title="phi.math.fourier_laplace" href="#phi.math.fourier_laplace">fourier_laplace</a></code></li>
<li><code><a title="phi.math.fourier_poisson" href="#phi.math.fourier_poisson">fourier_poisson</a></code></li>
<li><code><a title="phi.math.frequency_loss" href="#phi.math.frequency_loss">frequency_loss</a></code></li>
<li><code><a title="phi.math.functional_gradient" href="#phi.math.functional_gradient">functional_gradient</a></code></li>
<li><code><a title="phi.math.gather" href="#phi.math.gather">gather</a></code></li>
<li><code><a title="phi.math.get_precision" href="#phi.math.get_precision">get_precision</a></code></li>
<li><code><a title="phi.math.gradients" href="#phi.math.gradients">gradients</a></code></li>
<li><code><a title="phi.math.grid_sample" href="#phi.math.grid_sample">grid_sample</a></code></li>
<li><code><a title="phi.math.ifft" href="#phi.math.ifft">ifft</a></code></li>
<li><code><a title="phi.math.imag" href="#phi.math.imag">imag</a></code></li>
<li><code><a title="phi.math.isfinite" href="#phi.math.isfinite">isfinite</a></code></li>
<li><code><a title="phi.math.jit_compile" href="#phi.math.jit_compile">jit_compile</a></code></li>
<li><code><a title="phi.math.jit_compile_linear" href="#phi.math.jit_compile_linear">jit_compile_linear</a></code></li>
<li><code><a title="phi.math.join_dimensions" href="#phi.math.join_dimensions">join_dimensions</a></code></li>
<li><code><a title="phi.math.l1_loss" href="#phi.math.l1_loss">l1_loss</a></code></li>
<li><code><a title="phi.math.l2_loss" href="#phi.math.l2_loss">l2_loss</a></code></li>
<li><code><a title="phi.math.laplace" href="#phi.math.laplace">laplace</a></code></li>
<li><code><a title="phi.math.linspace" href="#phi.math.linspace">linspace</a></code></li>
<li><code><a title="phi.math.log" href="#phi.math.log">log</a></code></li>
<li><code><a title="phi.math.log10" href="#phi.math.log10">log10</a></code></li>
<li><code><a title="phi.math.log2" href="#phi.math.log2">log2</a></code></li>
<li><code><a title="phi.math.map" href="#phi.math.map">map</a></code></li>
<li><code><a title="phi.math.max" href="#phi.math.max">max</a></code></li>
<li><code><a title="phi.math.maximum" href="#phi.math.maximum">maximum</a></code></li>
<li><code><a title="phi.math.mean" href="#phi.math.mean">mean</a></code></li>
<li><code><a title="phi.math.meshgrid" href="#phi.math.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.min" href="#phi.math.min">min</a></code></li>
<li><code><a title="phi.math.minimize" href="#phi.math.minimize">minimize</a></code></li>
<li><code><a title="phi.math.minimum" href="#phi.math.minimum">minimum</a></code></li>
<li><code><a title="phi.math.native" href="#phi.math.native">native</a></code></li>
<li><code><a title="phi.math.native_call" href="#phi.math.native_call">native_call</a></code></li>
<li><code><a title="phi.math.nonzero" href="#phi.math.nonzero">nonzero</a></code></li>
<li><code><a title="phi.math.normalize_to" href="#phi.math.normalize_to">normalize_to</a></code></li>
<li><code><a title="phi.math.numpy" href="#phi.math.numpy">numpy</a></code></li>
<li><code><a title="phi.math.ones" href="#phi.math.ones">ones</a></code></li>
<li><code><a title="phi.math.ones_like" href="#phi.math.ones_like">ones_like</a></code></li>
<li><code><a title="phi.math.pad" href="#phi.math.pad">pad</a></code></li>
<li><code><a title="phi.math.precision" href="#phi.math.precision">precision</a></code></li>
<li><code><a title="phi.math.print" href="#phi.math.print">print</a></code></li>
<li><code><a title="phi.math.print_gradient" href="#phi.math.print_gradient">print_gradient</a></code></li>
<li><code><a title="phi.math.prod" href="#phi.math.prod">prod</a></code></li>
<li><code><a title="phi.math.random_normal" href="#phi.math.random_normal">random_normal</a></code></li>
<li><code><a title="phi.math.random_uniform" href="#phi.math.random_uniform">random_uniform</a></code></li>
<li><code><a title="phi.math.range" href="#phi.math.range">range</a></code></li>
<li><code><a title="phi.math.range_tensor" href="#phi.math.range_tensor">range_tensor</a></code></li>
<li><code><a title="phi.math.real" href="#phi.math.real">real</a></code></li>
<li><code><a title="phi.math.record_gradients" href="#phi.math.record_gradients">record_gradients</a></code></li>
<li><code><a title="phi.math.reshaped_native" href="#phi.math.reshaped_native">reshaped_native</a></code></li>
<li><code><a title="phi.math.reshaped_tensor" href="#phi.math.reshaped_tensor">reshaped_tensor</a></code></li>
<li><code><a title="phi.math.round" href="#phi.math.round">round</a></code></li>
<li><code><a title="phi.math.sample_subgrid" href="#phi.math.sample_subgrid">sample_subgrid</a></code></li>
<li><code><a title="phi.math.scatter" href="#phi.math.scatter">scatter</a></code></li>
<li><code><a title="phi.math.seed" href="#phi.math.seed">seed</a></code></li>
<li><code><a title="phi.math.set_global_precision" href="#phi.math.set_global_precision">set_global_precision</a></code></li>
<li><code><a title="phi.math.shape" href="#phi.math.shape">shape</a></code></li>
<li><code><a title="phi.math.shift" href="#phi.math.shift">shift</a></code></li>
<li><code><a title="phi.math.sign" href="#phi.math.sign">sign</a></code></li>
<li><code><a title="phi.math.sin" href="#phi.math.sin">sin</a></code></li>
<li><code><a title="phi.math.solve_linear" href="#phi.math.solve_linear">solve_linear</a></code></li>
<li><code><a title="phi.math.solve_nonlinear" href="#phi.math.solve_nonlinear">solve_nonlinear</a></code></li>
<li><code><a title="phi.math.spatial_gradient" href="#phi.math.spatial_gradient">spatial_gradient</a></code></li>
<li><code><a title="phi.math.spatial_shape" href="#phi.math.spatial_shape">spatial_shape</a></code></li>
<li><code><a title="phi.math.spatial_stack" href="#phi.math.spatial_stack">spatial_stack</a></code></li>
<li><code><a title="phi.math.spatial_sum" href="#phi.math.spatial_sum">spatial_sum</a></code></li>
<li><code><a title="phi.math.split_dimension" href="#phi.math.split_dimension">split_dimension</a></code></li>
<li><code><a title="phi.math.sqrt" href="#phi.math.sqrt">sqrt</a></code></li>
<li><code><a title="phi.math.std" href="#phi.math.std">std</a></code></li>
<li><code><a title="phi.math.stop_gradient" href="#phi.math.stop_gradient">stop_gradient</a></code></li>
<li><code><a title="phi.math.sum" href="#phi.math.sum">sum</a></code></li>
<li><code><a title="phi.math.tan" href="#phi.math.tan">tan</a></code></li>
<li><code><a title="phi.math.tensor" href="#phi.math.tensor">tensor</a></code></li>
<li><code><a title="phi.math.tensors" href="#phi.math.tensors">tensors</a></code></li>
<li><code><a title="phi.math.to_complex" href="#phi.math.to_complex">to_complex</a></code></li>
<li><code><a title="phi.math.to_float" href="#phi.math.to_float">to_float</a></code></li>
<li><code><a title="phi.math.to_int32" href="#phi.math.to_int32">to_int32</a></code></li>
<li><code><a title="phi.math.to_int64" href="#phi.math.to_int64">to_int64</a></code></li>
<li><code><a title="phi.math.transpose" href="#phi.math.transpose">transpose</a></code></li>
<li><code><a title="phi.math.unstack" href="#phi.math.unstack">unstack</a></code></li>
<li><code><a title="phi.math.upsample2x" href="#phi.math.upsample2x">upsample2x</a></code></li>
<li><code><a title="phi.math.vec_abs" href="#phi.math.vec_abs">vec_abs</a></code></li>
<li><code><a title="phi.math.vec_squared" href="#phi.math.vec_squared">vec_squared</a></code></li>
<li><code><a title="phi.math.where" href="#phi.math.where">where</a></code></li>
<li><code><a title="phi.math.wrap" href="#phi.math.wrap">wrap</a></code></li>
<li><code><a title="phi.math.zeros" href="#phi.math.zeros">zeros</a></code></li>
<li><code><a title="phi.math.zeros_like" href="#phi.math.zeros_like">zeros_like</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phi.math.ConvergenceException" href="#phi.math.ConvergenceException">ConvergenceException</a></code></h4>
<ul class="">
<li><code><a title="phi.math.ConvergenceException.result" href="#phi.math.ConvergenceException.result">result</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.DType" href="#phi.math.DType">DType</a></code></h4>
<ul class="">
<li><code><a title="phi.math.DType.bits" href="#phi.math.DType.bits">bits</a></code></li>
<li><code><a title="phi.math.DType.itemsize" href="#phi.math.DType.itemsize">itemsize</a></code></li>
<li><code><a title="phi.math.DType.kind" href="#phi.math.DType.kind">kind</a></code></li>
<li><code><a title="phi.math.DType.precision" href="#phi.math.DType.precision">precision</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Diverged" href="#phi.math.Diverged">Diverged</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Extrapolation" href="#phi.math.Extrapolation">Extrapolation</a></code></h4>
<ul class="">
<li><code><a title="phi.math.Extrapolation.is_copy_pad" href="#phi.math.Extrapolation.is_copy_pad">is_copy_pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.native_grid_sample_mode" href="#phi.math.Extrapolation.native_grid_sample_mode">native_grid_sample_mode</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad" href="#phi.math.Extrapolation.pad">pad</a></code></li>
<li><code><a title="phi.math.Extrapolation.pad_values" href="#phi.math.Extrapolation.pad_values">pad_values</a></code></li>
<li><code><a title="phi.math.Extrapolation.spatial_gradient" href="#phi.math.Extrapolation.spatial_gradient">spatial_gradient</a></code></li>
<li><code><a title="phi.math.Extrapolation.to_dict" href="#phi.math.Extrapolation.to_dict">to_dict</a></code></li>
<li><code><a title="phi.math.Extrapolation.transform_coordinates" href="#phi.math.Extrapolation.transform_coordinates">transform_coordinates</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.LinearFunction" href="#phi.math.LinearFunction">LinearFunction</a></code></h4>
<ul class="">
<li><code><a title="phi.math.LinearFunction.sparse_coordinate_matrix" href="#phi.math.LinearFunction.sparse_coordinate_matrix">sparse_coordinate_matrix</a></code></li>
<li><code><a title="phi.math.LinearFunction.stencil_inspector" href="#phi.math.LinearFunction.stencil_inspector">stencil_inspector</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.NotConverged" href="#phi.math.NotConverged">NotConverged</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Shape" href="#phi.math.Shape">Shape</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Shape.after_gather" href="#phi.math.Shape.after_gather">after_gather</a></code></li>
<li><code><a title="phi.math.Shape.after_pad" href="#phi.math.Shape.after_pad">after_pad</a></code></li>
<li><code><a title="phi.math.Shape.alphabetically" href="#phi.math.Shape.alphabetically">alphabetically</a></code></li>
<li><code><a title="phi.math.Shape.batch" href="#phi.math.Shape.batch">batch</a></code></li>
<li><code><a title="phi.math.Shape.batch_rank" href="#phi.math.Shape.batch_rank">batch_rank</a></code></li>
<li><code><a title="phi.math.Shape.channel" href="#phi.math.Shape.channel">channel</a></code></li>
<li><code><a title="phi.math.Shape.channel_rank" href="#phi.math.Shape.channel_rank">channel_rank</a></code></li>
<li><code><a title="phi.math.Shape.collection" href="#phi.math.Shape.collection">collection</a></code></li>
<li><code><a title="phi.math.Shape.combined" href="#phi.math.Shape.combined">combined</a></code></li>
<li><code><a title="phi.math.Shape.dimensions" href="#phi.math.Shape.dimensions">dimensions</a></code></li>
<li><code><a title="phi.math.Shape.expand" href="#phi.math.Shape.expand">expand</a></code></li>
<li><code><a title="phi.math.Shape.extend" href="#phi.math.Shape.extend">extend</a></code></li>
<li><code><a title="phi.math.Shape.get_size" href="#phi.math.Shape.get_size">get_size</a></code></li>
<li><code><a title="phi.math.Shape.get_type" href="#phi.math.Shape.get_type">get_type</a></code></li>
<li><code><a title="phi.math.Shape.index" href="#phi.math.Shape.index">index</a></code></li>
<li><code><a title="phi.math.Shape.indices" href="#phi.math.Shape.indices">indices</a></code></li>
<li><code><a title="phi.math.Shape.is_empty" href="#phi.math.Shape.is_empty">is_empty</a></code></li>
<li><code><a title="phi.math.Shape.is_non_uniform" href="#phi.math.Shape.is_non_uniform">is_non_uniform</a></code></li>
<li><code><a title="phi.math.Shape.is_uniform" href="#phi.math.Shape.is_uniform">is_uniform</a></code></li>
<li><code><a title="phi.math.Shape.mask" href="#phi.math.Shape.mask">mask</a></code></li>
<li><code><a title="phi.math.Shape.meshgrid" href="#phi.math.Shape.meshgrid">meshgrid</a></code></li>
<li><code><a title="phi.math.Shape.name" href="#phi.math.Shape.name">name</a></code></li>
<li><code><a title="phi.math.Shape.named_sizes" href="#phi.math.Shape.named_sizes">named_sizes</a></code></li>
<li><code><a title="phi.math.Shape.names" href="#phi.math.Shape.names">names</a></code></li>
<li><code><a title="phi.math.Shape.non_batch" href="#phi.math.Shape.non_batch">non_batch</a></code></li>
<li><code><a title="phi.math.Shape.non_channel" href="#phi.math.Shape.non_channel">non_channel</a></code></li>
<li><code><a title="phi.math.Shape.non_collection" href="#phi.math.Shape.non_collection">non_collection</a></code></li>
<li><code><a title="phi.math.Shape.non_spatial" href="#phi.math.Shape.non_spatial">non_spatial</a></code></li>
<li><code><a title="phi.math.Shape.normal_order" href="#phi.math.Shape.normal_order">normal_order</a></code></li>
<li><code><a title="phi.math.Shape.only" href="#phi.math.Shape.only">only</a></code></li>
<li><code><a title="phi.math.Shape.order" href="#phi.math.Shape.order">order</a></code></li>
<li><code><a title="phi.math.Shape.order_group" href="#phi.math.Shape.order_group">order_group</a></code></li>
<li><code><a title="phi.math.Shape.perm" href="#phi.math.Shape.perm">perm</a></code></li>
<li><code><a title="phi.math.Shape.product" href="#phi.math.Shape.product">product</a></code></li>
<li><code><a title="phi.math.Shape.rank" href="#phi.math.Shape.rank">rank</a></code></li>
<li><code><a title="phi.math.Shape.reduce" href="#phi.math.Shape.reduce">reduce</a></code></li>
<li><code><a title="phi.math.Shape.reorder" href="#phi.math.Shape.reorder">reorder</a></code></li>
<li><code><a title="phi.math.Shape.reversed" href="#phi.math.Shape.reversed">reversed</a></code></li>
<li><code><a title="phi.math.Shape.shape" href="#phi.math.Shape.shape">shape</a></code></li>
<li><code><a title="phi.math.Shape.sorted" href="#phi.math.Shape.sorted">sorted</a></code></li>
<li><code><a title="phi.math.Shape.spatial" href="#phi.math.Shape.spatial">spatial</a></code></li>
<li><code><a title="phi.math.Shape.spatial_rank" href="#phi.math.Shape.spatial_rank">spatial_rank</a></code></li>
<li><code><a title="phi.math.Shape.to_batch" href="#phi.math.Shape.to_batch">to_batch</a></code></li>
<li><code><a title="phi.math.Shape.unstack" href="#phi.math.Shape.unstack">unstack</a></code></li>
<li><code><a title="phi.math.Shape.volume" href="#phi.math.Shape.volume">volume</a></code></li>
<li><code><a title="phi.math.Shape.well_defined" href="#phi.math.Shape.well_defined">well_defined</a></code></li>
<li><code><a title="phi.math.Shape.with_names" href="#phi.math.Shape.with_names">with_names</a></code></li>
<li><code><a title="phi.math.Shape.with_size" href="#phi.math.Shape.with_size">with_size</a></code></li>
<li><code><a title="phi.math.Shape.with_sizes" href="#phi.math.Shape.with_sizes">with_sizes</a></code></li>
<li><code><a title="phi.math.Shape.with_types" href="#phi.math.Shape.with_types">with_types</a></code></li>
<li><code><a title="phi.math.Shape.without" href="#phi.math.Shape.without">without</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.Solve" href="#phi.math.Solve">Solve</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Solve.absolute_tolerance" href="#phi.math.Solve.absolute_tolerance">absolute_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.gradient_solve" href="#phi.math.Solve.gradient_solve">gradient_solve</a></code></li>
<li><code><a title="phi.math.Solve.max_iterations" href="#phi.math.Solve.max_iterations">max_iterations</a></code></li>
<li><code><a title="phi.math.Solve.method" href="#phi.math.Solve.method">method</a></code></li>
<li><code><a title="phi.math.Solve.relative_tolerance" href="#phi.math.Solve.relative_tolerance">relative_tolerance</a></code></li>
<li><code><a title="phi.math.Solve.suppress" href="#phi.math.Solve.suppress">suppress</a></code></li>
<li><code><a title="phi.math.Solve.x0" href="#phi.math.Solve.x0">x0</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.SolveInfo" href="#phi.math.SolveInfo">SolveInfo</a></code></h4>
<ul class="">
<li><code><a title="phi.math.SolveInfo.converged" href="#phi.math.SolveInfo.converged">converged</a></code></li>
<li><code><a title="phi.math.SolveInfo.convergence_check" href="#phi.math.SolveInfo.convergence_check">convergence_check</a></code></li>
<li><code><a title="phi.math.SolveInfo.diverged" href="#phi.math.SolveInfo.diverged">diverged</a></code></li>
<li><code><a title="phi.math.SolveInfo.function_evaluations" href="#phi.math.SolveInfo.function_evaluations">function_evaluations</a></code></li>
<li><code><a title="phi.math.SolveInfo.iterations" href="#phi.math.SolveInfo.iterations">iterations</a></code></li>
<li><code><a title="phi.math.SolveInfo.method" href="#phi.math.SolveInfo.method">method</a></code></li>
<li><code><a title="phi.math.SolveInfo.msg" href="#phi.math.SolveInfo.msg">msg</a></code></li>
<li><code><a title="phi.math.SolveInfo.residual" href="#phi.math.SolveInfo.residual">residual</a></code></li>
<li><code><a title="phi.math.SolveInfo.snapshot" href="#phi.math.SolveInfo.snapshot">snapshot</a></code></li>
<li><code><a title="phi.math.SolveInfo.solve" href="#phi.math.SolveInfo.solve">solve</a></code></li>
<li><code><a title="phi.math.SolveInfo.x" href="#phi.math.SolveInfo.x">x</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.SolveTape" href="#phi.math.SolveTape">SolveTape</a></code></h4>
</li>
<li>
<h4><code><a title="phi.math.Tensor" href="#phi.math.Tensor">Tensor</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.Tensor.all" href="#phi.math.Tensor.all">all</a></code></li>
<li><code><a title="phi.math.Tensor.any" href="#phi.math.Tensor.any">any</a></code></li>
<li><code><a title="phi.math.Tensor.default_backend" href="#phi.math.Tensor.default_backend">default_backend</a></code></li>
<li><code><a title="phi.math.Tensor.dimension" href="#phi.math.Tensor.dimension">dimension</a></code></li>
<li><code><a title="phi.math.Tensor.dtype" href="#phi.math.Tensor.dtype">dtype</a></code></li>
<li><code><a title="phi.math.Tensor.flip" href="#phi.math.Tensor.flip">flip</a></code></li>
<li><code><a title="phi.math.Tensor.mean" href="#phi.math.Tensor.mean">mean</a></code></li>
<li><code><a title="phi.math.Tensor.native" href="#phi.math.Tensor.native">native</a></code></li>
<li><code><a title="phi.math.Tensor.numpy" href="#phi.math.Tensor.numpy">numpy</a></code></li>
<li><code><a title="phi.math.Tensor.rank" href="#phi.math.Tensor.rank">rank</a></code></li>
<li><code><a title="phi.math.Tensor.shape" href="#phi.math.Tensor.shape">shape</a></code></li>
<li><code><a title="phi.math.Tensor.sum" href="#phi.math.Tensor.sum">sum</a></code></li>
<li><code><a title="phi.math.Tensor.unstack" href="#phi.math.Tensor.unstack">unstack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.TensorDim" href="#phi.math.TensorDim">TensorDim</a></code></h4>
<ul class="two-column">
<li><code><a title="phi.math.TensorDim.as_batch" href="#phi.math.TensorDim.as_batch">as_batch</a></code></li>
<li><code><a title="phi.math.TensorDim.as_channel" href="#phi.math.TensorDim.as_channel">as_channel</a></code></li>
<li><code><a title="phi.math.TensorDim.as_spatial" href="#phi.math.TensorDim.as_spatial">as_spatial</a></code></li>
<li><code><a title="phi.math.TensorDim.exists" href="#phi.math.TensorDim.exists">exists</a></code></li>
<li><code><a title="phi.math.TensorDim.flip" href="#phi.math.TensorDim.flip">flip</a></code></li>
<li><code><a title="phi.math.TensorDim.index" href="#phi.math.TensorDim.index">index</a></code></li>
<li><code><a title="phi.math.TensorDim.is_batch" href="#phi.math.TensorDim.is_batch">is_batch</a></code></li>
<li><code><a title="phi.math.TensorDim.is_channel" href="#phi.math.TensorDim.is_channel">is_channel</a></code></li>
<li><code><a title="phi.math.TensorDim.is_spatial" href="#phi.math.TensorDim.is_spatial">is_spatial</a></code></li>
<li><code><a title="phi.math.TensorDim.optional_unstack" href="#phi.math.TensorDim.optional_unstack">optional_unstack</a></code></li>
<li><code><a title="phi.math.TensorDim.size" href="#phi.math.TensorDim.size">size</a></code></li>
<li><code><a title="phi.math.TensorDim.split" href="#phi.math.TensorDim.split">split</a></code></li>
<li><code><a title="phi.math.TensorDim.unstack" href="#phi.math.TensorDim.unstack">unstack</a></code></li>
<li><code><a title="phi.math.TensorDim.unstack_spatial" href="#phi.math.TensorDim.unstack_spatial">unstack_spatial</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="phi.math.TensorLike" href="#phi.math.TensorLike">TensorLike</a></code></h4>
<ul class="">
<li><code><a title="phi.math.TensorLike.__value_attrs__" href="#phi.math.TensorLike.__value_attrs__">__value_attrs__</a></code></li>
<li><code><a title="phi.math.TensorLike.__variable_attrs__" href="#phi.math.TensorLike.__variable_attrs__">__variable_attrs__</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>