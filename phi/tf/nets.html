<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>phi.tf.nets API documentation</title>
<meta name="description" content="Jax implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phi.tf.nets</code></h1>
</header>
<section id="section-intro">
<p>Jax implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.</p>
<p>For API documentation, see <a href="https://tum-pbs.github.io/PhiFlow/Network_API">https://tum-pbs.github.io/PhiFlow/Network_API</a> .</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Jax implementation of the unified machine learning API.
Equivalent functions also exist for the other frameworks.

For API documentation, see https://tum-pbs.github.io/PhiFlow/Network_API .
&#34;&#34;&#34;
from typing import Callable, Tuple, List
import pickle

import numpy
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers as kl
from tensorflow import Tensor

from .. import math


def parameter_count(model: keras.Model):
    &#34;&#34;&#34;
    Counts the number of parameters in a model.

    Args:
        model: Keras model

    Returns:
        `int`
    &#34;&#34;&#34;
    total = 0
    for parameter in model.trainable_weights:
        total += numpy.prod(parameter.shape)
    return int(total)


def get_parameters(model: keras.Model, wrap=True) -&gt; dict:
    result = {}
    for var in model.trainable_weights:
        name: str = var.name
        layer = name[:name.index(&#39;/&#39;)].replace(&#39;_&#39;, &#39;&#39;).replace(&#39;dense&#39;, &#39;linear&#39;)
        try:
            int(layer[-1:])
        except ValueError:
            layer += &#39;0&#39;
        prop = name[name.index(&#39;/&#39;) + 1:].replace(&#39;kernel&#39;, &#39;weight&#39;)
        if prop.endswith(&#39;:0&#39;):
            prop = prop[:-2]
        name = f&#34;{layer}.{prop}&#34;
        var = var.numpy()
        if not wrap:
            result[name] = var
        else:
            if name.endswith(&#39;.weight&#39;):
                phi_tensor = math.wrap(var, math.channel(&#39;input,output&#39;))
            elif name.endswith(&#39;.bias&#39;):
                phi_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            else:
                raise NotImplementedError(name)
            result[name] = phi_tensor
    return result


def save_state(obj: keras.models.Model or keras.optimizers.Optimizer, path: str):
    &#34;&#34;&#34;
    Write the state of a module or optimizer to a file.

    See Also:
        `load_state()`

    Args:
        obj: `keras.models.Model or keras.optimizers.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.save_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        weights = obj.get_parameters()
        with open(path, &#39;wb&#39;) as f:
            pickle.dump(weights, f)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)


def load_state(obj: keras.models.Model or keras.optimizers.Optimizer, path: str):
    &#34;&#34;&#34;
    Read the state of a module or optimizer from a file.

    See Also:
        `save_state()`

    Args:
        obj: `keras.models.Model or keras.optimizers.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.load_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        with open(path, &#39;rb&#39;) as f:
            weights = pickle.load(f)
        obj.set_weights(weights)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)


def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
    &#34;&#34;&#34;
    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.

    This is the TensorFlow/Keras version. Analogue functions exist for other learning frameworks.

    Args:
        net: Learning model.
        optimizer: Optimizer.
        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
        *loss_args: Arguments given to `loss_function`.
        **loss_kwargs: Keyword arguments given to `loss_function`.

    Returns:
        Output of `loss_function`.
    &#34;&#34;&#34;
    with tf.GradientTape() as tape:
        output = loss_function(*loss_args, **loss_kwargs)
        loss = output[0] if isinstance(output, tuple) else output
        gradients = tape.gradient(loss.sum, net.trainable_variables)
    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
    return output


def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    &#34;&#34;&#34;
    Creates an Adam optimizer for `net`, alias for [`keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)


def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    &#34;&#34;&#34;
    Creates an SGD optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.SGD&#39;](https://keras.io/api/optimizers/sgd/)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.SGD(learning_rate, momentum, nesterov)


def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    &#34;&#34;&#34;
    Creates an Adagrad optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.Adagrad&#39;](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)


def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    &#34;&#34;&#34;
    Creates an RMSProp optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.RMSprop&#39;](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)


def dense_net(in_channels: int,
              out_channels: int,
              layers: Tuple[int, ...] or List[int],
              batch_norm=False,
              activation=&#39;ReLU&#39;) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    keras_layers = []
    for neuron_count in layers:
        keras_layers.append(kl.Dense(neuron_count, activation=activation))
        if batch_norm:
            keras_layers.append(kl.BatchNormalization())
    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
                                    *keras_layers,
                                    kl.Dense(out_channels, activation=&#39;linear&#39;)])


def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: int or tuple or list = 16,
          batch_norm: bool = True,
          activation: str or Callable = &#39;ReLU&#39;,
          in_spatial: tuple or int = 2,
          use_res_blocks: bool = False) -&gt; keras.Model:
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    # --- Construct the U-Net ---
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    x = resnet_block(x, x.shape[-1], filters[0], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation)
    xs = [x]
    for i in range(1, levels):
        x = MAX_POOL[d](2, padding=&#34;same&#34;)(x)
        x = resnet_block(x, x.shape[-1], filters[i], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation)
        xs.insert(0, x)
    for i in range(1, levels):
        x = UPSAMPLE[d](2)(x)
        x = kl.Concatenate()([x, xs[i]])
        x = resnet_block(x, x.shape[-1], filters[i - 1], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)


CONV = [None, kl.Conv1D, kl.Conv2D, kl.Conv3D]
MAX_POOL = [None, kl.MaxPool1D, kl.MaxPool2D, kl.MaxPool3D]
UPSAMPLE = [None, kl.UpSampling1D, kl.UpSampling2D, kl.UpSampling3D]
ACTIVATIONS = {&#39;tanh&#39;: keras.activations.tanh, &#39;ReLU&#39;: keras.activations.relu, &#39;Sigmoid&#39;: keras.activations.sigmoid,
               &#39;SiLU&#39;: keras.activations.selu}


def pad_periodic(x: Tensor):
    d = len(x.shape) - 2
    if d &gt;= 1:
        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
    if d &gt;= 2:
        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
    if d &gt;= 3:
        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
                      axis=3)
    return x


def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable):
    x = pad_periodic(x)
    x = CONV[d](mid_channels, 3, padding=&#39;valid&#39;)(x)
    # x = CONV[d](mid_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    x = pad_periodic(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(x)
    # x = CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    return x


def conv_net(in_channels: int,
             out_channels: int,
             layers: Tuple[int, ...] or List[int],
             batch_norm: bool = False,
             activation: str or Callable = &#39;ReLU&#39;,
             in_spatial: int or tuple = 2) -&gt; keras.Model:
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    x = inputs = keras.Input(shape=d + (in_channels,))
    for i in range(len(layers)):
        x = pad_periodic(x)
        x = CONV[in_spatial](layers[i], 3, padding=&#39;valid&#39;)(x)
        if batch_norm:
            x = kl.BatchNormalization()(x)
        x = activation(x)
    x = pad_periodic(x)
    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    return keras.Model(inputs, x)


def resnet_block(x, in_channels: int,
                 out_channels: int,
                 batch_norm: bool = False,
                 activation: str or Callable = &#39;ReLU&#39;,
                 in_spatial: int or tuple = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)

    d = (None,) * in_spatial
    # x = inputs = keras.Input(d + (in_channels,))

    x_1 = x
    x = pad_periodic(x)

    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    x = pad_periodic(x)

    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    if in_channels != out_channels:
        x_1 = CONV[in_spatial](out_channels, 1)(x_1)
        if batch_norm:
            x_1 = kl.BatchNormalization()(x_1)

    x = kl.Add()([x, x_1])
    # out = activation(out)
    return x
    # return keras.Model(inputs, out)


def res_net(in_channels: int,
            out_channels: int,
            layers: Tuple[int, ...] or List[int],
            batch_norm: bool = False,
            activation: str or Callable = &#39;ReLU&#39;,
            in_spatial: int or tuple = 2):
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)

    x = inputs = keras.Input(shape=d + (in_channels,))
    # print(&#39;X shape : &#39;, x.shape)
    out = resnet_block(x, in_channels, layers[0], batch_norm, activation, in_spatial)

    for i in range(1, len(layers)):
        out = resnet_block(out, layers[i - 1], layers[i], batch_norm, activation, in_spatial)

    out = resnet_block(out, layers[len(layers) - 1], out_channels, batch_norm, activation, in_spatial)
    return keras.Model(inputs, out)


def conv_classifier(input_shape: list, num_classes: int, batch_norm: bool, in_spatial: int or tuple):
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    # input_shape[0] = Channels
    spatial_shape_list = list(input_shape[1:])
    x = inputs = keras.Input(shape=in_spatial + (input_shape[0],))
    x = double_conv(x, d, 64, 64, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 128, 128, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 256, 256, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](256, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 512, 512, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](512, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 512, 512, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](512, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    for i in range(5):
        for j in range(len(spatial_shape_list)):
            spatial_shape_list[j] = math.floor((spatial_shape_list[j] - 2) / 2) + 1

    flattened_input_dim = 1
    for i in range(len(spatial_shape_list)):
        flattened_input_dim *= spatial_shape_list[i]
    flattened_input_dim *= 512

    x = kl.Flatten()(x)
    x = dense_net(flattened_input_dim, num_classes, [4096, 4096, 100], batch_norm, &#39;ReLU&#39;)(x)

    x = kl.Softmax()(x)

    return keras.Model(inputs, x)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="phi.tf.nets.adagrad"><code class="name flex">
<span>def <span class="ident">adagrad</span></span>(<span>net: keras.engine.training.Model, learning_rate: float = 0.001, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adagrad optimizer for 'net', alias for <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad">'keras.optimizers.Adagrad'</a>
Analogous functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adagrad(net: keras.Model, learning_rate: float = 1e-3, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10):
    &#34;&#34;&#34;
    Creates an Adagrad optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.Adagrad&#39;](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.Adagrad(learning_rate, initial_accumulator_value, eps)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.adam"><code class="name flex">
<span>def <span class="ident">adam</span></span>(<span>net: keras.engine.training.Model, learning_rate: float = 0.001, betas=(0.9, 0.999), epsilon=1e-07)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Adam optimizer for <code>net</code>, alias for <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam"><code>keras.optimizers.Adam</code></a>.
Analogous functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def adam(net: keras.Model, learning_rate: float = 1e-3, betas=(0.9, 0.999), epsilon=1e-07):
    &#34;&#34;&#34;
    Creates an Adam optimizer for `net`, alias for [`keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.Adam(learning_rate, betas[0], betas[1], epsilon)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.conv_classifier"><code class="name flex">
<span>def <span class="ident">conv_classifier</span></span>(<span>input_shape: list, num_classes: int, batch_norm: bool, in_spatial: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_classifier(input_shape: list, num_classes: int, batch_norm: bool, in_spatial: int or tuple):
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    # input_shape[0] = Channels
    spatial_shape_list = list(input_shape[1:])
    x = inputs = keras.Input(shape=in_spatial + (input_shape[0],))
    x = double_conv(x, d, 64, 64, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 128, 128, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 256, 256, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](256, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 512, 512, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](512, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    x = double_conv(x, d, 512, 512, batch_norm, ACTIVATIONS[&#39;ReLU&#39;])
    x = pad_periodic(x)
    x = CONV[d](512, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = ACTIVATIONS[&#39;ReLU&#39;](x)
    x = MAX_POOL[d](2)(x)

    for i in range(5):
        for j in range(len(spatial_shape_list)):
            spatial_shape_list[j] = math.floor((spatial_shape_list[j] - 2) / 2) + 1

    flattened_input_dim = 1
    for i in range(len(spatial_shape_list)):
        flattened_input_dim *= spatial_shape_list[i]
    flattened_input_dim *= 512

    x = kl.Flatten()(x)
    x = dense_net(flattened_input_dim, num_classes, [4096, 4096, 100], batch_norm, &#39;ReLU&#39;)(x)

    x = kl.Softmax()(x)

    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.conv_net"><code class="name flex">
<span>def <span class="ident">conv_net</span></span>(<span>in_channels: int, out_channels: int, layers: Tuple[int, ...], batch_norm: bool = False, activation: str = 'ReLU', in_spatial: int = 2) ‑> keras.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv_net(in_channels: int,
             out_channels: int,
             layers: Tuple[int, ...] or List[int],
             batch_norm: bool = False,
             activation: str or Callable = &#39;ReLU&#39;,
             in_spatial: int or tuple = 2) -&gt; keras.Model:
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    x = inputs = keras.Input(shape=d + (in_channels,))
    for i in range(len(layers)):
        x = pad_periodic(x)
        x = CONV[in_spatial](layers[i], 3, padding=&#39;valid&#39;)(x)
        if batch_norm:
            x = kl.BatchNormalization()(x)
        x = activation(x)
    x = pad_periodic(x)
    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.dense_net"><code class="name flex">
<span>def <span class="ident">dense_net</span></span>(<span>in_channels: int, out_channels: int, layers: Tuple[int, ...], batch_norm=False, activation='ReLU') ‑> keras.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dense_net(in_channels: int,
              out_channels: int,
              layers: Tuple[int, ...] or List[int],
              batch_norm=False,
              activation=&#39;ReLU&#39;) -&gt; keras.Model:
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    keras_layers = []
    for neuron_count in layers:
        keras_layers.append(kl.Dense(neuron_count, activation=activation))
        if batch_norm:
            keras_layers.append(kl.BatchNormalization())
    return keras.models.Sequential([kl.InputLayer(input_shape=(in_channels,)),
                                    *keras_layers,
                                    kl.Dense(out_channels, activation=&#39;linear&#39;)])</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.double_conv"><code class="name flex">
<span>def <span class="ident">double_conv</span></span>(<span>x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def double_conv(x, d: int, out_channels: int, mid_channels: int, batch_norm: bool, activation: Callable):
    x = pad_periodic(x)
    x = CONV[d](mid_channels, 3, padding=&#39;valid&#39;)(x)
    # x = CONV[d](mid_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    x = pad_periodic(x)
    x = CONV[d](out_channels, 3, padding=&#39;valid&#39;)(x)
    # x = CONV[d](out_channels, 3, padding=&#39;same&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)
    return x</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.get_parameters"><code class="name flex">
<span>def <span class="ident">get_parameters</span></span>(<span>model: keras.engine.training.Model, wrap=True) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_parameters(model: keras.Model, wrap=True) -&gt; dict:
    result = {}
    for var in model.trainable_weights:
        name: str = var.name
        layer = name[:name.index(&#39;/&#39;)].replace(&#39;_&#39;, &#39;&#39;).replace(&#39;dense&#39;, &#39;linear&#39;)
        try:
            int(layer[-1:])
        except ValueError:
            layer += &#39;0&#39;
        prop = name[name.index(&#39;/&#39;) + 1:].replace(&#39;kernel&#39;, &#39;weight&#39;)
        if prop.endswith(&#39;:0&#39;):
            prop = prop[:-2]
        name = f&#34;{layer}.{prop}&#34;
        var = var.numpy()
        if not wrap:
            result[name] = var
        else:
            if name.endswith(&#39;.weight&#39;):
                phi_tensor = math.wrap(var, math.channel(&#39;input,output&#39;))
            elif name.endswith(&#39;.bias&#39;):
                phi_tensor = math.wrap(var, math.channel(&#39;output&#39;))
            else:
                raise NotImplementedError(name)
            result[name] = phi_tensor
    return result</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.load_state"><code class="name flex">
<span>def <span class="ident">load_state</span></span>(<span>obj: keras.engine.training.Model, path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Read the state of a module or optimizer from a file.</p>
<p>See Also:
<code><a title="phi.tf.nets.save_state" href="#phi.tf.nets.save_state">save_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>keras.models.Model or keras.optimizers.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_state(obj: keras.models.Model or keras.optimizers.Optimizer, path: str):
    &#34;&#34;&#34;
    Read the state of a module or optimizer from a file.

    See Also:
        `save_state()`

    Args:
        obj: `keras.models.Model or keras.optimizers.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.load_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        with open(path, &#39;rb&#39;) as f:
            weights = pickle.load(f)
        obj.set_weights(weights)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.pad_periodic"><code class="name flex">
<span>def <span class="ident">pad_periodic</span></span>(<span>x: tensorflow.python.framework.ops.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad_periodic(x: Tensor):
    d = len(x.shape) - 2
    if d &gt;= 1:
        x = tf.concat([tf.expand_dims(x[:, -1, ...], axis=1), x, tf.expand_dims(x[:, 0, ...], axis=1)], axis=1)
    if d &gt;= 2:
        x = tf.concat([tf.expand_dims(x[:, :, -1, ...], axis=2), x, tf.expand_dims(x[:, :, 0, ...], axis=2)], axis=2)
    if d &gt;= 3:
        x = tf.concat([tf.expand_dims(x[:, :, :, -1, ...], axis=3), x, tf.expand_dims(x[:, :, :, 0, ...], axis=3)],
                      axis=3)
    return x</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.parameter_count"><code class="name flex">
<span>def <span class="ident">parameter_count</span></span>(<span>model: keras.engine.training.Model)</span>
</code></dt>
<dd>
<div class="desc"><p>Counts the number of parameters in a model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>Keras model</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>int</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parameter_count(model: keras.Model):
    &#34;&#34;&#34;
    Counts the number of parameters in a model.

    Args:
        model: Keras model

    Returns:
        `int`
    &#34;&#34;&#34;
    total = 0
    for parameter in model.trainable_weights:
        total += numpy.prod(parameter.shape)
    return int(total)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.res_net"><code class="name flex">
<span>def <span class="ident">res_net</span></span>(<span>in_channels: int, out_channels: int, layers: Tuple[int, ...], batch_norm: bool = False, activation: str = 'ReLU', in_spatial: int = 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def res_net(in_channels: int,
            out_channels: int,
            layers: Tuple[int, ...] or List[int],
            batch_norm: bool = False,
            activation: str or Callable = &#39;ReLU&#39;,
            in_spatial: int or tuple = 2):
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)

    x = inputs = keras.Input(shape=d + (in_channels,))
    # print(&#39;X shape : &#39;, x.shape)
    out = resnet_block(x, in_channels, layers[0], batch_norm, activation, in_spatial)

    for i in range(1, len(layers)):
        out = resnet_block(out, layers[i - 1], layers[i], batch_norm, activation, in_spatial)

    out = resnet_block(out, layers[len(layers) - 1], out_channels, batch_norm, activation, in_spatial)
    return keras.Model(inputs, out)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.resnet_block"><code class="name flex">
<span>def <span class="ident">resnet_block</span></span>(<span>x, in_channels: int, out_channels: int, batch_norm: bool = False, activation: str = 'ReLU', in_spatial: int = 2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet_block(x, in_channels: int,
                 out_channels: int,
                 batch_norm: bool = False,
                 activation: str or Callable = &#39;ReLU&#39;,
                 in_spatial: int or tuple = 2):
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(in_spatial, int):
        d = (None,) * in_spatial
    else:
        assert isinstance(in_spatial, tuple)
        d = in_spatial
        in_spatial = len(d)

    d = (None,) * in_spatial
    # x = inputs = keras.Input(d + (in_channels,))

    x_1 = x
    x = pad_periodic(x)

    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    x = pad_periodic(x)

    x = CONV[in_spatial](out_channels, 3, padding=&#39;valid&#39;)(x)
    if batch_norm:
        x = kl.BatchNormalization()(x)
    x = activation(x)

    if in_channels != out_channels:
        x_1 = CONV[in_spatial](out_channels, 1)(x_1)
        if batch_norm:
            x_1 = kl.BatchNormalization()(x_1)

    x = kl.Add()([x, x_1])
    # out = activation(out)
    return x
    # return keras.Model(inputs, out)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.rmsprop"><code class="name flex">
<span>def <span class="ident">rmsprop</span></span>(<span>net: keras.engine.training.Model, learning_rate: float = 0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an RMSProp optimizer for 'net', alias for <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop">'keras.optimizers.RMSprop'</a>
Analogous functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rmsprop(net: keras.Model, learning_rate: float = 1e-3, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False):
    &#34;&#34;&#34;
    Creates an RMSProp optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.RMSprop&#39;](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.RMSprop(learning_rate, alpha, momentum, eps, centered)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.save_state"><code class="name flex">
<span>def <span class="ident">save_state</span></span>(<span>obj: keras.engine.training.Model, path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Write the state of a module or optimizer to a file.</p>
<p>See Also:
<code><a title="phi.tf.nets.load_state" href="#phi.tf.nets.load_state">load_state()</a></code></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>obj</code></strong></dt>
<dd><code>keras.models.Model or keras.optimizers.Optimizer</code></dd>
<dt><strong><code>path</code></strong></dt>
<dd>File path as <code>str</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_state(obj: keras.models.Model or keras.optimizers.Optimizer, path: str):
    &#34;&#34;&#34;
    Write the state of a module or optimizer to a file.

    See Also:
        `load_state()`

    Args:
        obj: `keras.models.Model or keras.optimizers.Optimizer`
        path: File path as `str`.
    &#34;&#34;&#34;
    if isinstance(obj, keras.models.Model):
        if not path.endswith(&#39;.h5&#39;):
            path += &#39;.h5&#39;
        obj.save_weights(path)
    elif isinstance(obj, keras.optimizers.Optimizer):
        if not path.endswith(&#39;.pkl&#39;):
            path += &#39;.pkl&#39;
        weights = obj.get_parameters()
        with open(path, &#39;wb&#39;) as f:
            pickle.dump(weights, f)
    else:
        raise ValueError(&#34;obj must be a Keras model or optimizer&#34;)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.sgd"><code class="name flex">
<span>def <span class="ident">sgd</span></span>(<span>net: keras.engine.training.Model, learning_rate: float = 0.001, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an SGD optimizer for 'net', alias for <a href="https://keras.io/api/optimizers/sgd/">'keras.optimizers.SGD'</a>
Analogous functions exist for other learning frameworks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sgd(net: keras.Model, learning_rate: float = 1e-3, momentum=0, dampening=0, weight_decay=0, nesterov=False):
    &#34;&#34;&#34;
    Creates an SGD optimizer for &#39;net&#39;, alias for [&#39;keras.optimizers.SGD&#39;](https://keras.io/api/optimizers/sgd/)
    Analogous functions exist for other learning frameworks.
    &#34;&#34;&#34;
    return keras.optimizers.SGD(learning_rate, momentum, nesterov)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.u_net"><code class="name flex">
<span>def <span class="ident">u_net</span></span>(<span>in_channels: int, out_channels: int, levels: int = 4, filters: int = 16, batch_norm: bool = True, activation: str = 'ReLU', in_spatial: tuple = 2, use_res_blocks: bool = False) ‑> keras.engine.training.Model</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def u_net(in_channels: int,
          out_channels: int,
          levels: int = 4,
          filters: int or tuple or list = 16,
          batch_norm: bool = True,
          activation: str or Callable = &#39;ReLU&#39;,
          in_spatial: tuple or int = 2,
          use_res_blocks: bool = False) -&gt; keras.Model:
    if isinstance(in_spatial, int):
        d = in_spatial
        in_spatial = (None,) * d
    else:
        assert isinstance(in_spatial, tuple)
        d = len(in_spatial)
    activation = ACTIVATIONS[activation] if isinstance(activation, str) else activation
    if isinstance(filters, (tuple, list)):
        assert len(filters) == levels, f&#34;List of filters has length {len(filters)} but u-net has {levels} levels.&#34;
    else:
        filters = (filters,) * levels
    # --- Construct the U-Net ---
    x = inputs = keras.Input(shape=in_spatial + (in_channels,))
    x = resnet_block(x, x.shape[-1], filters[0], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[0], filters[0], batch_norm, activation)
    xs = [x]
    for i in range(1, levels):
        x = MAX_POOL[d](2, padding=&#34;same&#34;)(x)
        x = resnet_block(x, x.shape[-1], filters[i], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[i], filters[i], batch_norm, activation)
        xs.insert(0, x)
    for i in range(1, levels):
        x = UPSAMPLE[d](2)(x)
        x = kl.Concatenate()([x, xs[i]])
        x = resnet_block(x, x.shape[-1], filters[i - 1], batch_norm, activation, d) if use_res_blocks else double_conv(x, d, filters[i - 1], filters[i - 1], batch_norm, activation)
    x = CONV[d](out_channels, 1)(x)
    return keras.Model(inputs, x)</code></pre>
</details>
</dd>
<dt id="phi.tf.nets.update_weights"><code class="name flex">
<span>def <span class="ident">update_weights</span></span>(<span>net: keras.engine.training.Model, optimizer: keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2, loss_function: Callable, *loss_args, **loss_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the gradients of <code>loss_function</code> w.r.t. the parameters of <code>net</code> and updates its weights using <code>optimizer</code>.</p>
<p>This is the TensorFlow/Keras version. Analogue functions exist for other learning frameworks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>net</code></strong></dt>
<dd>Learning model.</dd>
<dt><strong><code>optimizer</code></strong></dt>
<dd>Optimizer.</dd>
<dt><strong><code>loss_function</code></strong></dt>
<dd>Loss function, called as <code>loss_function(*loss_args, **loss_kwargs)</code>.</dd>
<dt><strong><code>*loss_args</code></strong></dt>
<dd>Arguments given to <code>loss_function</code>.</dd>
<dt><strong><code>**loss_kwargs</code></strong></dt>
<dd>Keyword arguments given to <code>loss_function</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output of <code>loss_function</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_weights(net: keras.Model, optimizer: keras.optimizers.Optimizer, loss_function: Callable, *loss_args, **loss_kwargs):
    &#34;&#34;&#34;
    Computes the gradients of `loss_function` w.r.t. the parameters of `net` and updates its weights using `optimizer`.

    This is the TensorFlow/Keras version. Analogue functions exist for other learning frameworks.

    Args:
        net: Learning model.
        optimizer: Optimizer.
        loss_function: Loss function, called as `loss_function(*loss_args, **loss_kwargs)`.
        *loss_args: Arguments given to `loss_function`.
        **loss_kwargs: Keyword arguments given to `loss_function`.

    Returns:
        Output of `loss_function`.
    &#34;&#34;&#34;
    with tf.GradientTape() as tape:
        output = loss_function(*loss_args, **loss_kwargs)
        loss = output[0] if isinstance(output, tuple) else output
        gradients = tape.gradient(loss.sum, net.trainable_variables)
    optimizer.apply_gradients(zip(gradients, net.trainable_variables))
    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phi.tf" href="index.html">phi.tf</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="phi.tf.nets.adagrad" href="#phi.tf.nets.adagrad">adagrad</a></code></li>
<li><code><a title="phi.tf.nets.adam" href="#phi.tf.nets.adam">adam</a></code></li>
<li><code><a title="phi.tf.nets.conv_classifier" href="#phi.tf.nets.conv_classifier">conv_classifier</a></code></li>
<li><code><a title="phi.tf.nets.conv_net" href="#phi.tf.nets.conv_net">conv_net</a></code></li>
<li><code><a title="phi.tf.nets.dense_net" href="#phi.tf.nets.dense_net">dense_net</a></code></li>
<li><code><a title="phi.tf.nets.double_conv" href="#phi.tf.nets.double_conv">double_conv</a></code></li>
<li><code><a title="phi.tf.nets.get_parameters" href="#phi.tf.nets.get_parameters">get_parameters</a></code></li>
<li><code><a title="phi.tf.nets.load_state" href="#phi.tf.nets.load_state">load_state</a></code></li>
<li><code><a title="phi.tf.nets.pad_periodic" href="#phi.tf.nets.pad_periodic">pad_periodic</a></code></li>
<li><code><a title="phi.tf.nets.parameter_count" href="#phi.tf.nets.parameter_count">parameter_count</a></code></li>
<li><code><a title="phi.tf.nets.res_net" href="#phi.tf.nets.res_net">res_net</a></code></li>
<li><code><a title="phi.tf.nets.resnet_block" href="#phi.tf.nets.resnet_block">resnet_block</a></code></li>
<li><code><a title="phi.tf.nets.rmsprop" href="#phi.tf.nets.rmsprop">rmsprop</a></code></li>
<li><code><a title="phi.tf.nets.save_state" href="#phi.tf.nets.save_state">save_state</a></code></li>
<li><code><a title="phi.tf.nets.sgd" href="#phi.tf.nets.sgd">sgd</a></code></li>
<li><code><a title="phi.tf.nets.u_net" href="#phi.tf.nets.u_net">u_net</a></code></li>
<li><code><a title="phi.tf.nets.update_weights" href="#phi.tf.nets.update_weights">update_weights</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>